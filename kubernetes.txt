REF: @[./kubernetes_core_v1_types.go.html] (/core/v1/types.go)

● Kubernetes 101 [[{101,02_doc_has.keypoint]]

- Kubernetes gets in charge of managing (up to thousands of!!!) containers, in particular:
  - distribution of images:
  - storage for file-systems/objects:
  - networking:
  - life-cycle scheduling:
  - load balancing: both of traffic comming to N containers (maybe peer replicas)
                    and of containers themself balanced to different CPUs in different
                    servers.

- Kubernetes uses labels as “nametags” to identify and query for "Kubernetes resources".
- A "kubernetes resource" can be tagged with N simultaneous labels to mark its
  role, target-enviroment, billing account, "anything else" ...

- Pods: runnable "unit of work". Ussually a single container, but maybe a container
  and "extension" containers (or side-cars). For example PostgreSQL + Postgraphile.
  Containers inside a Pods are scheduled togheter and share the same network and
  file-system.

- Replication controllers: Pod template to be replicated N times.
  (they way we talk to Kubernetes so that it can take sensible action to scale up/down
   a given Pod one or more times).

- Services: external INMUTABLE VIEW of our Application (IP address, ports, ...)
        (vs internal "moving" Pods)

- Volumes: Reserved space seen as a local filesystem by Pods.
  Internally the can be backed by:
  - local Hard Disk storage
  - Ceph   : (Hyperscalable networked and clustered file-system compatible with POSIX)
  - Gluster: (Yet another Ceph altern
  - Elastic Block Storage
  - ...
- Namespaces: Virtual Kubernetes clusters (similar to DDBB Schemas, ...)
  ( It's a good habit to split projects by namespace (vs using "default" )
[[}]]

● WARN! Kubernetes is NOT for you! [[{]]
Extracted from @[https://pythonspeed.com/articles/dont-need-kubernetes/]
  """...
    MICROSERVICES ARE AN ORGANIZATIONAL SCALING TECHNIQUE: WHEN YOU HAVE
    500 DEVELOPERS WORKING ON ONE LIVE WEBSITE, IT MAKES SENSE TO PAY THE
    COST OF A LARGE-SCALE DISTRIBUTED SYSTEM IF IT MEANS THE DEVELOPER
    TEAMS CAN WORK INDEPENDENTLY. SO YOU GIVE EACH TEAM OF 5 DEVELOPERS A
    SINGLE MICROSERVICE, AND THAT TEAM PRETENDS THE REST OF THE
    MICROSERVICES ARE EXTERNAL SERVICES THEY CAN’T TRUST.

    IF YOU’RE A TEAM OF 5 AND YOU HAVE 20 MICROSERVICES, AND YOU
    DON’T HAVE A VERY COMPELLING NEED FOR A DISTRIBUTED SYSTEM,
    YOU’RE DOING IT WRONG. INSTEAD OF 5 PEOPLE PER SERVICE LIKE THE BIG
    COMPANY HAS, YOU HAVE 0.25 PEOPLE PER SERVICE.
  """

- Extracted from Local Persistent Volume documentation:
  """  Because of these constraints,   IT’S BEST TO EXCLUDE NODES WITH
    LOCAL VOLUMES FROM AUTOMATIC UPGRADES OR REPAIRS, and in fact some
    cloud providers explicitly mention this as a best practice. """

  Basically most of the benefits of k8s are lost for apps managing
  storage at App level. This is the case with most modern DDBBs and stream
  architectures (PostgreSQL, MySQL, kafka, p2p-alike Ethereum, ....).

- As a reference Google, original creator of K8s, launches
  2 BILLION CONTAINERS PER WEEK!!! in its infrastructures.
  This is not excatly a normal IT load.

- See also:
@[https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing]
   1.- "The network is reliable"
   2.- "Latency is zero"
   3.- "Bandwidth is infinite"
   4.- "The network is secure"
   5.- "Topology doesn't change"
   6.- "There is one administrator"
   7.- "Transport cost is zero"
   8.- "The network is homogeneous"

See also: Reasons NOT to use k8s:
https://thenewstack.io/question-the-current-dogma-is-kubernetes-hyper-scale-necessary-for-everyone/

● Why Coinbase Is Not Using Kubernetes to Run Their Container Workloads
@[https://www.infoq.com/news/2020/07/coinbase-kubernetes/]

● Real Problems when applying Readiness Probes that will not appear
  when not using Readiness Probes
(yes, it can be fixed with better readiness probes, but who knows it!)
  @[https://github.com/helm/charts/issues/24537]
  """ The creation of a new replica fails when the database to replicate is
     very large. It seems that the pod is restarted by the readiness probe
     whilst importing the data into the database. As the import and setup
     are not yet finished the next start fails.

     Database creation/import for slave is interrupted because of
     readiness probe restarting the pod. When the pod has restarted it
     complains the postgres folder is corrupted (missing postgres version).

    How to reproduce it:
    Primary with a very large database and add a replica, log follow and
    see the process being interrupted.
  """
  Unfortunately this problem will appear at random depending on storage/CPU speed.
  It can pass all tests and fail in production due to "parallel" tasks slowing down
  storage.


● WARN! Kubernetes is for you!
- You plan to have a running infrastructure for years,
  and you know in advance that you will need to automate
  lot of RESTfull deployments in an standard way.
- You want to bet for a technology that is well known
  by software industry, so you can hire people that already
  are familiar with it.
- Distributed systems are complex are is better to skip,
  but you can not avoid them. Then k8s is the most familiar
  approach.
- Your company has different isolated teams with random knowledge
  and skills, from finances, to science, to marketing, .....
    They all want a common base and future-resistent virtual
  computing infrastructure.
    They all want to share computer resources, and balance
  computation when needed, as well as reuse DevOps knowledge
  for databases, web servers, networking, ...
- Some k8s operator automate all the life-cycle of software
  management for a given database, CMS, ... You want to profit
  from such existing operator and the knowledge and experience
  of the operator developers.
[[}]]

● External Links
- About Desired State management   @[https://www.youtube.com/watch?v=PH-2FfFD2PU]

- Online training:
@[https://www.katacoda.com/courses/kubernetes/playground]
@[https://labs.play-with-k8s.com/]

- @[https://kubernetes.io/]
- @[https://kubectl.docs.kubernetes.io/]   ← Book
- @[https://kubernetes.io/docs/reference/] (Generated from source)
- @[https://kubernetes.io/docs/reference/glossary/]
- @[https://kubernetes.io/docs/reference/tools/]

- Most voted questions k8s on serverfault:
@[https://serverfault.com/questions/tagged/kubernetes?sort=votes]
- Most voted questions k8s on stackoverflow:
@[https://stackoverflow.com/questions/tagged/kubernetes?sort=votes]

- Real app Examples:
@[https://github.com/kubernetes/examples]

ºk8s Issues:º
- @[https://kubernetes.io/docs/reference/issues-security/issues/]
- @[https://kubernetes.io/docs/reference/issues-security/security/]

ºk8s Enhancements/Backlogº
- @[https://github.com/kubernetes/enhancements]


ºSIG-Appsº
- @[https://github.com/kubernetes/community/blob/master/sig-apps/README.md]
  Special Interest Group for deploying and operating apps in Kubernetes.
  - They meet each week to demo and discuss tools and projects.
   """ Covers deploying and operating applications in Kubernetes. We focus
    on the developer and devops experience of running applications in
    Kubernetes. We discuss how to define and run apps in Kubernetes, demo
    relevant tools and projects, and discuss areas of friction that can lead
    to suggesting improvements or feature requests """
- Yaml Tips:
@[https://www.redhat.com/sysadmin/yaml-tips]
● k8s UI Control Panels [[{01_PM.low_code,monitoring,]]
- Kubernetes Dashboard (kube-dashboard) [most mature and popular]
  · WARN: Filtering ability is limited.
- Lens [most powerful one]:
  · Electro based.
  · Supports CRD and Helm3
  · Built-in Terminal (for kubectl)

- Octant [easy to install and most portable]
  · homepage shows overview of Deployments/Pods/ReplicaSets/Services/...
  · It allows to enable port-forwarding on a pod, read logs, modify pods manifest,
    check pod conditions (initialized, ready, containersReady, PodsScheduled, ...)
  $ octant # ← creates dashboard at http://localhost:7777 by default.
- kubenav [Android and iOS support]
[[}]]

[[}]]

● K8S Summary [[{]]
- k8s: orchestates pools-of-CPUs, local/lan-storage and networks balancing
       load to a pool of N VMs/physical machines.

- Kubernetes Cluster Components:

 ºetcdº: k8s "brain memory"        ºcluster of 1+ mastersº:
 - High availability key/value      - manages the cluster.
   ddbb used to save the cluster    - keeps tracks of:
   metadata, service registrat.       -ºdesired stateº
   and discovery                      - application scalation
                                      - rolling updates
                                    - Raft consensus used in multimaster mode
                                      (requires 1,3,5,... masters)
                                     └────────┬─────────────────┘
        ┌───────────────┬────────────┬────────┴────────────────┬──────────┬─────────────────────┐
 kube─apiserver         │ kube─controller─manager              │ kube-scheduler                 │
 ─ (REST) Wrapper around│ ─ EMBEDS K8S CORE CONTROL LOOPS!!!   │ ─ Assigns workloads to nodes   │
   k8s objects          │ ─ handles a number of controllers:   │ ─ tracks host-resource ussage  │
 - Listen for management│   ─ regulating the state of  cluster │ ─ tracks total resources       │
   tools (kubectl,      │   ─ perform routine tasks            │   available per server and     │
   dashboard,...)       │     ─ ensures number of replicas for │   resources allocated to       │
                        │       a service,                     │   existing workloads assigned  │
                        │     ─ ...                            │   to each server               │
                        │                                      │ ─ Manages availability,        │
                        │                                      │   performance and capacity     │
                        │                                      │                                │
       |federation-apiserver|         |federation-controller-manager|     |cloud─controller─manager|
       - API server for federated     - embeds the core control loops      - k8s (alpha in v1.6)
         clusters                       shipped with k8s federation.       - runs controllers
                                                                             interacting with
                                                                             cloud provider
                                                                           - affected controllers:
                                                                             - Node    Controller
     NOTE:                                                                   - Route   Controller
     An "Application" will contain also app. related Controllers             - Service Controller
     (replica-sets, batch, Deployments ...) not shown in the diagram         - Volume  Controller

  ┌─ K8S──┐      ┌─────────┐      ┌─────────────┐
  │CLUSTER│1 ←→ N│Namespace│ 1 ←→ │"Application"│
  └┬──────┘      └──┬──────┘      └──────┬──────┘
   ├ etcd      "Virtual cluster"  In practice it could be something similar to:
   ├ Master    *1                   1 DDBB     Deployment
   └ Node Pool for App            + 1 ESB      Deployment
     └┬──────────────┘            + 1 Middlewa.Deployment
      ├ node 1 exec. App pods     + ...        └────┬───┘
      ├ node 2 exec. App pods         0+ PVCs, 1 Service, N ConfigMaps+Secrets, 0-1 Ingress,...)
        └┬───┘                      + Desired states for pods (Containers, #replicas, live-probes, ...)
         ├ kubelet agent:           + Rolling policies
         │  monitor that containers are running
         │  as described in (Pod)Specs
         ├ Container Runtime
         │ (Docker/rkt/...)
         └ kube-proxy: Soft. Defined Network "Enabler" @[#kube-proxy_summary]

        Pod     1 ←················→ 1+ Container (core/v1/types.go)
        ====                            =========
     - Minimum Unit of Exec/Sched.      - executable OCI ("Docker") image
     - At creation time resource        -ºLifecycle Hooksº
       limits for CPU/memory are          - PostStart
       defined (min and max-opt)          - PreStop
     - Ussually Pod 1←→1 Container      - containers in a Pod are co-located/co-scheduled
       (rarely 2 when such containers    on the same cluster node.
        are closely related)
     -**Pods are ephemeral.**
     - Define shared resources for its container/s:
       · Volumes:    (Shared Storage)
       · Networking: (Shared unique internal cluster IP)
       · Metadata:   (ports to use,...)
     - Represent a "logical host"
     - It is tied to the Node where it has been scheduler to run,
       and remains there until termination. In case of failure,
       they can be scheduled to run on other available Nodes.
       (Controllers takes charge of scalling /"healing" pods)

     - NODE STATUS: (USED TO DE/SCHEDULE PODS)  (core/v1/types.go)
     - Addresses  ←─(HostName, ExternalIP, InternalIP)
     - Capacity (CPU, memory,  max.number of pods supported)
     - Info: (kernel/k8s/docker version, OS name, ...)
     - Condition: status of ALL Running nodes.
       Node               Description
       ─────────────────  ────────────────────────────────────────────────
       OutOfDisk          True → insufficient free space to add new pods
       Ready              True → node healthy, ready to accept pods
                          False after ºpod─eviction─timeoutº (5 minutes by default):
                          reschedule in another Pod.
       MemoryPressure     True → pressure exists on the node memory
       PIDPressure        True → pressure exists on the processes
       DiskPressure       True → pressure exists on the disk size
       NetworkUnavailable True → node network not correctly configured

● k8s Application
┌─ KEY POINT: ───────────────────────────────────────────────────────────────┐
│ A K8S APPLICATION CAN BE DEFINED AS THE SUM OF PODS, SERVICE, INGRESS,     │
│ CONFIGMAP, SECRETS, VOLUMES, DEPLOYMENT AND STATEFULSET DESCRIBING IT:     │
│ (ALSO WE CAN SEE A "SOLUTION" AS A SET OF APPLICATION AND EACH APPLICATION │
│ AS A DEPLOYMENT ─OR DAEMONSET, JOB, ...)                                   │
│ **WARN: THIS DEFINITION IS INFORMAL**                                      │
└────────────────────────────────────────────────────────────────────────────┘
 Creating Pods   ╶╶╶╶▷ Create Volumes  ╶╶╶╶▷ Create ConfigMap⅋
 running the app       to store input/       Secrets for initial
 listening for         ouput data.           parametrizable config.
 network requests                               ┆
     ┌╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴╴┘
     ▽
 Creating Services     ╶╶╶╶▷ Creating Ingress Rules       ╶╶╶╶▷  Creating Deployment|
 to offer a fixes IP/DNS     to expose HTTP/S Services           StatefulSet|Daemons
 to "moving" Pods.           externally allowing to              |(Batch)Jobs
 Pod creating/destruction    split requests based on host name,
 will fastly trigger         HTTP path/subpaths, ...
 kube-proxy updates on each
 working node.
 network requests

● Config. Secrets and ConfigMaps
- A Pods references the secrets in 2 ways:
  - as files in a mounted volume.
  - as ENVIRONMENT variables.
  (Also used by kubelet when pulling images for the pod)

- k8s Built-in Secrets
  - Service Accounts automatically create and attach secrets with API Credentials.
  - k8s automatically creates secrets with credentials granting API access.
  **NOTE: Pods are automatically modified to use them.**

- CREATING USER SECRETS
  =====================
  - STEP 1) DECLARE SECRETS

    ALTERNATIVE 1               │ ALTERNATIVE 2                 │ ALTERNATIVE 3
    Create un-encrypted secrets │ Create Secret with data       │ Create Secret with stringData
    locally                     │                               │
    $ echo -n ... > ./user.txt  │ cat << EOF > secret.yaml      │ apiVersion: v1
    $ echo -n ... > ./pass.txt  │ apiVersion: v1                │ kind:ºSecretº
              ^^^               │ kind:ºSecretº                 │ metadata:
      special chars. must       │ metadata:                     │   name: mysecret
      be '\' escaped.           │   name: mysecret              │ type: Opaque
                                │ type: Opaque                  │ stringData:
                                │ data:                         │   config.yaml: |-
                                │   user:$(echo -n .. | base64) │     apiUrl: "https://.../api/v1"
                                │   pass:$(echo -n .. | base64) │     user: admin
                                │ EOF                           │     pass: 1f2...
                                ·
  - STEP 2) SAVE TO CLUSTER     ·
                                │   $ kubectl apply -f ./secret.yaml
    $ kubectl create secret  \  │
     generic **db-user-pass**\  │
      --from-file=./user.txt \  │
      --from-file=./pass.txt    │

  - STEP 3) VERIFY/CHECK CREATED SECRET
      $ kubectl get secrets                        $ kubectl describe secrets/db-user-pass
      → NAME           TYPE    DATA  AGE           Name: **db-user-pass**
    **→ db-user-pass   Opaque  2     51s*          ...
                                                   Data
                                                   ====
                                                   user.txt:    5 bytes
                                                   pass.txt:    12 bytes

- "INJECTING" SECRETS INTO POD's CONTAINERS:
  Alt 1.a:                        │ Alt 1.b:                              │ Alt 2: Consume as ENV.VAR
  Mount file as volume            │ Mount items file as volume            │ (App will read env.vars to fetch
                                  │                                       │  the secrets)
                                  │                                       │
  apiVersion: v1                  │ apiVersion: v1                        │ apiVersion: v1
  kind:ºPodº                      │ kind:ºPodº                            │ kind: Pod
  metadata:                       │ metadata:                             │ metadata:
    name: mypod                   │   name: mypod                         │   name: secret-env-pod
  spec:                           │ spec:                                 │ spec:
    containers:                   │   containers:                         │  containers:
    ─ name: mypod                 │   - name: mypod                       │  - name: mycontainer
      image: redis                │     image: redis                      │   image: redis
      volumeMounts:               │     volumeMounts:                     │  ºenv:º
      ─ name:ºfoOº                │     - name:ºfooº                      │    - name: SECRET_USERNAME
        mountPath: "/etc/foo"     │     º mountPath: "/etc/secret" º      │      valueFrom:
        readOnly:ºtrueº           │       readOnly: true                  │      **secretKeyRef:**
    volumes:                      │   volumes:                            │         name:**db-user-pass**
    ─ name: foo                   │   - name: foo                         │         key: username
      secret:                     │     secret:                           │    - name: SECRET_PASSWORD
        secretName:**db─user─pass**       secretName:**db─user─pass**     │      valueFrom:
        defaultMode: 256          │       items:                          │      **secretKeyRef:**
                      ^           │       - key: username                 │         name:**db─user─pass**
                      ·           │         path: my-group/my-username    │         key: password
                      ·                           ^^^^^^^^^^^^^^^^^^^^
           JSON does NOT support     · username will be seen in container as:
           octal notation.             /etc/foo/my-group/my-username
           256 = 0400                · password secret is not projected


- CREATING A ConfigMap:
   INPUT DATA                   CREATE ConfigMap                   VERIFY ConfigMap created
   =======================   →  STEP 1.Alt 1)                      ========================
   ../config1/                 $ kubectl create configmap \        $ kubectl get configmaps \
      ├─ºgame.propertiesº        config1 --from-file=config1/        config1 -o yaml
      └─º  ui.propertiesº                                          ...
                                                                     data:
                               STEP 1.Alt 2)                         game.properties:
                               $ kubectl create configmap \            ...
                                 --from-literal=key1.sub=val1 \      ui.properties:
                                 --from-literal=...

    ┌────────────────────────────────┐
    │kind: Pod                       │
    │...                             │
    │spec:                           │
    │ containers:                    │
    │  ─ name: test─container        │
    │    ...                         │
    │    env:                        ← INJECTING IN CONTAINER
    │    º─ name: ENEMIES_CHEAT     º│ =======================
    │    º  valueFrom:              º│
    │    º    configMapKeyRef:      º│
    │    º      name: game─config   º│
    │    º      key: enemies.cheat  º│
    ...

● Service: INMUTABLE VIEW OF "APP" (set of moving Pods) [[{101.service]]
- Pods and Pods config is mutable. Pods can move to a different working node in the k8s pool
  "at random", changing their internal IP.
- The Service provides the INMUTABLE VIEW (as seen by other Pods/Apps) of the moving
  pods with a PERMANENT internal Cluster IP|DNS name by grouping Pods in a "logical set"
  with a NETWORK POLICY to access them.

  NOTE: The "kube-proxy" gets in charge of the "network magic": @[#kube-proxy_summary]

   Time0 ·················> Time1 ·······> Time2 ·············...
     v                        v              v
   client ··> Service ··┐                  client ··> Service ··┐
   request              ·                  request              ·
              Pod@Node1<┘   ┌···· Node1                         ·
                            ·                                   ·
                  Node2     └>Pod@Node2               Pod@Node2<┘

- SERVICE TYPES:
  =============
  · ClusterIP    : (default) internal-to-cluster virtual IP.
                   No IP exposed outside the cluster.

  · ExternalName : Maps apps (Pods services) to an externally
                   visible DNS entry (ex: foo.example.com)

┌ · NodePort     : (L4) Directly Exposes an external Node-IP:Port to internal Pods.
│
├ · LoadBalancer : (L4) Exposes 'Service' externally using cloud provider's load
│                       balancer. NodePort⅋ClusterIP services are automa. created.
│
└→ Alternatively 'Ingress' (L7) can be used to expose HTTP/S services externally.
   'NodePort'/'LoadBalancer' Services allows for TCP,UDP,PROXY and SCTP(k8s 1.2+)
   - Advanced load balancing (persistent sessions, dynamic weights) are NOT yet
     supported by Ingress rules (20??-??)
   - 'Ingress' allows to expose services based on HTTP HEADER 'host' (virtual hosting),
      HTTP paths/path regex) ,...  not available to 'NodePort'/'LoadBalancer' Services

              ┌Cloud Provider┐
          ┌···│ LoadBalancer │······┐
          ·   └──────────────┘      ·
          v                         v
      ClusterIP          │       NodePort             │      ExternalName
      =========          │       ========             │      ============
                         │                            │
            ┌···>:80     │   :3100             :80    │           ┌·>|node1├·>:80
            ·    │Pod│   │   ┌··>│node├··┐  ┌─>│Pod│  │           ·           │Pod│
  Internal  ·            │   ·           ·  ·         │ External  ·
  Traffic   ·            │ Incomming     v  ·         │ Traffic   ·
  └·······> ClusterIP:80 │ traffic      │NodePort│    │ └·····> ExternalIP:80
            ·            │   ·           ^  ·         │     │   │LoadBalancer│
            ·            │   ·           ·  ·         │     │     ·
            ·    │Pod│   │   └··>│node├··┘  └·>│Pod│  │ Incomming ·            Pod│
            └···>:80     │   :3100          :80       │   traffic └·>│node2├·>:80
                         │   ^                ^       │
                         │   Creates mapping between  │
                         │   node port(3100) and      │
                         │   Pod port (80)            │


  ┌ service.yaml ────────┐   $ kubectl expose deployment/my-nginx # <- alt1:
  │ apiVersion: v1       │   $ kubectl apply -f service.yaml      # <- alt2:
  │ kind: Service        │
  │ metadata:            │
  │   name: Redis        │ ← Must be valid DNS mapping to DNS entry 'redis.namespace01'
  │ spec:                │    (when using strongly recommended k8s DNS add-on).
  │                      │
  │   selector:          │ ← Targeted Pods (logical group of Pods) *1
  │     app: MyApp       │   ← Matching label/s (ussually something like an App or App Sub-service)
  │                      │
  │   clusterIP: 1.1.1.1 │ ← Optional (discouraged). Used to force IP matching existing DNS entry
  │                      │   or hardcoded ("legacy") IPs difficult to reconfigure to new k8s
  │                      │   deployments (by default clusterIP is  auto-assigned).
  │                      │   KEY-POINT: When the service is up, Pods will be injected ENV.VARs like:
  │                      │     REDIS_SERVICE_HOST : 1.1.1.1   REDIS_PORT: ...
  │   ports:             │     REDIS_SERVICE_PORT : 6379      REDIS_PORT_6379_TCP_...
  │   - name: http       │
  │     protocol: TCP    │ ← := TCP* | UDP | HTTP | PROXY | SCTP(k8s 1.2+)
  │     port: 80         │ ← Request to port 80 will be forwarded to Port 9376
  │     targetPort: 9376 │   on "any" pod matching the selector.
  │   - name: https      │
  │     ...              │
  │   sessionAffinity: "ClientIP" ← Optionally set timeout (defaults to 10800)     [[{troubleshooting.network]]
  └──────────────────────┘          sessionAffinityConfig.clientIP.timeoutSeconds  [[}]]

    *1: There is a particular scenario where the selector is empty:
        'Service' pointing to an 'Endpoint' object  ┌ kind: Endpoints ─────────────────┐
         representing an existing external          │ metadata: { name: ddbb01 }       │
         (TCP) service (DDBB, ERP, API REST, ...)   │ subsets:                         │
         in a different namespace, cluster,         │   - addresses: {  ip: 10.0.0.10 }│
         VM or non-k8s controlled IP                │   - ports:     { port: 5432 }    │
                                                    └──────────────────────────────────┘
  NOTE:  (Much more) detailed info available at
  @[https://kubernetes.io/docs/concepts/services-networking/service/]
   (Section: about kube-proxy + iptables +... advanced config settings)

[[101.service}]]

● Ingress Rules/Controller: [[{network.101.ingress]]
  - Load Balancer      : layer 4, unaware of the actual apps.       [[{02_doc_has.comparative]]
                         persistent session, dynamic weights.
                         (better for non-HTTP like apps)
  - Ingress controllers: layer 7, can use advanced rules based on
                         inbound URL, ..., apply TLS termination    [[{network.TLS,security.TLS}]]
                         removing TLS cert.complexity from Apps
                         (Better for HTTP like apps)                [[}]]

                           Ingress                   ┌············┐
      POST               | controller │              · ┌ node1 -┐ ·┌ node2  ┐
      Host: app1.com ····>····┐    ┌··│··>│ServiceA├·┤ │        │ ·│        │
                         |  ┌······┘  │              └·> │PodA│ │ └> │PodA│ │
      POST               │  · ·       │                │        │  │        │
      Host: app2.com ····>··┘ └··········>│ServiceB├···> │PodB│ │  │        │
                         │            │                │        │  │        │
                         │            │                │        │  │        │
                   external HTTP/s                     └────────┘  └────────┘
                    endpoing


- PRE-SETUP) an ingress controller must be in place: ingress-(linkerd|istio|nginx...)
  **WARN: Different Ingress controllers operate slightly differently. **

  ┌─ app1_tls_secrets.yml ─────────┐                                                    [[{security.tls]]
  │ apiVersion: v1                 │
  │ kind: Secret                   ← We need to setup secrets with private cert. key
  │ type: kubernetes.io/tls        │ The secret will be "injected" to the ingress controller.
  │ metadata:                      │ (nginx, istio,...)
┌→│   name: secretTLSPrivateKey    │
· │   namespace: default           │
· │ data:                          │
· │   tls.key: base64 encoded key  ← Secret to protect
· │   tls.crt: base64 encoded cert ← Not really secret.  Public certificate.
· └────────────────────────────────┘                                                    [[}]]
· ┌─ app1_ingress.yml ────────────────────┐
· │ apiVersion: networking.k8s.io/v1beta1 │
· │ kind: Ingress                         │
· │ metadata:                             │
· │   name: test-ingress                  │
· │   annotations:                        │
· │     nginx.ingress.kubernetes.io/rewrite-target: / ← Annotation used before IngressClass was added
· │                                       │             to define the underlying controller implementation.
· │                                       │             .ingressClassName can replace it now.
· │ spec:                                 │
· │   tls:                                ← Optional but recomended. Only 443 port supported.
· │     - hosts:                          │
· │         - secure.foo.com              ← values must  match cert CNs
· │                                       │
└·│······ secretName: secretTLSPrivateKey │
  │   rules:                              ← A backend with no rules could be use to expose a single service
  │   - host: reddis.bar.com              ← Optional. filter-out HTTP requests not addressing this host
  │     http:                             │
  │       paths:                          ← If 2+ paths in list match input request, longest match "wins".
  │       - path: /reddis                 │
  │         pathType: Prefix              ← Implementation Specific: delegate matching to IngressClass (nginx,...)
  │                                       │ Exact : Matches case-sensitive URL path exactly
  │         backend:                      │ Prefix: Matches case-sensitive based on request URL prefix
  │           serviceName: reddissrv      ← targeted k8s service
  │           servicePort: 80             ← Need if Service defines 2+ ports
  │                                       │
  │   - host: monit.bar.com               ← Optional. filter-out HTTP requests not matching host
  │     http:                             │
  │       - path: /grafana                │
  │         pathType: Prefix              │
  │         backend:                      │
  │           serviceName: grafanasrv     │ ← targeted k8s service
  │       - path: /prometheus             │
  │         ...                           │
  │   - http:                             ← Default service for non-matching/not-defined host
  │       - path: /                       │
  │         pathType: Prefix              │
  │         backend:                      │
  │           serviceName: httpdsrv       │ ← targeted k8s service
  └───────────────────────────────────────┘
[[network.101.ingress}]]

● Controllers [[{101.controllers]]
• ReplicaSet Controller:   [[{101.controllers.replicaset]]
  =====================
  - ensure "N" pod replicas are running simultaneously.
  - Most of the times used indirectly by "Deployments" to
    orchestrate pod creation/deletion/updates.
  - 'Job' controller prefered (vs ReplicaSet) for pods terminating on their own.        [[{02_doc_has.comparative]]
    (batch jobs, cleaning tasks, ...)
  - 'DaemonSet' controller prefered for pods providing a machine-level function.
    (monitoring, logging, pods that need to be running before others pods starts).
    KEY-POINT: DaemonSet PODS  LIFETIME == MACHINE LIFETIMEº                            [[}]]

  ┌─ app1_frontend_repset.yaml  ┐   $ kubectl create -f http:...app1_frontend_repset.yaml
  │ apiVersion: apps/v1         │   · replicaset.apps/frontend created
  │ kind: ReplicaSet            │   $ kubectl describe rs/frontend
  │                             │   · ...
  │                             │   · Replicas:  **3 current / 3 desired**
  │                             │   · Pod Template: ...
  │ metadata:                   │   ·   Containers:
  │   name: frontend            │   ·    php-redis: ...
  │   labels:                   │   ·   Volumes:              <none>
┌·····  app: guestbook          │   · Events:
├·····  tier: frontend          │   · 1stSeen LastSeen ... Reason Message
· │ spec:                       │   ·  1m     1m       ... SuccessfulCreate  Created pod: frontend-qhloh
· │   replicas: 3               <···· Default to 1
· │   selector:                 <···· Affected Pods.
· │     matchLabels:            │
· │       tier: frontend        │
· │     matchExpressions:       │
· │       - {key: tier, operator: In, values: [frontend]}
· │   template:                 <···· Pod template (nested pod schema, removing
· │     metadata:               │                   apiVersion/kind properties -)
· │       labels:               <···· Needed in pod-template (vs isolated pod)
├·····      app: guestbook      │      .spec.template.metadata.labels must match
└·····      tier: frontend      │      .spec.selector
  │     spec:                   │
  │       restartPolicy: Always <·· default/only allowed value
  │       containers:           │
  │       - name: php-redis     │
  │         image: gcr.io/google_samples/gb-frontend:v3
  │         resources:          <·· Always indicate resources               [[{qa.best_practices]]
  │           requests:         │   to help scheduler / autoscalers place in most suitable work-node
  │             cpu: 100m       │   and estimate capacity. affinities/anti-affinities also provide hints
  │             memory: 100Mi   │   the scheduler.
  │           limits:           <·· Limits are risky. Use them only when absolute sure that process must
┌··········>    cpu: 200m       │   never pass such limits in normal conditions.
· │             memory: 200Mi   │
· │             ephemeral-storage: 1G ← k8s v1.8+                          [[}]]
· │         env:                │   $ kubectl get pods
· │         - name: PARAM1      │   · NAME             READY     STATUS    RESTARTS   AGE
· │           value: VALUE1     │   · frontend-9si5l   1/1       Running   0          1m
· │         ports:              │   · frontend-dnjpy   1/1       Running   0          1m
· │         - containerPort: 80 │   · frontend-qhloh   1/1       Running   0          1m
· └─────────────────────────────┘
└··· - 1 cpu is equivalent to:
       1 (AWS vCPU|GCP Core|Azure vCore|IBM vCPU|Hyperthread bare-metal)
     - 200m = 0.2 cpu
[[101.controllers.replicaset}]]


● **Deployment** [[{101.controllers.deployment,application.101]]

 - An application is "blur" term in Kubernetes. "Deployment" object is the    [[{02_doc_has.keypoint]]
   most similar entity mapping to the intuitive concept of "Application".

 - Deployments Adds application lifecycle to ReplicaSet (creation, updates,...)
   e.g: Pod definition changed: Deployment takes care of gradually moving from
        a running (old) ReplicaSet to a new (automatically created) ReplicaSet.
   Deployments also rollback to (old) ReplicaSet if the new one is not stable.
   (or removing old ones otherwise)                                           [[}]]

─ ┌─ app01_deployment.yaml ──────┐
  │ apiVersion: apps/v1beta2     ←  for vers.<1.7.0 use apps/v1beta1
  │ºkind: Deployment º           │    $ kubectl create -f app01_deployment.yaml
  │ metadata:                    │    $ kubectl get deployments
  │   name: app1-deployment      │    → NAME             DESIRED CURRENT  ...
  │   labels:                    │    · nginx-deployment 3       0
  │     app: app1                │
  │ namespace: testing_dept1     ← Namespace can be used to
  │ spec:                        │
  │   replicas: 3                ← 3 replicated Pods
  │   strategy:                  │
  │    - type : RollingUpdate    ← Alt: -type: Recreate (used when new version -of DDBB
  │      rollingUpdate:          │      node, ... is NOT compatible with old one)
  │        {maxSurge:2,maxUnavailable:25%}
  │   selector:                  │
  │     matchLabels:             │
 ┌│···    app: app1              │
 ·│   template:                  ← pod template.
 ·│     metadata:                │ TIP: 1+ 'PodPreset' can be used to apply common presets
 ·│       labels:                │      for different pods (similar mount points, injected
 └│······   app: nginx           │      env vars, ...)
  │     spec:                    ← template pod spec
  │       containers:            │ change triggers new rollout
  │       - name: nginx          │
  │         image: nginx:1.7.9   │   $ kubectl rollout status deployment/nginx-deployment
  │         ports:               │   → Waiting for rollout to finish: 2/3 new replicas
  │         - containerPort: 80  │   · have been updated...
  │                              │   · deployment "nginx-deployment" successfully rolled out
  │        volumes:              ← Volumes and ports are the main I/O "devices"
  │    ┌·· - name: test-vol      │ to read input and write output for containers.
  │    ·     hostPath:           │
  │    ·       path: /data       ← location on host
  │    ·       type: Directory   ← (optional)
  │    · ┌ - name:  cache-vol    │
  │    · ·   emptyDir: {}        ← Remove volume when pod is deleted
  │    · ·                       │
  │    · ·   volumeMounts:       │
  │    · ·   - mountPath: /cache ← where to mount
  │    · └..   name:cache-vol    ← volume (name) to mount
  │    ·     - mountPath: ...    │
  │    └····   name:test-vol     │

  │         livenessProbe:       ← Best pattern. Other options include grpc|exec|tcp
  │           httpGet:           ← Alt.: exec: { command: [cat, /tmp/healthy ] }
  │             path: /heartbeat │ Optional parameters:
  │             port: 80         │ · timeoutSeconds  : (before timing out), def: 1sec
  │             scheme: HTTP     │ · successThreshold: Minimum consecutive "OK"s after failure to consider "OK".
  │                              │ · failureThreshold: num.of "KOs" before "giving up" (Pod marked Unready)
  │         readinessProbe: ...  ← Useful when containers takes a long time to start.
  │       activeDeadlineSeconds: 70  ← Optional. Use it when also using Init-Pods (See behind)
  │                              │   Rºto prevent them form failing forever.º
  │       initContainers:        │
  │       - name: init-from-url1 ← INIT CONTAINERS: 1+ specialized cont. running IN ORDER before normal ones
  │         image: busybox:1.28  │              - They always run to completion, with k8s restarting
  │                              │                them repeatedly until succeed. (if restartPolicy != Never)
  │         command:             │              - Each one must complete successfully before next one start.
  │         - bash               │              - status is returned in .status.initContainerStatuses
  │         - "-c"               │                (vs .status.containerStatuses) (readiness probes do not apply)
  │         - |                  ← Yaml syntax sugar allowing to embed complex scripts,...
  │           set -ex            │
  │           cat << EOF > file1 │
  │           ...                │
  │           EOF                │
  │           ...                │
  │       ─ name: init-from-db   │
  │         image: busybox:1.28  │
  │         command: ['sh', '─c', 'psql ...']
  └──────────────────────────────┘

  $ kubectl get rs                   <·· dump ReplicaSet created by the deployment
  NAME                    DESIRED ...
  app1-deployment-...4211 3
  └──────┬──────┘ └──┬──┘
  deployment-name- pod─tpl-hash

  $ kubectl get pods --show-labels                      <··· Display ALL labels automatically
  → NAME          ... LABELS                                generated for ALL pods
  · app1-..7ci7o ... app=nginx,...,
  · app1-..kzszj ... app=nginx,...,
  · app1-..qqcnn ... app=nginx,...,

  $ kubectl set image deployment/app1-deployment \      <··· Update nginx 1.7.9 → 1.9.1
    nginx=nginx:1.9.1

  $ kubectl rollout history deployment/app1-deployment  <··· Check deployment revisions
  deployments "nginx-deployment"
  R CHANGE-CAUSE
  1 kubectl create -f nginx-deployment.yaml ---record
  2 kubectl set image deployment/nginx-deployment \
                      nginx=nginx:1.9.1
  ...

  $ kubectl rollout undo deployment/app1-deployment \  <··· Rollback to rev. 2
   --to-revision=2

  $ kubectl scale deployment \                         <··· Scale Deployment
    app1-deployment --replicas=10

  $ kubectl autoscale deployment app1-deployment \
    --min=10 --max=15 --cpu-percent=80
[[101.controllers.deployment}]]
[[101.controllers}]]

● Object Management:
- REF:
  - @[https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/]
  - @[https://kubernetes.io/docs/concepts/overview/working-with-objects/object-management/]
  - @[https://github.com/kubernetes/api/blob/master/core/v1/types.go]

   ┌──────────┐                       ┌───────────┐
   │k8s object│                    ┌··→ metadata  │
   ├──────────┤                    ·  ├───────────┤
   │kind      < core/v1/types.go   ·  │name       < maps to /api/v1/pods/name
   │metadata  <····················┘  │UID        < Distinguish between historical
   │spec      < desired state         │namespace  │ occurrences of similar object
   │state     < present state         │labels     │
   └──────────┘                       │annotations│
                                      └───────────┘
                         BºMETADATA IS ORGANIZED AROUND THE CONCEPT OF AN APPLICATION.º
                           K8S DOES NOT ENFORCE A FORMAL NOTION OF APPLICATION.
                           APPS ARE DESCRIBED THORUGH METADATA IN A LOOSE DEFINITION.

● kubectl
                                           (/openapi/v2)
  etcd     ←······ k8s objects ←·········→ kube-apiserver ←···→  $ kubectl $action $resource
  (serialized     Represent API resources      ┌───────────────────────────┴─────┘
   states)        (apps running on nodes,      ├ get     : list resources
                   resources available to a    ├ describe: show details (and events for pods)
                   a given app, policies       ├ logs    : print container logs
                   (restart, upgrades, ...)    ├ exec    : exec command on container
                                               └ apply   : creates and updates resources
                                                 ...
• kubectl pre-setup:
  $ KUBE_EDITOR="vim"
  $ source <(kubectl completion bash)           < autocompletion for bash
  $ source <(kubectl completion zsh)            < autocompletion for bash

  $ KUBECONFIG=${KUBECONFIG}:~/.kube/config     < using multiple kubeconfig files
  $ KUBECONFIG=${KUBECONFIG}:~/.kube/kubconfig2
  $ kubectl config view                         < Show merged kubeconfig settings.

  $ kubectl config current-context
  $ kubectl config use-context my-cluster-name

  $ kubectl run nginx --image=nginx
  $ kubectl explain pods,svc

  $ kubectl edit svc/my-service-1               < Edit resource

  $ kubectl get pods  \                         < filter objects using field selectors.
    --field-selector  spec.restartPolicy=Always   fields depends on each object type/kind.
  $ kubectl get pods \                            'metadata.name' and 'metadata.namespace'
    --field-selector status.phase!=Running,..      are common to all types.

• kubectl common flags :  [[{throubleshooting.101]]
  --all-namespaces
  -o wide
  --include-uninitialized
  --sort-by=.metadata.name
  --sort-by='.status.containerStatuses[0].restartCount'
  --selector=app=cassandra
  --field-selector=....  [[}]]

● Labels/Selectors:
- labels are used to specify ºidentifying attributes of objectsº.
 ºmeaningful and relevant to application concerns/taxonomyº. (vs k8s core system)
  ex labels/app-taxonomies:
  "release"    : "stable"   # := "canary" ...
  "environment": "dev"      # := "qa", "pre",  "production"
  "tier"       : "frontend" # := "backend" "cache"
  "track"      : "daily"    # := "weekly" "monthly" ...

- ºRecommended ("standard") Labelsº
  Key                          |  Description             | Example
  ============================    =======================   ================
  app.kubernetes.io/name       |  app-name                | mysql
  app.kubernetes.io/instance   |  unique ID for instance  | wordpress-abcxzy
  app.kubernetes.io/version    |  app-version             | 5.7.21
  app.kubernetes.io/component  |                          | database
  app.kubernetes.io/part-of    |                          | wordpress
  app.kubernetes.io/managed-by |  tool used to manage ops | helm

- See also recommended labels for Helm charts:
@[https://helm.sh/docs/chart_best_practices/labels/]

- WELL-KNOWN LABEL KEY ANNOTATIONS AND TAINTS
  ===========================================
  kubernetes.io/arch
  kubernetes.io/os
  beta.kubernetes.io/arch (deprecated)
  beta.kubernetes.io/os (deprecated)
  kubernetes.io/hostname
  beta.kubernetes.io/instance-type
  failure-domain.beta.kubernetes.io/region
  failure-domain.beta.kubernetes.io/zone

● Label selectors
- Similar in concept to CSS selectors. They allow to point to k8s objects
  (Pods, services, ...) by matching labels.
- two types of selectors:
  - equility selectors. Ex.:
    environment=production,tier!=frontend  ← "," == "AND"

  - set-based selectors: 'in', 'notin' 'exists'. Ex:
    - 'key equal to environment and value equal to production or qa'
    - 'environment in (production, qa)'
    - 'tier notin (frontend, backend)'
       └───────────┬────────────────┘
       all resources with (key == "tier" AND
                           values != frontend or backend )
       AND
       all resources with (key != "tier")

    - 'partition'   ← all resources including a label with key 'partition'
    - '!partition'  ← all resources without a label with key 'partition'

- LIST and WATCH operations may specify label selectors to filter the sets
  of objects returned using a query parameter. Ex.:
  $ kubectl get pods -l environment=production,tier=frontend
  $ kubectl get pods -l 'environment in (production),tier in (frontend)'
  $ kubectl get pods -l 'environment in (production, qa)'
  $ kubectl get pods -l 'environment,environment notin (frontend)'

● Resource Types
- RESOURCE TYPES SUMMARY:
      clusters            │ podtemplates               │ statefulsets
  (cs)componentstatuses   │ (rs)replicasets            │ (pvc)persistentvolumeclaims
  (cm)configmaps          │ (rc)replicationcontrollers │ (pv) persistentvolumes
  (ds)daemonsets          │ (quota)resourcequotas      │ (po) pods
  (deploy)deployments     │ cronjob                    │ (psp)podsecuritypolicies
  (ep)endpoints           │ jobs                       │ secrets
  (ev)event               │ (limits)limitranges        │ (sa)serviceaccount
  (hpa)horizon...oscalers │ (ns)namespaces             │ (svc)services
  (ing)ingresses          │ networkpolicies            │ storageclasses
                          │ (no)nodes                  │ thirdpartyresources

- RESOURCE TYPES EXTENDED:
  https://github.com/kubernetes/api/blob/master/core/v1/types.go
  https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/types.go



● k8s (def.)TCP Ports
REF @[https://kubernetes.io/docs/tasks/tools/install-kubeadm/]
                                       ┌───────┬───────┐
                                       │Master │Worker │
                                       │node(s)│node(s)│
┌──────────────────────────────────────┼───────┼───────┤
│Port Range   │ Purpose                │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│6443*        │ Kubernetes API server  │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│2379─2380    │ etcd server client API │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│10250        │ Kubelet API            │   X   │  X    │
├──────────────────────────────────────┼───────┼───────┤
│10251        │ kube─scheduler         │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│10252        │ kube─controller─manager│   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│10255        │ Read─only Kubelet API  │   X   │  X    │
├──────────────────────────────────────┼───────┼───────┤
│ 30000─32767 │ NodePort Services      │       │  X    │
└──────────────────────────────────────┴───────┴───────┘
[[}]]


● kube-proxy Service setup @ kube-proxy [[{network.service]] #[kube-proxy_summary]

  kube-proxy, installed on each k8s working node, get in charge of the network 'magic'.
  It is more reliable than DNS (no problems with TTLs, client DNS cache,...).

  ┌─ Service registry seq USER-SPACE PROXY MODE ("legacy mode") ──────────────────┐
  │ master →     kube─proxy: Event  +Service.EndPoint                             │
  │ kube─proxy → kube─proxy: Open Random Port 01 and setup round─robin from       │
  │                          random─port 01 to ºall existingº Backend Pods        │
  │                          matching the Service selector.                       │
  │ kube─proxy → iptables  : Install iptables rule in kernel to capture traffic to│
  │                          (virtual) clusterIP:port to Random Port 01           │
  │ ...                                                                           │
  │ app        → kernel    : request to clusterIP:port                            │
  │ kernel     → iptables  : request to clusterIP:port                            │
  │ iptables   → kube─proxy: request to random port                               │
  │ kube─proxy → kube─proxy: Round─robin amongst all availables Pods.             │
  │                          (The kube─proxy keeps a list of live─Pods matching   │
  │                           Service.Selector using also SessionAffinity for     │
  │                           better balancing)                                   │
  │ kube─proxy → Pod N     : request                                              │
  └───────────────────────────────────────────────────────────────────────────────┘
  ┌─ Service registry seq IPTABLES PROXY MODE ────────────────────────────────────┐
  │ master →     kube─proxy: Event +Service.EndPoint                              │
  │ kube─proxy → iptables  : Install iptables rule in kernel to capture traffice to
  │                          (virtual) clusterIP:port to some Pod_i(and just one) │
  │                          from all Pods matching the Service.selecotr          │
  │ ...                                                                           │
  │ app        → kernel    : request to clusterIP:port                            │
  │ kernel     → iptables  : request to clusterIP:port                            │
  │ iptables   → Pod i     : request  (Request fails if pod is down, readiness    │
  │                          probes must be set to skip faulty Pods               │
  └───────────────────────────────────────────────────────────────────────────────┘
  ┌─ Service registry seq IPVS PROXY MODE ────────────────────────────────────────┐
  │  PRE─SETUP: IPVS module must be available on the node.                        │
  │                                                                               │
  │  master     → kube-proxy: Event +Service.EndPoint                             │
  │  kube─proxy → netlink   : Create IPVS rules (sync status periodically)        │
  │                           to all Pods in Selector (allowing for balancing)    │
  │  app        → kernel    : request to clusterIP:port                           │
  │  kernel     → netlink   : request to clusterIP:port                           │
  │  netlink    → Pod i     : request                                             │
  └──^────────────────────────────────────────────────────────────────────────────┘
   netlink allows next balancing options:
   · rr: round-robin                 · sh: source hashing
   · lc: least connection (smallest  · sed: shortest expected delay
         number of open con.)        · nq: never queue
   · dh: destination hashing
[[}]]

● POD MONITORING 101 [[{101,troubleshooting.101,01_PM.low_code,02_doc_has.diagram.decission_tree,101.kubectl,application]] #[k8s_troubleshooting_summary]
  $ kubectl get pods                           # <·· List pods (in default namespace)
  $ kubectl get pod my-pod                     # <·· ensure pod is running
  $ kubectl top pod POD_NAME --containers      #
  $ kubectl logs my-pod (-c my-container) (-f) # <·· Get logs (-f) to follow "tail"
  $ kubectl attach my-pod -i                   #
  $ kubectl port-forward my-pod 5000:6000      # <·· local-machine-port:pod-port
  $ kubectl run -it  busybox \                 # <·· Exec. (temp. pod) shell for OCI image
            --image=busybox -- sh              # <·· -t: create tty, -i: Interactive
  $ kubectl exec my-running-pod -it \          # <·· Get a shell inside already-running Pod
           -- /bin/bash                        # <·· -c my-container needed if Pod has 2+ containers
  $ kubectl exec my-pod env                    # <·· Dump all ENV.VARs inside container
                                                     Very useful to see Services that were "UP"
                                                     at container startup.
[[}]]

● "APPLICATION" TROUBLESHOOTING [[{troubleshooting.101]]
REF: @[https://learnk8s.io/]

- @[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/]

  On a separate (tmux)window monitor kubectl resources ike:
  $  watch -n 4 "kubectl get pods,services,... -n $namespace"

  $ kubectl get pods
    └─────┬────────┘
QºQ: Is there any pending Pod?º
  └─────────┬────────────────┘
  ┌─────────┘
  ├ YES >   $ kubectl describe   < Q: Is the cluster full?
  │              pod $pod_name     └────────┬────────────┘
  │           ┌─────────────────────────────┘
  │           ├ NO  >  Q: Are you hitting ResourceQuotaLimits?
  │           │        └────────────────────┬────────────────┘
  │           │       ┌─────────────────────┘
  │           │       ├ NO  >  Q: Are you mounting a PENDING
  │           │       │           PersistentVolumeClaim?
  │           │       │          (kubectl describe pod $pod will show an even
  │           │       │           "pod has unbound immediate PersistenceVolumeClaim")
  │           │       │        └───────────┬────────────────┘
  │           │       │       ┌────────────┘
  │           │       │       ├ NO  >  $ kubectl get pods     Q: Is the Pod assigned
  │           │       │       │            -o wide               to the Node?
  │           │       │       │                               └────┬───────────────┘
  │           │       │       │      ┌─────────────────────────────┘
  │           │       │       │      ├ YES >  There is an issue with the Kubelet
  │           │       │       │      │
  │           │       │       │      └ NO  >  There is an issue with the Scheduler
  │           │       │       │
  │           │       │       └ YES >  Fix the PersistentVolumeClaim
  │           │       │
  │           │       └ YES >  Relax Quota Limits
  │           │
  │           └ YES > Q: Are nodes "OK"  <·$ kubectl get nodes             [[{cluster_admin]]
  │             ┌─────┴───────────────┘      NAME          STATUS   ROLES    ....
  │             ├  NO > Fix Node/s           master-node   Ready    master,control-plane,...
  │             │                            worker-node01 Ready    worker
  │             │                            worker-node02 Ready    worker [[cluster_admin}]]
  │             └ YES > Add worker node      ...           ^^^^^
  │                                          "Node Problem Detector" DaemonSet helps to monitor
  │                                          node's health
  │
  └ NO  >  Q: Are the Pods Running?
           └──────┬───────────────┘
  ┌───────────────┘
  ├ NO  →  $ kubectl logs $pod_name ←    Q: Can you see the logs
  │        ↑                                for the App?
  │        │                             └─────────┬───────────┘
  │        │  ┌────────────────────────────────────┘
  │        │  ├ Yes >  Fix the issue in the App
  │        │  └ NO  >  Q: Did the container died too Quickly?
  │        │           └──────┬─────────────────────────────┘
  │        │      ┌───────────┘
  │        │      ├ NO  >   $ kubectl describe     Q: Is the Pod in status
  │        │      │             pod $pod_name         ImagePullBackOff?
  │        │      │                                └──┬──────────────────┘
  │        │      │  ┌────────────────────────────────┘
  │        │      │  ├ NO  >  Q: Is the Pode Status CrashLoopBackOff?
  │        │      │  │        └─────────┬─────────────────────────┘
  │        │      │  │ ┌────────────────┘
  │        │      │  │ ├ NO >  Q: Is the Pod status RunContainerError?
  │        │      │  │ │       └─────────┬───────────────────────────┘
  │        │      │  │ │ ┌───────────────┘
  │        │      │  │ │ ├ NO  >  Consult StackOverflow
  │        │      │  │ │ │
  │        │      │  │ │ └ YES >  The issue is likely to be with
  │        │      │  │ │          mounting volumes
  │        │      │  │ │
  │        │      │  │ └ YES >  Q: Did you inspect the logs and fixed the crashes?
  │        │      │  │            $ kubectl logs --previous $POD_NAME
  │        │      │  │          (--previous: See logs of chrased pod)
  │        │      │  │          └─────────┬─────────────────────────┘
  │        │      │  │    ┌───────────────┘
  │        │      │  │    ├ NO  >  Fix the app crahses
  │        │      │  │    │
  │        │      │  │    └ YES >  Q: Did you forget the 'CMD' instruction
  │        │      │  │                in the Dockerfile?
  │        │      │  │             └──────────┬──────────────────────────┘
  │        │      │  │       ┌────────────────┘
  │        │      │  │       ├ YES >  Fix the Dockerfile
  │        │      │  │       │
  │        │      │  │       └ NO  >  Q: Is the Pod restarting frequently?
  │        │      │  │                   Cycling between Running and
  │        │      │  │                   CrashLoopBackoff?
  │        │      │  │                └──────────┬────────────────────────┘
  │        │      │  │        ┌──────────────────┘
  │        │      │  │        ├ YES >  Fix the liveness probe
  │        │      │  │        │
  │        │      │  │        └ NO  >  Unknown State
  │        │      │  │
  │        │      │  └ YES >  Q: Is the name of the image correct?
  │        │      │           └────┬─────────────────────────────┘
  │        │      │   ┌────────────┘
  │        │      │   ├ NO  >  Fix the image name
  │        │      │   │
  │        │      │   └ YES >  Q: Is the image tag valid?
  │        │      │               Does it exists?
  │        │      │            └──┬──────────────────────┘
  │        │      │   ┌───────────┘
  │        │      │   ├ NO  >  Fix the tag
  │        │      │   │
  │        │      │   └ YES >  Q: Are you pulling images from a
  │        │      │               private registry?
  │        │      │            └─────────┬────────────────────┘
  │        │      │    ┌─────────────────┘
  │        │      │    ├ NO  >  The Issue could be with CRI|Kubelet
  │        │      │    │
  │        │      │    └ YES >  Configure pulling images from a
  │        │      │             private registry
  │        │      │
  │        │      └ YES >   $ kubectl logs $pod_name --previous
  │        │                  └────────────┬──────────────────┘
  │        └───────────────────────────────┘
  └ YES >  Q: Are ther Pods READY?
           └──┬──────────────────┘
  ┌───────────┘
  ├ NO  >   $ kubectl describe  <  Q: Is the Readiness probe
  │             pod $pod_name         failing?
  │                                └──┬────────────────────┘
  │           ┌───────────────────────┘
  │           ├ YES >  Fix the Readiness Probe
  │           │
  │           └ NO  >  Unknown State
  │
  └ YES >   $ kubectl port-forward      \   <  Q: Can you access the app?  *1
                $pod_name  8080:$pod_port      └────────────┬───────────┘
                                                            │
      *1 TIP: Test with a command similar to   $ wget localhost:8080/...
         if error "... error forwarding port 8080 to pod 1234..."
         is displayed check that pod 1234... is the intended one by
         executing '$ kubectl describe pods' and checking pod number
         is correct. If it isn't, delete and recreate the service.
         try again until to see if '$ wget ...' works. Continue next
         checks otherwise.)                                 │
                                                            │
  ┌─────────────────────────────────────────────────────────┘
  └   Q: Do you have 2+ different deployments with colliding selector names?
         (this can be the case in "complex" Apps composed of N deployments)
         └──────────────────────┬─────────────────────────────────────────┘
  ┌─────────────────────────────┘
  ├ YES: Fix one or more deployments to avoid colliding selectors
  │      For example if two deployments have a selector labels like
  │      'app: myApp' split into 'app: myApp-ddbb' and 'app: myApp-frontend' or
  │      selector 1:     selector 2:
  │      'app: myapp'    'app: myapp'
  │      'layer: ddbb'   'layer: frontend'
  │      update both on Deployment and related services
  │
  ├ NO  →  Q: Is the port exposed by container correct
  │           and listening on 0.0.0.0?
  │           You can check it like: (-c flag optional for 1 container pods)
  │        $ kubectl exec -ti $pod -c $container -- /bin/sh
  │        # netstat -ntlp
  │        └────────────────────┬────────────────────┘
  │       ┌─────────────────────┘
  │       ├ NO  >  Fix the app. It should listen on
  │       │        0.0.0.0.
  │       │        Update the containerPort
  │       │
  │       └ YES >  Unknown State
  │               Try debugging issues in cluster:
  │              $ SELECTOR=""
  │              $ SELECTOR="${SELECTOR},status.phase!=Running"
  │              $ SELECTOR="${SELECTOR},spec.restartPolicy=Always"
  │              $ kubectl get pods --field-selector=${SELECTOR} \  ← Check failing pods in
  │                -n kube-system                                     kube-system namespace

  ↓
  YES
  ↓
  ===========================
  =POD ARE RUNNING CORRECTLY=
  ===========================
   └──────────┬────────────┘
              ↓
  $ kubectl describe   \    <  Q: Can you see a list of endpoints?
    service $SERVICE_NAME      └────────────┬────────────────────┘
  ┌─────────────────────────────────────────┘
  ├ NO  >  Q: Is the Selector matching the right Pod label?
  │        └──┬───────────────────────────────────────────┘
  │     ┌─────┘
  │     ├ NO  > Fix the Service selector to match targeted-Pod labels
  │     │
  │     └ YES > Q: Does the Pod have an IP address assigned?
  │             └──┬───────────────────────────────────────┘
  │           ┌────┘
  │           ├ NO  > There is an issue with
  │           │       the Controller Manager
  │           │
  │           └ YES > There is an issue with the Kubelet
  │
  └ YES >  $ kubectl port-forward    \     Q: Can you visit the app?
               service/$SERVICE_NAME \     └──┬────────────────────┘
               8080:$SERVICE_PORT             │
                                              │
  ┌───────────────────────────────────────────┘
  ├ NO  >  Q: Is the targetPort on the Service
  │           matching the containerPort in the
  │           Pod?
  │        └──┬───────────────────────────────┘
  │     ┌─────┘
  │     ├ NO  > Fix the Service targetPort and
  │     │     > the containerPod
  │     │
  │     └ YES > The issue could be with Kube Proxy
  ↓
 YES
  ↓
  ==============================
  =SERVICE IS RUNNING CORRECTLY=
  ==============================
  ↓
  $ kubectl describe    \     Q: Can you see a list of Backends?
    ingress $INGRESS_NAME     └──┬─────────────────────────────┘
  ┌──────────────────────────────┘
  ├ NO  >  Q: Are the serviceName and servicePort
  │           mathcing the service?
  │        └──┬─────────────────────────────────┘
  │       ┌───┘
  │       ├ NO  > Fix the ingress serviceName and servicePort
  │       │
  │       └ YES > The issue is specific to the Ingress Controller
  │               Consult the docs for your Ingress
  ↓
 YES
  ↓
  **********************************
  *THE INGRESS IS RUNNING CORRECTLY*
  **********************************
  (The app should be working now!!!)
  |
  v                         ┌ NO  >  The issue is likely to be with the
                            │        Infrastructure and how the cluster is
  Q:  Can you visit app  → ─┤        exposed
      from the Internet?    │
                            │       =========
                            └ YES > =**END**=
                                    =========
[[troubleshooting.101}]]

● HELM Charts [[{application.helm,101,application,]] @[helm_charts_summary]
  Package manager for Kubernetes with versioning, upgrades and rollbacks.


@[https://helm.sh/docs/intro/quickstart/]

  PRE-SETUP) "kubectl" installed locally.

  INITIAL-SETUP)
  - Download from @[https://github.com/helm/helm/releases]
    and add "helm" command to path.

• HELM "DAILY" USSAGE

  $ helm get -h
  $ helm repo add stable \                  <·· Add repo with name "stable"
    https://kubernetes-charts.storage.googleapis.com/
  $ helm repo add brigade \                 <·· Add another repo
    https://brigadecore.github.io/charts
  $ helm search repo stable                 <·· alt 1: search charts available in repo named 'stable'
  > NAME                          CHART     APP         DESCRIPTION
  >                               VERSION   VERSION
  > stable/acs-engine-autoscaler  2.2.2     2.1.1       DEPRECATED Scales worker nodes within agent pools
  > stable/aerospike              0.2.8     v4.5.0.5    A Helm chart for Aerospike in Kubernetes
  > stable/airflow                4.1.0     1.10.4      Airflow is a platform to programmatically autho...
  > ...

  $ helm search hub wordpress               <·· alt 2: search charts in "ARTIFACT HUB" (dozens of different repos).
  > URL                                                 CHART     APP     DESCRIPTION
  >                                                     VERSION   VERSION
  > https://hub.helm.sh/charts/bitnami/wordpress        7.6.7     5.2.4   Web publishing ...
  > https://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3    v0.6.3  Presslabs WordPress Operator ....
  > ...

  $ helm repo update                          <··  Make sure we get the latest list of charts (sort of "apt update")
  $ helm show values stable/mysql             <··  Show what params can be customized during install
  $ editor custom01.yaml
  { mysql.auth.username: ...  }

┌>$ helm install -f custom01.yaml \           <·· Install MySQL (it can take a several minutes in some cases)
·        -values custom01.yaml                    <·· (optional) custom values (overwrite defaults)
·        --set   a[0].b=v1,b.c=v1                 <·· (optional) custom values (overwrite --values and/or defaults)
·        --set   name={a, b, c}                   <·· (optional) custom values list
·        --wait                                   <·· (optional)  Waits for: [min. num of Pods in "ready state", PVCs bound]
·        --timeout: 0m20s                         <·· (optional, def: 5m0s)
·        stable/mysql --generate-name
·
·
· $ helm show chart stable/mysql              ← get an idea of the chart features
· $ helm show all   stable/mysql              ← get all information about chart
·
└ Whenever an install is done, a new local-release is created
  allowing to install multiple times into the same cluster.
  Each local-release can be independently managed and upgraded.
  To list local released:
  $ helm ls --all
  NAME             VERSION   UPDATED        STATUS    CHART
  smiling-penguin  1         Wed Sep 28...  DEPLOYED  mysql-0.1.0
  └──────┬──────┴─················································ local-release == running instance of chart
  Available through INMUTABLE local DNS name:
  miling-penguin.default.svc.cluster.local (port 3306)
                 └─────┴································· (or any active namespace)
  $ kubectl get svc -w smiling-penguin      <·· get service  status("INMUTABLE VIEW" of App -moving pods)

  $ helm uninstall smiling-penguin \        <·· Uninstall local-release
  $      --keep-history                     <·· Allows to rollback deleted install
  Removed smiling-penguin
  $ helm status smiling-penguin
  Status: UNINSTALLED                       <·· works only with '--keep-history'
  $ helm rollback smiling-penguin 1         <···┘

@[https://helm.sh/docs/intro/using_helm/]

NOTE: @[https://www.infoworld.com/article/3541608/kubernetes-helm-gets-full-cncf-approval.html]



  ############################
• # Creating new HELM Charts #
  ############################
  @[https://www.youtube.com/watch?v=3GPpm2nZb2s]
  @[https://helm.sh/docs/developing_charts/]

    $ helm create myapp    ← alt 1: Create from helm template
    myapp/             ← template layout
    ├─ charts/         ← complex apps can consists of many "parallel" charts.
    │                    (front-end service/s, backend/s, middleware, cache, ...)
    │
    │                    Alternatively use requirements.yaml like:
    │                    $ cat requirements.yaml
    │                    | dependencies:
    │                    |   - name: apache
    │                    |     version: 1.2.3
    │                    |     repository: http://repo1.com/charts  ← Pre-Setup:
    │                    |   - name: mysql                            $ helm repo add ...
    │                    |     version: 4.5.6
    │                    |     repository: http://repo2.com/charts
    │                    $ helm dependency update  ← Download deps. to charts/
    │
    ├─ Chart.yaml      ← Sort of chart metadata
    │  ▶│apiversion: v1  ┌Change to 0.2.0 then :
    │   │name: my-nginx  │  $ helm upgrade  my-nginx .   ← Upgrade  0.1.0 → 0.2.0
    │   │version: 0.1.0 ←┘  $ helm rollback my-nginx 1   ← Rollback to rev 1 (0.1.0)
    │   │                   $ helm rollback my-nginx 2   ← Rollback to rev 2 (0.2.0)
    │   │appVersion: 1.0-SNAPSHOT (e.g: we can have different type of helm
    │   │                          deployments (versions) with different types of
    │   │                          replica instances for the same web application)
    │   │description: "chart -vs app- description"
    │   │...  ← eubeVersion, maintainers, template engine, deprecated, ...
    │
    ├─ values.yaml     ← Default values for parameters in yaml templates
    │                    (ports, replica counts, max/min, ...) to be replaced as
    │                    {{ .Values.configKey.subConfigKey.param }}.
    │                    They can be overloaded at install/upgrade like:
    │                    $ helm install|upgrade ... --set configKey.subConfigKey.param=...
    └─ templates
       ├─ deployment.yaml  ← Can be created manually like:
       │                     $ kubectl create deploy nginx --image nginx \
       │                       --dry-run -o yaml > deployment.yaml
       │ 
       ├─ service.yaml     ← Can be created manually like:
       │                     $ kubectl expose deploy mn-nginx -port 80   \
       │                       --dry-run -o yaml > service.yaml
       │ 
       ├─ _helpers.tpl     ← files prefixed with '_' do not create k8s output
       │                     Used for reusable pieces of template
       ├─ hpa.yaml
       ├─ ingress.yaml
       ├─ NOTES.txt
       ├─ serviceaccount.yaml
       └─ tests
          └── test-connection.yaml

-ºDEBUGGING TEMPLATE/s:
  $ helm lint                            ← verify best-practices
  $ helm install --dry-run --debug myApp
  $ helm template --debug

- INSTALLING THE CHART:                 Install from current dir. (vs URL / Repo)
  $ helm install --name my-nginx .   ←  TIP: Monitor deployment in real time in another
                                        console like: $ watch -n 2 "kubectl get all"
  $ helm delete --purge my-nginx     ← Clean-up install

● Helm Operator:
@[https://docs.fluxcd.io/projects/helm-operator/en/1.0.0-rc9/references/helmrelease-custom-resource.html]

- operator watching for (Custom Resource) HelmRelease change events (from K8s).
  It reacts by installing or upgrading the named Helm release.
  (See oficial dock for setup/config params)


    apiVersion: helm.fluxcd.io/v1
   ºkind: HelmReleaseº             ← Custom resource
    metadata:
      name: rabbit
      namespace: default
    spec:
    · releaseName: rabbitmq        ← Automatically generated if not provided
    · targetNamespace: mq          ← same as HelmRelease project if not provided.
    · timeout: 500                 ← Defaults to 300secs.
    · resetValues: false           ← Reset values on helm upgrade
    · wait: false                  ← true: operator waits for Helm upgrade completion
    · forceUpgrade: false          ← true: force Helm upgrade through delete/recreate
    · chart:                       ← alt 1: chart from helm repository
    ·   repository: https://charts....     ← HTTP/s and also S3 / GCP Storage through extensions
    ·   name: rabbitmq
    ·   version: 3.3.6
    · # chart:                       ← alt 2: chart from git repository: Repo will be cloned
    · #                                       - git pooled every 5 minutes (See oficial doc. to
    · #                                         skip waiting)  *1
    · #
    · #   git: git@github.com:fluxcd/flux-get-started
    · #   ref: master
    · #   path: charts/ghost
    · values:                      ← values to override on source chart
    ·   replicas: 1
    · valuesFrom:                  ← Secrets, config Maps, external sources, ...
    · - configMapKeyRef:
    ·     name: default-values     ← mandatory
    ·     namespace: my-ns         ← (of config. map) defaults to HelmRelease one
    ·     key: values.yaml         ← Key in config map to get values from
    ·                                (defaults to values.yaml)
    ·     optional: false          ← defaults to false (fail-fast),
    ·                                true=> continue if values not found
    · - secretKeyRef:
    ·     name: default-values         ← mandatory
    ·     namespace: my-ns             ← (Of secrets) defautl to HelRelease one
    ·     key: values.yaml             ← Key in the secret to get the values from
    ·                                    (defaults to values.yaml)
    ·     optional: false          ← defaults to false (fail-fast),
    ·                                true=> continue if values not found
    ·  - externalSourceRef:
    ·      url: https://example.com/static/raw/values.yaml
    ·     optional: false          ← defaults to false (fail-fast),
    ·                                true=> continue if values not found
    ·
    · rollback:                    ← What to do if new release fails
        enable: false              ← true : perform rollbacks for releas.
        force: false               ← Rollback through delete/recreate if needed.
        disableHooks: false        ← Prevent hooks from running during rollback.
        timeout: 300               ← Timeout to consider failure install
        wait: false                ← true => wait for min.number of Pods, PVCs, Services
                                             to be ready state before marking release as
                                             successful.


   $ kubectl delete hr/my-release  ← force reinstall Helm release
                                     (upgrade fails, ... )
                                     Helm Operator will receive delete-event and force
                                     purge of Helm release. On next Flux sync, a new Helm Release
                                     object will be created and Helm Operator will install it.


   *1 : Git Authentication how-to:
        - Setup the SSH key with read-only access ("deploy key" in GitHub)
          (one for each accessed repo)

        - Inject each read-only ssh key into Helm Release operator
          by mounting "/etc/fluxd/ssh/ssh_config" into the container
          operator and mounting also (as secrets) each referenced
          priv.key in "/etc/fluxd/ssh/ssh_config"


- About helm-charts pod templates:
@[https://helm.sh/docs/chart_best_practices/pods/]
"...A container image should use a fixed tag or the SHA of the image. It
 should not use the tags latest, head, canary, or other tags that are
 designed to be "floating"..."

● Kubeapps:
- Application Dashboard for Kubernetes
@[https://kubeapps.com/]
  - Deploy Apps Internally:
    Browse Helm charts from public or your own private chart repositories
    and deploy them into your cluster.
  - Manage Apps:
    Upgrade, manage and delete the applications that are deployed in your
    Kubernetes cluster.
  - Service Catalog:
    Browse and provision external services from the Service Catalog and
    available Service Brokers.
[[}]]



● ServiceAccount [[{security.101,security.aaa]]
- Useful for k8s internal pods (and knative apps?) that want to
  interact with k8s apiserver.
  Running processes (at pod-containers) are authenticated trough
  a given (namespaced) Service Account:
- a 1-hour-expiration-token for API access is provided through
  an "injected" mounted volumeSource added to each pod container
  @ '/var/run/secrets/kubernetes.io/serviceaccount'
  (can be disabled with automountServiceAccountToken)
  note:
     kube-controller-manager --service-account-private-key-file ← set priv. sign.key
     kube-apiserver          --service-account-key-file         ← pub.key

      $ kubectl get serviceAccounts
      NAME      SECRETS    AGE
      default   1          1d   ← service-account controller ensures  it exists in
      ...                         every active namespace.

     $ kubectl apply -f - <<EOF  ← Creating new ServiceAccount:
     apiVersion: v1                (by k8s cluster-namespace-admin)
     kind: ServiceAccount
     metadata:
       name: build-robot
     EOF

- To create additional API tokens:
    kind: "Secret"
    type: "kubernetes.io/service-account-token"
    apiVersion": "v1"
    metadata":
      name: secretForTaskN
      annotations:
        kubernetes.io/service-account.name : "myserviceaccount"
                                              └───────┬──────┘
                                         reference to service account
[[}]]

● DaemonSet [[{application.daemonset,01_PM.TODO]]
- DaemonSet controller ensures "N" Pods are running in a working node.
- typical uses: cluster storage, log collection, monitoring
  - Ex (simple case): one DaemonSet, covering all nodes, would be used
    for each type of daemon. A more complex setup might use multiple DaemonSets
    for a single type of daemon, but with different flags and/or different memory
    and cpu requests for different hardware types.

Ex.  DaemonSet for fluentd-elasticsearch:
  ┌─ app1_logging_daemonset.yaml ────────┐
  │ apiVersion: apps/v1                  │
  │*kind: DaemonSet *                    │
  │ metadata: ...                        │
  │ spec:                                │
  │   selector: ...                      ← Must be a pod-selector
  │   template:                          ← Pod Template RestartPolicy MUST BE 'Always'
  │     metadata: ...                    │ (default if un-specified)
  │     spec:                            │
  │       nodeSelector: ...              ← ALlows to run only on subset of nodes (vs all)
  │       affinity:     ...              ← Will ron only on nodes matching affinity.
  │       terminationGracePeriodSeconds: 30
  │       tolerations:                   │
  │       - key: node-role.kubernetes.io/master
  │         effect: NoSchedule           │
  │       volumes: ...                   │
  │       containers:                    │
  │       - ...                          │
  └──────────────────────────────────────┘

Daemon Pods do respect taints and tolerations,
but they are created with NoExecute tolerations
for the following taints with no tolerationSeconds:
    node.kubernetes.io/not-ready
    node.alpha.kubernetes.io/unreachable

The following NoSchedule taints are respected:
    node.kubernetes.io/memory-pressure
    node.kubernetes.io/disk-pressure

Pods in DaemonSet can also be marked as critical.

- It is possible to run daemon processes using init or  systemd.
  DaemonSet allows to manages them as standard k8s apps.
[[}]]

● Jobs [[{101.jobs,application]]
- (reliably) run 1+ Pod/s to "N" completions, managing Pods that are
  expected to terminate ("batch jobs") (vs Deployments long-running services).

  ┌─ compute_job.yaml ────────┐ Run like:
  │ apiVersion: batch/v1      │   $ kubectl create -f ./compute_job.yaml   ← Run it
  │ kind:  Job                │
  │ metadata:                 │   $ kubectl describe jobs/pi   ← Check status
  │   name: pi                │   (Parallelism, Completions, Events, ...)
  │ spec:                     │
  │   backoofLimit: 10        ← (Optional, def: 6) Fail after N retries
  │   activeDeadlineSeconds   ← (Optional)
  │   template:               ← associated Pod template
  │     spec:                 │
  │       containers:         │   $ pods=$(kubectl get pods --selector=job-name=pi \
  │       - name: pi          │     --output=**jsonpath={.items..metadata.name}**)
  │         image: perl       │
  │         command: [..,..,] │ - Pods are NOT deleted on completion to allow
  │         - python          │   inspecting logs/output/errors.
  │         - -c              │   '$ kubectl get pods -a' will show them.
  │         - "...."          │
  │         restartPolicy: Never
  │   backoffLimit: 4         │
  └───────────────────────────┘

- Parallel Jobs: Run parallel job with a fixed completion-count
  - job is complete when there is 1-successful-pod for each value
     in the range 1 to .spec.completions.
  - pods must coordinate with themselves or external service to determine
    what each should work on.
  - each pod is independently capable of determining whether or not all its peers
    are done, thus the entire Job is done.
  - For Non-parallel job, leave both .spec.completions and .spec.parallelism unset.
  - Actual parallelism (number of pods running at any instant) may be more or less
    than requested parallelism, for a variety of reasons

- ┌─ cronjob.yaml (v 1.8+) ────────────┐   Alternatively:
  │ apiVersion: batch/v1beta1          │   $ kubectl run hello \
  │ kind: CronJob                      │       --schedule="*/1 0 0 0 0"  \
  │ metadata:                          │       --restart=OnFailure \
  │   name: hello                      │       --image=busybox \
  │ spec:                              │       -- /bin/sh -c "date;"
  │   schedule: "*/1 0 0 0 0"          │
  │   jobTemplate:                     │   $ kubectl get cronjob hello <·· Monitor
  │     spec: ...                      │   $ kubectl get jobs --watch  <·· Watch for job Creat.
  └────────────────────────────────────┘

- TODO:
  - https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/
  - https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/
[[101.jobs}]]

Storage [[{ $storage ]]
● Storage 101 [[{storage.101,application,cloud.storage,01_PM.TODO,]]
REF:@[https://www.youtube.com/watch?v=OulmwTYTauI]

     STEP 0)      ╶╶╶╶╶╶▷ STEP 1)          ╶╶╶▷ STEP 2)     ╶╶╶╶╶╶╶╶╶▷ STEP 3)
     cluster admins       cluster admins        App.Dev                App.Dev
     ==============       ==============        =======                =======
     Add storage          Add PV (storage       Add PVC                Link PVC to Volumes
     "hardware" (or       I/O resource applying                         and Volumes to
     NFS, Cloud,..)       to whole cluster.                             Mount Points@Containers

                                               once underlying
                                               storage has been
                                               assigned to a pod
        ┌···plugin is one of ····←┐              PV is bound to PVC
    ┌───↓──────────────┐    ┌Persistent ┐      one-to-one relation
    │-NFS: path,srvDNS │    │ Volume(PV)│···┐ ┌───────────────────┐   ┌ Container@Pod┐
    │-AWS: EBS|EFS|... │    └───────────┘   · │ Persistent  *1    ←·┐ │              │
    │-Azure:...        │                    · │ Volume            │ · │ - Vol.Mounts │
    │-...              │    ┌ Local ────┐   · │ Claim (PVC)       │ · │   /foot      │
    └ NETWORK ATTACHED ┘  ┌·· Persistent····┤ │                   │ · │              │
      STORAGE             · │Volume(LPV)│   · │  100Gi            │ · │ ┌───────────┐│
                          · └── GA 1.14+┘   └··· Selector         │ · │ │ Volumes:  ││
    ┌──────────────┐      ·          *2     ┌··· StorageClassName │ └···· ─PVC      ││
    │ eNVM,SSD     ·······┘                 · └───────────────────┘   │ │ ─claimName││
    └─ PCI/CPU-BUS ┘          ┌─Storage ─┐  ·                         │ └───────────┘│
  ▶ Local disk/s requires     │ Class(SC)│··┘                         └──────────────┘
    LocalPV(vs PV) since      └──────────┘
    it also affects Pod      (L)PV LIFESPAM: ◁ ─ ─ ─ ─ ─ ─ ─ ─ ─ ▷     VOLUME LIFESPAM:
    allocation scheduling    that of cluster                           that of Pod

  TIP:  Sometimes, it is useful to share one volume for multiple uses in a single pod.
  'volumeMounts.subPath' can be used to specify a sub-path inside the volume (vs root).

   *1  - Similar to how Pods can request/limit CPU and Memory, PVClaims can request
         specific size and access modes (mount once read/write, many-times read-only, ...)
       - PV contains "maxsize", PVC contains "minsize" with PV.maxsize > PVC.minsize

 *2 Local Persistent Volume:
  - REF: @[https://kubernetes.io/blog/2019/04/04/kubernetes-1.14-local-persistent-volumes-ga/]
    Authors: Michelle Au (Google), Matt Schallert (Uber), Celina Ward (Uber)
  - LPV EVERAGES HIGH PERF./DIRECTLY-ATTACHED DISKS (PCI vs network)
  - Preferred for applications handlin data replication themself.
    (software defined storage, replicated databases, blockchains, kafka,
     Cassandra, ...).
    Discourages for other types of apps.PVs takes care of replication.

  - "SOCRATIC DIALOGUE":
    Q: "hostPath" already allows to use local disk as a storage for Pods.
       why SHOULD we use Local Persistent Volume instead?

    A: With Local PV, the Scheduler is aware of the Local PV, so on Pod
       re-start, execution will be assigned to the same worker node.
       With hostPath, k8s can re-schedule in a different node, loosing all
       previously stored data.

      WARN: DYNAMIC VOLUME PROVISIONING NOT SUPPORTED.
      (@[https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner]
       can be used to help manage the Local PV lifecycle - create,clean up,reuse -
       for individual disks),

  - manual PRE-SETUP steps to do:
    1) Cluster-admin: Pre-partition, format and mount disks to nodes (independently
       of k8s, using standard OS utilities).
       NOTE 1: raw block device support, used by Oracle,... is also provided.
               bypassing OS file-system and gaining in performance.
       NOTE 2: K8s is also able to format the block device on demand, avoiding
               manual formating. (any gain?)
    2) Create Persistent Volumes:
       - Alt 1: Manually
       - Alt 2: Let a DaemonSet handle the creation.

  - USSAGE STEPS:

     PRE-SETUP Plannification)
     - how many LOCAL disks would each node cluster have?
     - How would they be partitioned?
       (The local static provisioner provides guidance to help answer
        these questions).
       Hint: It’s best to be able to dedicate a full disk to each local
       volume (for IO isolation) and a full partition per-volume (for
       capacity isolation).

     STEP 1) Create StorageClass:
       ┌──────────────────────────────────────────┐
       │ kind: StorageClass                       │
       │ apiVersion: storage.k8s.io/v1            │
       │ metadata:                                │
       │   name: local-storage                    │
       │ provisioner: kubernetes.io/no─provisioner│
       │ volumeBindingMode: WaitForFirstConsumer  ← enable  volume topology-aware scheduling
       └──────────────────────────────────────────┘ Consumer == "Pod requesting the volume"

     STEP 2) the external static provisioner can be configured and run to
             create PVs for all the local disks on your nodes.
       $ kubectl get pv
       NAME              CAPA  ACC. RECLAIM  STATUS    CLAIM STORAGECLASS  REASON AGE
                               MODE POLICY
       local-pv-27c0f084 368Gi RWO  Delete   Available       local-storage         8s
       local-pv-3796b049 368Gi RWO  Delete   Available       local-storage         7s
       local-pv-3ddecaea 368Gi RWO  Delete   Available       local-storage         7s

     STEP 3) Start using PVs in workloads by:
       - Alt 1: creating a PVC and Pod (not show)
       - Alt 2: creating a StatefulSet with volumeClaimTemplates
         ┌ Ex: ───────────────────────────────────┐
         │ apiVersion: apps/v1                    │
         │ kind: StatefulSet                      │
         │ metadata:                              │
         │   name: local-test                     │
         │ spec:                                  │
         │   serviceName: "local-service"         │
         │   replicas: 3                          │
         │   selector: ...                        │
         │   template: ...                        │
         │     spec:                              │
         │       containers:                      │
         │       - name: test-container           │
         │         ...                            │
         │         volumeMounts:                  │
         │ ┌····   - name: local-vol              │
         │ ·         mountPath: /usr/test-pod     │
         │ · volumeClaimTemplates:                │
         │ · - metadata:                          │
         │ └···· name: local-vol                  │
         │     spec:                              │
         │       accessModes: [ "ReadWriteOnce" ] │
         │       storageClassName:"local-storage" │
         │       resources:                       │
         │         requests:                      │
         │           storage: 368Gi               │
         └────────────────────────────────────────┘
          Once the StatefulSet is up and running, the PVCs will be bound:

       $ kubectl get pvc
          NAME                   STATUS VOLUME              CAPACITY ACC.. STORAGECLASS   AGE
          local-vol-local-test-0 Bound  local-pv-27c0f084   368Gi    RWO   local-storage  3m45s
          local-vol-local-test-1 Bound  local-pv-3ddecaea   368Gi    RWO   local-storage  3m40s
          local-vol-local-test-2 Bound  local-pv-3796b049   368Gi    RWO   local-storage  3m36s

     STEP 4) Automatic Clean Up. Ex modifying replicas for stateful set (sts):
       $ kubectl patch sts local-test \     ←  e.g: Reduce Pod replicates 3→2 for StatefulSet
         -p '{"spec":{"replicas":2}}'               associated local-pv-... is not needed anymore.
       statefulset.apps/local-test patched          The external static provisioner will clean up
                                                    the disk and make the PV available for use again.

       $ kubectl delete pvc local-vol-local-test-2
       persistentvolumeclaim "local-vol-local-test-2" deleted

       $ kubectl get pv
       NAME              CAPA  ACC. RECLAIM  STATUS    CLAIM STORAGECLASS  REASON AGE
                               MODE POLICY
       local-pv-27c0f084 368Gi RWO  Delete   Bound      ...-0 local-storage        11m
       local-pv-3796b049 368Gi RWO  Delete   Available        local-storage         7s
       local-pv-3ddecaea 368Gi RWO  Delete   Bound      ...-1 local-storage        19m
                                                        └─┬─┘
                                            default/local-vol-local-test-0/1

    - LPV LIMITATIONS AND CAVEATS:
      - It ties apps to a specific node, making it harder to schedule.
        Those apps Bºshould specify a high priorityº so that lower priority pods,
        can be preempted if necessary.
      - Also if the tied-to node|local volume become inaccessible, then the pod
        also becomes inaccessible requiring manual intervention, external controllers
        or operators.
      - If a node becomes unavailable (removed from the cluster or drained), pods using
        local volumes on that node are stuck in "Unknown" or "Pending" state depending
        on whether or not the node was removed gracefully.
        To recover from these interim states:
        STEP 1) the PVC binding the pod to its local volume must be deleted
        STEP 2) the pod must be deleted to forcerescheduled
        (or wait until the node and disk are available again).
        "... We took this into account when building our operator for
         M3DB, which makes changes to the cluster topology when a pod is
         rescheduled such that the new one gracefully streams data from the
         remaining two peers..."
        "...Thanks to the k8s scheduler’s intelligent handling of volume topology,
         M3DB is able to programmatically evenly disperse its replicas across multiple
         local persistent volumes in all available cloud zones, or, in the case of     [cloud]
         on-prem clusters, across all available server racks..."

      - Because of these constraints, IT’S BEST TO EXCLUDE NODES WITH
        LOCAL VOLUMES FROM AUTOMATIC UPGRADES OR REPAIRS, and in fact some
        cloud providers explicitly mention this as a best practice.
        (Basically most of the benefits of k8s are lost for apps managing
         storage at App level. This is the case with most DDBBs and stream
         architectures).
[[}]]

● Running storage services [[{storage.101,01_PM.TODO]]
(GlusterFS,iSCSI,...)
@[https://opensource.com/article/17/12/storage-services-kubernetes]
[[}]]

● Rook.io [[{storage.distributed.101,01_PM.low_code,01_PM.TODO]]
@[https://rook.io/]
@[https://www.infoq.com/news/2019/08/rook-v1-release/]
- Rook turns distributed storage systems into self-managing, self-scaling,
  self-healing storage services AUTOMATING THE TASKS OF A STORAGE
  ADMINISTRATOR: DEPLOYMENT, BOOTSTRAPPING, CONFIGURATION,
  PROVISIONING, SCALING, UPGRADING, MIGRATION, DISASTER RECOVERY,
  MONITORING, AND RESOURCE MANAGEMENT.

- 2019-08: release v1.0 for production-ready workloads that use file,
  block, and object storage in containers.
  storage providers through operators include:
  - Ceph Nautilus
  - EdgeFS
  - NFS.
  eg: pod requests an NFS file system, Rook provisions it without any
      manual intervention.

- 1st storage project accepted by (CNCF)
[[}]]

● Ceph [[{storage.distributed,01_PM.TODO]]
@[https://ceph.io/]
[[}]]

● Noobaa [[{storage.object.S3,storage.distributed,application.knative,cloud,01_PM.TODO]]
@[https://www.noobaa.io/try]

- NooBaa can collapse multiple storage silos into a single, scalable
  storage fabric, by its ability to virtualize any local storage,
  whether shared or dedicated, physical or virtual and include both
  private and public cloud storage, using the same S3 API and
  management tools.
- NooBaa also gives you full control over data placement, letting
  you place data based on security, strategy and cost considerations,
  in the granularity of an application.

- Easily scales locally on top of PVs or Ceph external clusters.

- Workload Portability
  Easily mirror data to other cluster or native cloud storage.
[[}]]


[[ $storage }]]

NETWORKING [[{ $networking ]]
● Max Node Lattency [[{101,cluster_admin,network.101.kubelet,troubleshooting]]
@[https://stackoverflow.com/questions/46891273/kubernetes-what-is-the-maximum-distance-latency-supported-between-kubernetes-no]
- There is no latency limitation between nodes in kubernetes
  cluster. They are configurable parameters.
  - For kubelet on worker node:
    --node-status-update-frequency secs
      Sets how often kubelet posts node status to master.
      Note: be cautious when changing the constant, it must work
      with nodeMonitorGracePeriod in nodecontroller. (default 10s)
  - For controller-manager on master node:
    --node-monitor-grace-period secs
      Amount of time which we allow running Node to be unresponsive
      before marking it unhealthy. Must be N times more than kubelet's
      'nodeStatusUpdateFrequency', where N means number of retries
      allowed for kubelet to post node status. (default 40s)
    --node-monitor-period secs
      Period for syncing NodeStatus in NodeController. (default 5s)
    --node-startup-grace-period secs
      Amount of time which we allow starting Node to be unresponsive before
      marking it unhealthy. (default 1m0s)
[[}]]
● SDN intro: [[{cloud.sdn,01_PM.TODO]]
@[https://www.redhat.com/sysadmin/getting-started-sdn]
- provide sdns: allows admin teams to control network traffic in
      complex networking topologies through a centralized panel,
      rather than handling each network device, such as routers
      and switches, manually ("hierarchical" topology)

## container network interface (cni):
- library definition and a set of tools to configure network interfaces in linux containers
  through many supported plugins.
  - multiple plugins can run at the same time in a container
    that participates in a network driven by different plugins.
  - networks use json configuration files and instantiated as
    new namespaces when the cni plugin is invoked.

- common cni plugins include:
  -*calico*:
    - high scalability.
    @[https://docs.projectcalico.org/v3.7/getting-started/kubernetes/]
    @[https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/]
  -*cilium*:
    - provides network connectivity and load balancing
      between application workloads, such as application
      containers and processes, and ensures transparent security.
  -*contiv*:
    - integrates containers, virtualization, and physical servers
      based on the container network using a single networking fabric.
  -*contrail*:
    - provides overlay networking for multi-cloud and
      hybrid cloud through network policy enforcement.
  -*flannel*:
    - makes it easier for developers to configure a layer 3
      network fabric for kubernetes.
  -*multus*:
    - supports multiple network interfaces in a single pod on
      kubernetes for sriov, sriov-dpdk, ovs-dpdk, and vpp workloads.
  -*open vswitch (ovs)*:
    - production-grade cni platform with a standard management
      interface on openshift and openstack.
  -*ovn-kubernetes*:
    - enables virtual networks for multiple containers on different
      hosts using an overlay function.
  -*romana*:
    - makes cloud network functions less expensive to build,
      easier to operate, and better performing than traditional
      cloud networks.

- in addition to network namespaces, an sdn should increase security by
  offering isolation between multiple namespaces with the multi-tenant plugin:
  packets from one namespace, by default, will not be visible to
  other namespaces, so containers from different namespaces cannot
  send packets to or receive packets from pods and services of a
  different namespace.
[[}]]


● SERVICE MESH COMPARATIVE [[{cloud.sdn,02_doc_has.comparative,01_PM.low_code,application.observability,application.development]]
• istio.io SERVICE MESH
- L7 Software defined network.
- istio forwards "any" request from Pod to Pod (for app-to-app) through a set
  of Envoys (C++) proxies installed on each working node and controlled by istio.
  This allows istio to control the traffic "globally" (at the cost of some extra
  "slow-down" in network communications). It can control routing, rate limiting,
  (distributed microservice traffic) OBSERVABILITY, mTLS, and other capabilities
  with "zero" development costs, apply global network policies, ...
- Network/Applications admins interact with istio through a control pane.
- Works within/outside Kubernetes.

• linkerd SERVICE MESH:
- GO/Rust based.
- Used linkerd2-proxy, faster/lighter than (istio) Envoy.
- Similar to istio.io but looks to be lighter and faster for K8s.
- Only works in Kubernetes.
- Does NOT offer as much options as Istio but is simpler to install.
[[ $networking }]]


● Monitoring [[{ $monitoring ]]
● crictl: Debug node
@[https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/]
[[}]]

● Monitoring the cluster

● Auditing [[{cluster_admin,security.auditing,01_PM.TODO]]
@[https://kubernetes.io/docs/tasks/debug-application-cluster/audit/]
[[}]]


● Events in Stackdriver [[{cluster_admin,monitoring,troubleshooting,debug,01_PM.TODO]]
@[href=https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/]
- Kubernetes events are objects that provide insight into what is
  happening inside a cluster, such as what decisions were made by
  scheduler or why some pods were evicted from the node.

- Since events are API objects, they are stored in the apiserver on
  master. To avoid filling up master's disk, a retention policy is
  enforced: events are removed one hour after the last occurrence. To
  provide longer history and aggregation capabilities, a third party
  solution should be installed to capture events.

- This article describes a solution that exports Kubernetes events to
  Stackdriver Logging, where they can be processed and analyzed.
[[}]]

● Metrics API+Pipeline [[{cluster_admin,monitoring,troubleshooting,debug,01_PM.TODO]]
- Resource usage metrics, such as container CPU and memory usage, are
  available in Kubernetes through the Metrics API. These metrics can be
  either accessed directly by user, for example by using kubectl top
  command, or used by a controller in the cluster, e.g. Horizontal Pod
  Autoscaler, to make decisions.

@[https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/]
@[https://github.com/kubernetes-incubator/metrics-server]

  Extracted from @[https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/]
  """...If running on Minikube, run the following command to enable the metrics-server:
  $ minikube addons enable metrics-server

  ... to see whether the metrics-server is running, or another provider of the resource metrics
  API (metrics.k8s.io), run the following command:

  $ kubectl get apiservices
     output must include a reference to metrics.k8s.io.
     → ...
     → v1beta1.metrics.k8s.io
  """

● All components of a typical k8s app!!
@[https://medium.com/geekculture/how-to-deploy-spring-boot-and-mongodb-to-kubernetes-minikube-71c92c273d5e]
[[}]]

[[ $monitoring }]]

Auditing [[{]]
● Auditing [[{security.auditing,qa,01_PM.TODO.NOW]]
@[https://kubernetes.io/docs/tasks/debug-application-cluster/audit/]
- Kubernetes auditing provides a security-relevant chronological set of
  records documenting the sequence of activities that have affected
  system by individual users, administrators or other components of the
  system. It allows cluster administrator to answer the following
  questions:
  - what happened?
  - when did it happen?
  - who initiated it?
  - on what did it happen?
  - where was it observed?
  - from where was it initiated?
  - to where was it going?
[[}]]
[[}]]

K8s Extension Tools [[{ $extension_tools ]]

● Teresa [[{application,01_PM.low_code,01_PM.TODO]]
@[https://github.com/luizalabs/teresa]
- extremely simple platform as a service that runs on top
  of Kubernetes. It uses a client-server model: the client sends high
  level commands (create application, deploy, etc.) to the server,
  which translates them to the Kubernetes API.
[[}]]

● Gravity Portable clusters and snahpshots [[{cluster_admin.backup,security.backup,01_PM.TODO]]
"take a cluster as is and deploy it somewhere else"

- Gravity takes snapshots of Kubernetes clusters, their container
  registries, and their running applications, called "application
  bundles." The bundle, which is just a .tar file, can replicate the
  cluster anywhere Kubernetes runs.

- Gravity also ensures that the target infrastructure can support the
  same behavioral requirements as the source, and that the Kubernetes
  runtime on the target is up to snuff. The enterprise version of
  Gravity adds security features including role-based access controls
  and the ability to synchronize security configurations across
  multiple cluster deployments.

- latest major version, Gravity 7, can deploy a Gravity image into
  an existing Kubernetes cluster, versus spinning up an all-new cluster
  using the image. Gravity 7 can also deploy into clusters that
  aren’t already running a Gravity-defined image. Plus, Gravity now
  supports SELinux and integrates natively with the Teleport SSH
  gateway.
[[}]]

● KubeDB: Production DBs [[{storage.kubedb,application.operators,01_PM.low_code,qa,01_PM.TODO]]
- operators exists for MySQL/PostgreSQL/Redis/... BUT THERE ARE PLENTY OF GAPS.

  KUBEDB ALLOWS TO CREATE CUSTOM KUBERNETES OPERATORS FOR MANAGING DATABASES
  - Running backups, cloning, monitoring, snapshotting, and declaratively
    creating databases are all part of the mix.
  - supported features vary among databases. Ex: clustering is available for
    PostgreSQL but not MySQL.
[[}]]

● Kube-monkey: Chaos testing [[{qa.testing.chaos,01_PM.TODO]]
- stress test, breaking stuff at random.
- It works by randomly killing pods in a cluster
  that you specifically designate, and can be fine-tuned
  to operate within specific time windows.

See also:
https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/
[[}]]

● Ingress Controller for AWS [[{network.ingress,cloud.aws,01_PM.TODO]]
- Allows to reuse AWS load balancing functionality.
- It uses AWS CloudFormation to ensure that cluster state
  remains consistent.
[[}]]

[[$extension_tools }]]

Governance [[{ $governance ]]
● Gatekeeper policy controls [[{security.101,qa.governance,security.standards,security.aaa,security.auditing,01_PM.backlog]] #[gatekeeper_summary]
- The Open Policy Agent project (OPA) provides a way to create policies
  across cloud-native application stacks, from ingress to service-mesh
  components to Kubernetes. More info available at:
  @[../Cloud/cloud_map.html#OPA_Summary]
- OPA DECOUPLES POLICY DECISION-MAKING FROM POLICY ENFORCEMENT.
- Gatekeeper provides a Kubernetes-native way to enforce OPA
  policies on a cluster automatically, and to audit for any events or
  resources violating policy. All this is handled by a relatively new
  mechanism in Kubernetes, admission controller Webhooks, that fire on
  changes to resources. With Gatekeeper, OPA policies can be maintained
  as just another part of your Kubernetes cluster's defined state,
  without needing constant babysitting.
[[}]]
● Teleport [[{security.aaa.2FA,cluster_admin.ssh,security.auditing,01_PM.backlog]]
- implement industry-best practices for SSH and Kubernetes access,
  meet compliance requirements, and have complete visibility into access and behavior.

· Security best practices out-of-the-box.
· Isolate critical infra and enforce 2FA with SSH and Kubernetes.
· Provide role-based access controls (RBAC) using short-lived
certificates and your existing identity management service.
· Log events and record session activity for full auditability.
[[}]]

● Kubecost [[{security.auditing,cloud.billing,troubleshooting,01_PM.backlog]]
- monitor "the dollars" cost of running Kubernetes?

- Kubecost uses real-time Kubernetes metrics, and real-world cost
  information derived from running clusters on the major cloud
  providers, to provide a dashboard view of the monthly cost of each
  cluster deployment. Costs for memory, CPU, GPU, and storage are all
  broken out by Kubernetes component (container, pod, service,
  deployment, etc.).

- Kubecost can also track the costs of “out of cluster” resources,
  such as Amazon S3 buckets, although this is currently limited to AWS.

- Kubecost is free to use if you only need to keep 15 days of logs. For
  more advanced features, pricing starts at $199 per month for
  monitoring 50 nodes.
[[}]]


[[$governance }]]

cluster Administration [[{ $cluster_admin ]]
● Bootstrap/Manage k8s clusters [[{101,cluster_admin.bootstrap,01_PM.low_code,troubleshooting]]
● "Kind": lightweight k8s setup over Docker @[kind_summary]
@[https://kind.sigs.k8s.io/]
Extracted from "Cloud_Native_Spring_in_Action_v7.pdf" (Chapter 2.4)
"""... There  are  a  few  ways  to  install  Kubernetes  in  your
    local development environment.  The  approach  I recommend  is
   "Kind", since you have already installed Docker on your dev machine."""

- Kind replaces "k8s server nodes" by "k8s Docker nodes".
  Features:
  - support for multi-node (including HA) clusters
  - easy switch to a different Kubernetes version/release (from source)
  - Support for Linux, macOS and Windows

- Install
  More options available at:
@[https://kind.sigs.k8s.io/docs/user/quick-start]
Latest Releases: https://github.com/kubernetes-sigs/kind/releases
  $ KIND_V="v0.11.1"
  $ SRC_URL="https://kind.sigs.k8s.io/dl"
  $ SRC_URL="${SRC_URL}/${KIND_V}/kind-linux-amd64"
  $ curl -Lo ./kind ${SRC_URL}
  $ chmod +x ./kind

- Ussage:
  $ kind create cluster      ← bootstrap k8s cluster, 1 single command!!!

  $ kind load docker-image \ ← Import image from local docker registry
       $imgName:$imgVer

  $ kind delete cluster      ← Remove cluster, 1 single command!!!
[[}]]

● Kubespray: Cluster Bootstrap [[{cluster_admin.bootstrap,_doc_has.comparative,cloud.aws,cloud.gce,01_PM.TODO]]
@[https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md]
- Kubespray is formed by a set of Ansible playbooks (optionally Vagrant) allowing
  to create a new production-ready k8s cluster (mono/multi-master,
  single/distributed etcd, Flannel|Calico|Weave|... network...).
  Extra playbooks allows to add/remove new nodes to a running k8s cluster.
- It targets "mostly" any environment, from bare metal (RedHat, CoreOS, Ubuntu,...)
  to public clouds.
- Compared to "Kops", "Kops" is more tightly integrated and profits better from
  the unique features of the supported clouds (AWS,GCE, ...). "Kops" doesn't
  allow to deploy on standard linux distros running on bare-metal/VMs.
[[}]]

● Kubeadm [[{cluster_admin.kubeadmin,01_PM.ext_resource,cluster_admin.IaS,qa.gitops,01_PM.TODO]]
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-version/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/
  https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/

Note: Kubespray offers a "simpler" front-end based on Ansible playbooks to Kubeadm.
Example Tasks:
- Upgrading kubeadm HA clusters from 1.9.x to 1.9.y
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha/]
- Upgrading/downgrading kubeadm clusters between v1.8 to v1.9:
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-9/]
- Upgrading kubeadm clusters from 1.7 to 1.8
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-8/]
- Upgrading kubeadm clusters from v1.10 to v1.11:
@[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-11/]
[[}]]

● Other Admin Tasks [[{cluster_admin,network,01_PM.TODO]]
- TLS Cert Mng:
@[https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/]
@[https://kubernetes.io/docs/tasks/tls/certificate-rotation/]
- kubelet TLS setup:
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/]
- Mng.Cluster DaemonSets:
- Perform a Rollback on a DaemonSet:
@[https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/]
- Perform a Rolling Update on a DaemonSet:
@[https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/]
[[}]]

[[$cluster_admin }]]

● Understanding K8S Code [[{]]
- K8s Implementation Summary     [[{101.k8s_internals,01_PM.TODO]]
@[https://jvns.ca/blog/2017/06/04/learning-about-kubernetes/]
  by Julia Evans "A few things I've learned about Kubernetes"

- """... you can run the kubelet by itself! And if you have a kubelet, you
  can add the API server and just run those two things by themselves! Okay,
  awesome, now let’s add the scheduler!"""

- the “kubelet” is in charge of running containers on nodes.
- If you tell the API server to run a container on a node, it will tell
  the kubelet to get it done (indirectly)
- The scheduler translates "run a container" to "run a container on node X"

ºetcd is Kubernetes’ brainº
- Every component in Kubernetes (API server, scheduler, kubelets, controller manager, ...) is stateless.
   All of the state is stored in the (key-value store) etcd database.
- Communication between components (often) happens via etcd.
-Oºbasically everything in Kubernetes works by watching etcd for stuff it has to do,º
 Oºdoing it, and then writing the new state back into etcd º

  Ex 1: Run a container on Machine "X":
   Wrong way: ask kubelet@Machine"X" to run the container.
   Right way: kubectl*1 →(API Server)→ etcd: "This pod should run on Machine X"
              kubelet@Machine"X"     → etcd: check work to do
              kubelet@Machine"X"     ← etcd: "This pod should run on Machine X"
              kubelet@Machine"X"     ← kubelet@Machine"X": Run pod

  Ex 2: Run a container anywhere on the k8s cluster
    kubectl*1 → (API Server) → etcd: "This pod should run somewhere"
    scheduler                → etcd: Something to run?
    scheduler                ← etcd: "This pod should run somewhere"
    scheduler                → kuberlet@Machine"Y":  Run pod

 *1 The kubectl is used from the command line.
    In the sequence diagram it can be replaced for any
    of the existing controllers (ReplicaSet, Deployment, DaemonSet, Job,...)

ºAPI server roles in cluster:º
API Server is responsible for:
1.- putting stuff into etcd

    kubectl    → API Server : put "stuff" in etcd
    API Server → API Server : check "stuff"
    alt 1:
       kubectl ← API Server : error: "stuff" is wrong
    alt 2:
       API Server → etcd    : set/update "stuff"

2.- Managing authentication:
    ("who is allowed to put what stuff into etcd")
    The normal way is through X509 client certs.


ºcontroller manager does a bunch of stuffº
Responsible for:
- Inspect etcd for pending to schedule pods.
- daemon-set-controllers will inspect etcd for
  pending daemonsets and will call the scheduler
  to run them on every machine with the given
  pod configuration.
- The "replica set controller" will inspect etcd for
  pending replicasets and will create 5 pods that
  the scheduler will then schedule.
- "deployment controller" ...

ºTroubleshooting:º
something isn’t working? figure out which controller is
responsible and look at its logs

ºCore K8s components run inside of k8sº
- Only 5 things needs to be running before k8s starts up:
  - the scheduler
  - the API server
  - etcd
  - kubelets on every node (to actually execute containers)
  - the controller manager (because to set up daemonsets you
                            need the controller manager)

  Any other core system (DNS, overlay network,... ) can
  be scheduled by k8s inside k8s
[[}]]

● API Conventions [[{101.k8s_internals,01_PM.TODO]]
@[https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md]
[[}]]

● Source Code Layout [[{101.k8s_internals,01_PM.TODO]]
Note: Main k8s are placed in @[https://github.com/kubernetes/kubernetes/tree/master/pkg]
      (API, kubectl, kubelet, controller, ...)

- REF: A Tour of the Kubernetes Source Code Part One: From kubectl to API Server
 https://developer.ibm.com/opentech/2017/06/21/tour-kubernetes-source-code-part-one-kubectl-api-server/

- Examining kubectl source:
  Locating the implementation of kubectl commands in the Kubernetes source code

- @[https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd]
  kubectl entry point for all commands:
  - Inside there is a name of a go directory that matches the kubectl command:
    Example:
    @[https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd/create](.go)

ºK8s loves the Cobra Command Frameworkº
- k8s commands are implemented using the Cobra command framework.
- Cobra provides lot of features for building command line interfaces
  amongst them, Cobra puts the command usage message and command
  descriptions adjacent to the code that runs the command.
  Ex.:
  | // NewCmdCreate returns new initialized instance of create sub command
  | func NewCmdCreate(f cmdutil.Factory, ioStreams genericclioptions.IOStreams) *cobra.Command {
  |     o := NewCreateOptions(ioStreams)
  |
  |     cmd := ⅋cobra.Command{
  |         Use:                   "create -f FILENAME",
  |         DisableFlagsInUseLine: true,
  |         Short:                 i18n.T("Create a resource from a file or from stdin."),
  |         Long:                  createLong,
  |         Example:               createExample,
  |         Run: func(cmd *cobra.Command, args []string) {
  |             if cmdutil.IsFilenameSliceEmpty(o.FilenameOptions.Filenames) {
  |                 defaultRunFunc := cmdutil.DefaultSubCommandRun(ioStreams.ErrOut)
  |                 defaultRunFunc(cmd, args)
  |                 return
  |             }
  |             cmdutil.CheckErr(o.Complete(f, cmd))
  |             cmdutil.CheckErr(o.ValidateArgs(cmd, args))
  |             cmdutil.CheckErr(o.RunCreate(f, cmd))
  |         },
  |     }
  |
  |     // bind flag structs
  |     o.RecordFlags.AddFlags(cmd)
  |
  |     usage := "to use to create the resource"
  |     cmdutil.AddFilenameOptionFlags(cmd, ⅋o.FilenameOptions, usage)
  |     ...
  |     o.PrintFlags.AddFlags(cmd)
  |
  |     // create subcommands
  |     cmd.AddCommand(NewCmdCreateNamespace(f, ioStreams))
  |     ...
  |     return cmd
  | }

ºBuilders and Visitors Abound in Kubernetesº
  Ex. code:
  | r := f.NewBuilder().
  |     Unstructured().
  |     Schema(schema).
  |     ContinueOnError().
  |     NamespaceParam(cmdNamespace).DefaultNamespace().
  |     FilenameParam(enforceNamespace, ⅋o.FilenameOptions).
  |     LabelSelectorParam(o.Selector).
  |     Flatten().
  |     Do()
  The functions Unstructured, Schema, ContinueOnError,...  Flatten
  all take in a pointer to a Builder struct, perform some form of
  modification on the Builder struct, and then return the pointer to
  the Builder struct for the next method in the chain to use when it
  performs its modifications defined at:
  @[https://github.com/kubernetes/cli-runtime/blob/master/pkg/genericclioptions/resource/builder.go]

  | ...
  | func (b ºBuilder) Schema(schema validation.Schema) ºBuilder {
  |     b.schema = schema
  |     return b
  | }
  | ...
  | func (b ºBuilder) ContinueOnError() ºBuilder {
  |     b.continueOnError = true
  |     return b
  | }

 The Do function finally returns a Result object that will be used to drive
the creation of our resource. It also creates a Visitor object that can be
used to traverse the list of resources that were associated with this
invocation of resource.NewBuilder. The Do function implementation is shown below.


  a new DecoratedVisitor is created and stored as part of the Result object
that is returned by the Builder Do function. The DecoratedVisitor has a Visit
function that will call the Visitor function that is passed into it.
  |// Visit implements Visitor
  |func (v DecoratedVisitor) Visit(fn VisitorFunc) error {
  |    return v.visitor.Visit(func(info *Info, err error) error {
  |        if err != nil {
  |            return err
  |        }
  |        for i := range v.decorators {
  |            if err := v.decorators[i](info, nil); err != nil {
  |                return err
  |            }
  |        }
  |        return fn(info, nil)
  |    })
  |}


  Create eventually will call the anonymous function that contains the
  createAndRefresh function that will lead to the code making a REST call
  to the API server.

  The createAndRefresh function invokes the Resource NewHelper
  function found in ...helper.go returning a new Helper object:
  | func NewHelper(client RESTClient, mapping ºmeta.RESTMapping) ºHelper {
  |     return ⅋Helper{
  |         Resource:        mapping.Resource,
  |         RESTClient:      client,
  |         Versioner:       mapping.MetadataAccessor,
  |         NamespaceScoped: mapping.Scope.Name() == meta.RESTScopeNameNamespace,
  |     }
  | }

  Finally the Create function iwill invoke a createResource function of the
  Helper Create function.
  The Helper createResource function, will performs the actual REST call to
  the API server to create the resource we defined in our YAML file.


ºCompiling and Running Kubernetesº
- we are going to use a special option that informs the Kubernetes build process

$ make WHAT='cmd/kubectl'  # ← compile only kubectl
Test it like:
On terminal 1 boot up local test hack cluster:
$ PATH=$PATH KUBERNETES_PROVIDER=local hack/local-up-cluster.sh
On terminal 2 execute the compiled kubectl:
$ cluster/kubectl.sh create -f nginx_replica_pod.yaml

ºCode Learning Toolsº
Tools and techniques that can really help accelerate your ability to learn the k8s src:
- Chrome Sourcegraph Plugin:
  provides several advanced IDE features that make it dramatically
  easier to understand Kubernetes Go code when browsing GitHub repositories.
  Ussage:
  - start by looking at an absolutely depressing snippet of code,
    with ton of functions.
  - Hover over each code function with Chrome browser + Sourcegraph extension
    installed:
    It will popup a description of the function, what is passed into it
    and what it returns.
  - It also provides advanced view with the ability to peek into the function
    being invoked.
- Properly formatted print statements:
  fmt.Println("\n createAndRefresh Info = %#v", info)
- Use of a go panic to get desperately needed stack traces:
  | func createAndRefresh(info *resource.Info) error {
  |     fmt.Println("\n createAndRefresh Info = %#v", info)
  |     ºpanic("Want Stack Trace")º
  |     obj, err := resource.NewHelper(info.Client, info.Mapping).Create(info.Namespace, true, info.Object)
  |     if err != nil {
  |         return err
  |     }
  |     info.Refresh(obj, true)
  |     return nil
  | }
- GitHub Blame to travel back in time:
  "What was the person thinking when they committed those lines of code?"
  - GitHub browser interface has a blame option available as a button on the user interface:
      It returns a view of the code that has the commits responsible for each line of code
    in the source file. This allows you to go back in time and look at the commit that added
    a particular line of code and determine what the developer was trying to accomplish when
    that line of code was added.
[[}]]

● Extending k8s API [[{101.k8s_internals,application.knative,01_PM.TODO]]

- Custom Resources:
@[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/]

- Extend API with CustomResourceDefinitions:
@[https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/]

- Versions of CustomResourceDefinitions:
@[https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/]

- Migrate a ThirdPartyResource to CustomResourceDefinition:
@[https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/migrate-third-party-resource/]
- Aggregation Layer:
@[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/]
  - Configure the Aggregation Layer:
  @[https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/]
- Setup an Extension API Server:
@[https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/">
- Use HTTP Proxy to Access the k8s API:
@[https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/]
[[}]]

[[}]]

● kubespy: observe Kubernetes resources in REAL TIME  [[{TROUBLESHOOTING,APPLICATION.OBSERVABILITY,MONITORING]]

  - What happens when you boot up a Pod? What happens to a Service
    before it is allocated a public IP address? How often is a
    Deployment's status changing?

      kubespy is a small tool that makes it easy to observe how
    Kubernetes resources change in real time, derived from the work we
    did to make Kubernetes deployments predictable in Pulumi's CLI. Run
    kubespy at any point in time, and it will watch and report
    information about a Kubernetes resource continuously until you kill it
[[}]]

● Kubesec cli tool for Security risk analysis of resources [[{security.101,security.auditing.kubesec]]
 @[https://github.com/controlplaneio/kubesec
 @[https://kubesec.io/

 $ kubesec ./deployment.yml  # multiple YAML documents can be scanned in a single input file.
 [
   {
     "object": "Pod/security-context-demo.default",
     "valid": true,
     "message": "Failed with a score of -30 points",
     "score": -30,
     "scoring": {
       "critical": [
         {
           "selector": "containers[] .securityContext .capabilities .add == SYS_ADMIN",
           "reason": "CAP_SYS_ADMIN is the most privileged capability and should always be avoided"
         }
       ],
       "advise": [
         {
           "selector": "containers[] .securityContext .runAsNonRoot == true",
           "reason": "Force the running image to run as a non-root user to ensure least privilege"
         },
         ...
       ]
     }
   }
 ]

[[}]]



TODO / UNORDERED NOTES [[{ $undordered_notes ]]

● TODO Recheck info mentioned in
Helm installs resources in the following order:  [[{101]]
    Namespace
    NetworkPolicy
    ResourceQuota
    LimitRange
    PodSecurityPolicy
    PodDisruptionBudget
    ServiceAccount
    Secret
    SecretList
    ConfigMap
    StorageClass
    PersistentVolume
    PersistentVolumeClaim
    CustomResourceDefinition
    ClusterRole
    ClusterRoleList
    ClusterRoleBinding
    ClusterRoleBindingList
    Role
    RoleList
    RoleBinding
    RoleBindingList
    Service
    DaemonSet
    Pod
    ReplicationController
    ReplicaSet
    Deployment
    HorizontalPodAutoscaler
    StatefulSet
    Job
    CronJob
    Ingress
    APIService
[[101}]]


● GPU and Kubernetes [[{cluster_admin.gpu,01_PM.TODO]]
@[https://www.infoq.com/news/2018/01/gpu-workflows-kubernetes]
@[https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/]
[[}]]

● Skaffold [[{application,01_PM.low_code,qa,01_PM.TODO]]
@[https://www.infoq.com/news/2018/03/skaffold-kubernetes/]

-ºlightweight client-sideºtool triggering ºhighly optimized local deploymentº
 ºpipelinesºwhen source-code changes are detected by compiling, building
  OCI images, pushing, and deploying the app automatically with policy-based
  image tagging, resource port-forwarding, logging, file syncing, and much more.

  NON-SKAFFOLD                      │  SKAFFOLD
    PIPELINE                        │  PIPELINE
  ============                      │  =========
  · git clone                       │  git clone
  · build app                       │  $ skaffold run   ← Support for: ✓ profiles ✓ env.vars
  · create Dockerfile               │    └─────┬────┘                  ✓ local-user-config ✓ flags
  · build app OCI                   │    Build,deploy, watch for changes. When source changes
  · push image to k8s cluster       │    , roll out updated  objects in local k8s.
  · deploy DDBB from Helm chart     │    Skaffold's inner loop PROVIDES
  · create k8s YAML manifest        │    BºINSTANT FEEDBACK WHILE DEVELOPING!!!º
      for app deployment, services, │
      ...                           │  $ skaffold dev   --port-forward ← like run, forward ports
  · collecting logs and dump into   │  $ skaffold debug --port-forward ← like run, forward ports
  · local console.                  │                                    and add remote debugger
                                                                         (JVM def: localhost:5005)
  ===================
  = Skaffold HOW-TO =
  ===================
  $ cd .../"project_root"
  $ skaffold init --XXenableBuildbacksInit   ← init skaffold using "Cloud Native Buildpacks"
                                               to build OCI image.
                                              './skaffold.yaml' will be created: It can be
                                               customized to use "Cloud Native Buildpacks" Paketo
                                               implementation (e.g. Used by Spring Boot),
                                               define new resources to deploy, ...

apiVersion: skaffold/v2beta13                ← k8s manifest for k8s non-standard object
kind: Config
metadata:
  name: catalog-service
build:                                       ← How to build the App from source
  artifacts:                                   (java+mvn in this example)
    - image: myCompany/myApp01
      buildpacks:
        builder: gcr.io/paketo-buildpacks/builder:base
        env:
          - BP_JVM_VERSION=11.*
deploy:
  helm:
    releases:
      - name: myApp01-ddbb
      remoteChart: bitnami/postgresql
      setValues:
        postgresqlUsername: admin
        postgresqlPassword: admin
        postgresqlDatabase: myApp01-ddbb
        image.tag: 13
  kubectl:
    manifests:
      - k8s/*.yml
[[}]]

● JAVA api Client [[{101,application.knative,dev_lang.java]]
- Useful for example to get k8s service config inside
  running container.
Summary extracted from:
https://github.com/hyperledger/besu/blob/master/nat/src/main/java/org/hyperledger/besu/nat/kubernetes/KubernetesNatManager.java
import io.kubernetes.client.ApiClient;
import io.kubernetes.client.Configuration;
import io.kubernetes.client.apis.CoreV1Api;
import io.kubernetes.client.models.V1Service;
import io.kubernetes.client.util.ClientBuilder;
import io.kubernetes.client.util.KubeConfig;
import io.kubernetes.client.util.authenticators.GCPAuthenticator;
...

      try {
  KubeConfig.registerAuthenticator(new GCPAuthenticator());
  final ApiClient client = ClientBuilder.cluster().build();
  Configuration.setDefaultApiClient(client);
  final CoreV1Api api = new CoreV1Api();
  // invokes the CoreV1Api client
  final V1Service service =
      api.listServiceForAllNamespaces(null, null, null, null, null, null, null, null, null)
          .getItems().stream()
          .filter(
              v1Service -> v1Service.getMetadata().getName().contains(besuServiceNameFilter))
          .findFirst()
          .orElseThrow(() -> new NatInitializationException("Service not found"));
  updateUsingBesuService(service);

  internalAdvertisedHost =
      getIpDetector(service)
          .detectAdvertisedIp()
          .orElseThrow(
              () -> new NatInitializationException("Unable to retrieve IP from service"));

   final String internalHost = queryLocalIPAddress().get(TIMEOUT_SECONDS, TimeUnit.SECONDS);
  service.getSpec().getPorts().forEach( v1ServicePort -> { ... } );

  final String serviceType = service.getSpec().getType();
  final Optional<String> clusterIP = v1Service.getSpec().getClusterIP();
      } catch (Exception e) {
  throw new RuntimeException(
    "Failed update information using pod metadata : " + e.getMessage(), e);
      }
[[}]]

● Running Apps [[{101,01_PM.TODO]]
Run Applications
- Run  Single-Instance Stateful Application
@[https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/]
@[https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/]
@[https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/]
@[https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/]
@[https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/]
@[https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/]
@[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/]
@[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/]
[[}]]


● CRDs (Custom Resource Definition) [[{k8s_api,01_PM.TODO]]
https://kubernetes.io/blog/page/17/
KubeDirector: The easy way to run complex stateful applications on Kubernetes
 open source project designed to make it easy to run complex stateful
scale-out application clusters on Kubernetes. KubeDirector is built
using the custom resource definition (CRD) framework and leverages
the native Kubernetes API extensions and design philosophy. This
enables transparent integration with Kubernetes user/resource
management as well as existing clients and tools.
[[}]]

● Topology-Aware Volume Provisioning [[{storage.101,01_PM.TODO]]
@[https://kubernetes.io/blog/page/12/]
- multi-zone cluster experience with persistent volumes is
  improving in k8s 1.12 with topology-aware dynamic provisioning
  beta feature.
- It allows k8s to make  intelligent decisions when dynamically
  provisioning volumes by getting scheduler input on the best place(zone)
  to provision a volume for a pod.
[[}]]

● IPVS-Based Load Balancing [[{network.IPVS,01_PM.TODO]]
@[https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/]
- IPVS: stands for IP Virtual Server.
- It is built on top of the Netfilter and implements
  transport-layer load balancing as part of the Linux kernel.
- IPVS is incorporated into the LVS (Linux Virtual Server), where it
  runs on a host and acts as a load balancer in front of a cluster of
  real servers. IPVS can direct requests for TCP- and UDP-based
  services to the real servers, and make services of the real servers
  appear as virtual services on a single IP address. Therefore, IPVS
  naturally supports Kubernetes Service.
[[}]]

● RBAC TLS Sec [[{security.aaa.rbac,01_PM.TODO]]
@[https://sysdig.com/blog/kubernetes-security-rbac-tls/]
[[}]]

● CNCF landscape beginner’s guide [[{01_PM.ext_resource,01_PM.TODO]]

https://www.cncf.io/blog/2018/11/05/34097/
[[}]]

● Failures learnt [[{qa.DONTs,01_PM.TODO]]
https://es.slideshare.net/try_except_/kubernetes-on-aws-at-zalando-failures-learnings-devops-nrw
[[}]]

● k8s IDEs [[{application.knative,qa.IDE,application.development,01_PM.TODO]]
● IntelliJ k8s plugin
@[https://blog.jetbrains.com/idea/2018/03/intellij-idea-2018-1-kubernetes-support/]


● k8s Native IDE
https://www.zdnet.com/article/red-hat-introduces-first-kubernetes-native-ide/?ftag=TRE-03-10aaa6b&bhid=28374205867001011904732094012637
[[}]]

● Ambassador API GW [[{network,network.ingress,application.knative.GW,01_PM.TODO]]
@[https://www.infoq.com/articles/ambassador-api-gateway-kubernetes]
- Ambassador: control plane tailored for edge/API configuration for
              managing the Envoy Proxy “data plane”.
- Envoy     : cloud native Layer 7 proxy and communication bus used
              for handling “edge” ingress and service-to-service
              networking communication.
[[}]]

● Best Patterns [[{101,qa,01_PM.TODO]]
"...Without an indication how much CPU and memory a container needs,
Kubernetes has no other option than to treat all containers equally.
That often produces a very uneven distribution of resource usage.
Asking Kubernetes to schedule containers without resource
specifications is like entering a taxi driven by a blind person..."
"...The fact that we can use Deployments with PersistentVolumes
does not mean that is the best way to run stateful applications..."
[[}]]

● Hybrid Cloud+On-Premise [[{network.hybrid,01_PM.TODO]]
@[https://cloud.google.com/blog/products/gcp/going-hybrid-with-kubernetes-on-google-cloud-platform-and-nutanix]
[[}]]

● Rancher K3s [[{cluster_admin.embedded,_doc_has,01_PM.TODO]]
https://k3s.io/
https://www.itprotoday.com/containers/rancher-labs-k3s-shrinks-kubernetes-edge
 i[././how-it-works-k3s-revised.svg|width=10em]

- highly available, certified k8s distribution designed for
  PRODUCTION WORKLOADS in unattended, resource-constrained,
  remote locations or inside IoT appliances.
- Simplified and Secure:
  K3s is packaged as a single <50MB binary that reduces the
  dependencies and steps needed to install, run and auto-update a
  production Kubernetes cluster.
- Optimized for ARM RaspPi ut to AWS a1.4xlarge 32GiB server.
- 512MB RAM to run.
[[}]]

● Operators [[{101,application.operators,qa,01_PM.TODO]]
- "Clearing House" for k8s Ops:
@[https://www.datacenterknowledge.com/open-source/aws-google-microsoft-red-hats-new-registry-act-clearing-house-kubernetes-operators]
  - AWS, Google, Microsoft, Red Hat's New Registry to Act as Clearing
    House for Kubernetes Operators.
  - Operators make life easier for Kubernetes users, but they're so
    popular that finding good ones is not easy. Operatorhub.io is an
    attempt to fix that.
[[}]]

● Gluster-k8s [[{storage.distributed.gluster,01_PM.TODO]]
@[https://github.com/gluster/gluster-kubernetes]
- gluster-kubernetes is a project to provide Kubernetes administrators a
  mechanism to easily deploy GlusterFS as a native storage service onto an
  existing Kubernetes cluster. Here, GlusterFS is managed and orchestrated like
  any other app in Kubernetes. This is a convenient way to unlock the power of
  dynamically provisioned, persistent GlusterFS volumes in Kubernetes.
[[}]]


● Ranher Submariner Multicluster [[{cluster_admin,01_PM.backlog]]
@[https://www.infoq.com/news/2019/03/rancher-submariner-multicluster]
● proper k8s cluster shutdown
https://serverfault.com/questions/893886/proper-shutdown-of-a-kubernetes-cluster
[[}]]


● pci/dss compliance [[{01_PM.TODO]]
https://www.infoq.com/news/2019/04/kubernetes-pci-dss-compliance
[[}]]

● Zero Downtime Migrations in Istio era [[{01_PM.TODO]]
C⅋P from JBCNConf 2019:
By Alex Soto
   Java Champion, Engineer @ Red Hat. Speaker, CoAuthor of Testing Java
   Microservices book, Member of JSR374 and Java advocate
   Zero Downtime Migrations in Istio era

- You joined the DevOps movement and want to release software even
  faster and safer. You started reading about Advanced deployment
  techniques like Blue-Green Deployment, Canary Releases or Dark Shadow
  Technique. But how do you implement them without disrupting your
  users in production? With Zero Downtime! This is easy with your code,
  but what about ephemeral and persistent states? Most of the examples
  out there does not tackle this scenario (which is the most common in
  real systems). Come to this session and you’ll learn in a practical
  way how you can achieve zero downtime deployments applying advanced
  deployment techniques for maintaining the ephemeral and persistent
  state with Istio
[[}]]

● Litmus chaos engineering framework [[{01_PM.TODO]]
https://www.infoq.com/news/2019/05/litmus-chaos-engineering-kube/
Chaos Engineering Kubernetes with the Litmus Framework

Litmus:
- open source chaos engineering framework for Kubernetes
  environments runninGºstateful applicationsº
- Litmus can be added to CI/CD pipelines
- designed to catch hard-to-detect bugs in Kubernetes
  that might be missed by unit or integration tests.
- focus on application resilience:
  - pre-existing tests for undesirable behavior such:
    - container crash
    - disk failure
    - or network delay
    - packet loss.
- can also be used to determine if a Kubernetes deployment
  is suitable for stateful workloads.
[[}]]

● cli reference [[{]]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/cloud-controller-manager/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/]
@[https://kubernetes.io/docs/concepts/architecture/master-node-communication/]
[[}]]

● Kubermatic: [[{cluster_admin,01_PM.low_code,cloud,qa.scalability,qa.vendor_lockin,01_PM.TODO]]
@[https://www.loodse.com/]
- Run Kubernetes on Autopilot:
  Automate operations of thousands of Kubernetes clusters across
  multi-cloud, on-prem, and edge environments with unparalleled density
  and resilience. Powered by Kubermatic Kubernetes Platform.
-ºEliminate vendor lock-in and dynamically move workloadsº.
[[}]]

● python + k8s api [[{dev_lang.python,application.knative,01_PM.TODO]]
@[https://www.redhat.com/sysadmin/create-kubernetes-cron-job-okd]
[[}]]

● 12factor [[{101,01_PM.TODO]]
@[https://12factor.net/]
[[}]]

● k8s cli utilities [[{101,troubleshooting,01_PM.TODO]]
https://developers.redhat.com/blog/2019/05/27/command-line-tools-for-kubernetes-kubectl-stern-kubectx-kubens/
-ºsternº                                                                   [[{troubleshooting,monitoring]]
 - display the tail end of logs for containers and multiple pods.
 - the stern project comes from Wercker (acquired by Oracle in 2017).
 - rather than viewing an entire log to see what happened most
   recently, you can use stern to watch a log unfurl.                      [[}]]

- kubectx
 - helpful for multi-cluster installations, where you need to
   switch context between one cluster and another.
 - Rather than type a series of lengthy kubectl command, kubectx
   works it magic in one short command.
 - It also allows you to alias a lengthy cluster name into an alias.
 - For example (taken directly from the kubectx website),
   kubectx eu=gke_ahmetb-samples-playground_europe-west1-b_dublin allows
   you to switch to that cluster by running kubectx eu.
 - Another slick trick is that kubectx remembers your previous context—
   much like the “Previous” button on a television remote—and allows
   you to switch back by running kubectx -.

-ºkubetailº
  Bash script that enables you to aggregate (tail/follow) logs from
  multiple pods into one stream. This is the same as running "kubectl
  logs -f " but for multiple pods.
[[}]]

● portworx Persistent Storage [[{storage,01_PM.TODO]]
https://portworx.com/
calable Persistent Storage for Kubernetes

Built from the ground up for containers, PX-Store provides cloud
native storage for applications running in the cloud, on-prem and in
hybrid/multi-cloud environments.

[[}]]

● Tigera.io [[{network,01_PM.TODO]]
https://www.tigera.io/
(authors of the Calico Zero-Trust Network with Policy-based micro-segmentation)
https://www.projectcalico.org/,
Why add another layer of overhead when you don't need it?

Sometimes, an overlay network (encapsulating packets inside an extra
IP header) is necessary. Often, though, it just adds unnecessary
overhead, resulting in multiple layers of nested packets, impacting
performance and complicating trouble-shooting. Wouldn't it be nice if
your virtual networking solution adapted to the underlying
infrastructure, using an overlay only when required? That's what
Calico does. In most environments, Calico simply routes packets from
the workload onto the underlying IP network without any extra
headers. Where an overlay is needed – for example when crossing
availability zone boundaries in public cloud – it can use
lightweight encapsulation including IP-in-IP and VxLAN. Project
Calico even supports both IPv4 and IPv6 networks!
[[}]]

● Kube-router [[{01_PM.TODO]]
https://github.com/cloudnativelabs/kube-router
[[}]]

● kubectl-trace (bpftrace) [[{01_PM.TODO]]
https://github.com/iovisor/kubectl-trace
Schedule bpftrace programs on your kubernetes cluster using the kubectl
[[}]]

● kubeless [[{01_PM.TODO]]
https://github.com/kubeless/kubeless
Kubernetes Native Serverless Framework https://kubeless.io
[[}]]

● [[{01_PM.TODO]]
Automatically sync groups into Kubernetes RBAC
@[https://github.com/cruise-automation/rbacsync]
[[}]]

● Red Hat Quay 3.1 [[{cluster_admin.image_registry,01_PM.TODO]]
@[https://www.zdnet.com/article/red-hat-quay-3-1-a-highly-available-kubernetes-container-registry-arrives/]
- Red Hat Quay:, highly available Kubernetes container registry
[[}]]

● raefik edge router [[{network.services,cloud,01_PM.TODO]]
@[https://docs.traefik.io/]
- raefik is an open-source Edge Router that makes publishing your
  services a fun and easy experience. It receives requests on behalf of
  your system and finds out which components are responsible for
  handling them.

- What sets Traefik apart, besides its many features, is that it
  automatically discovers the right configuration for your services.
  The magic happens when Traefik inspects your infrastructure, where it
  finds relevant information and discovers which service serves which
  request.

- Traefik is natively compliant with every major cluster technology,
  such as Kubernetes, Docker, Docker Swarm, AWS, Mesos, Marathon, and
  the list goes on; and can handle many at the same time. (It even
  works for legacy software running on bare metal.)
[[}]]

● K8s Sec. Policies [[{101,security.governance,01_PM.TODO]]
- Security Policies - To define ⅋ control security aspects of Pods,
  use Pod Security Policy (available on v1.15). According Kubernetes
  Documentation, it would enable fine-grained authorization of pod
  creation and updates. Defines a set of conditions that a pod must run
  with in order to be accepted into the system, as well as defaults for
  the related fields. They allow an administrator to control the
  following:
        Running of privileged containers
        Usage of host namespaces
        Usage of host networking and ports
        Usage of volume types
        Usage of the host filesystem
        Restricting escalation to root privileges
        The user and group IDs of the container
        AppArmor or seccomp or sysctl profile used by containers
[[}]]

● Automated Testing of IaC [[{qa.testing,cloud.IaC,01_PM.TODO]]
@[https://qconsf.com/system/files/presentation-slides/qconsf2019-yevgeniy-brikman-automated-testing-for-terraform-docker-packer-kubernetes-and-more.pdf]
https://www.infoq.com/news/2019/11/automated-testing-infrastructure/

- Automated Testing for Terraform, Docker, Packer, Kubernetes, and More
[[}]]

● Microsoft Keda [[{qa.scalability,01_PM.TODO]]
@[https://www.infoq.com/news/2019/11/microsoft-keda-1-0-kubernetes/]
- Microsoft Announces 1.0 Release of Kubernetes-Based Event-Driven Autoscaling (KEDA)
[[}]]

● k8s reality check [[{qa.DONTS,01_PM.TODO]]
https://enterprisersproject.com/article/2019/11/kubernetes-reality-check-3-takeaways-kubecon
[[}]]

● "Hard Way" datadog [[{01_PM.TODO]]
@[https://www.infoq.com/news/2019/12/kubernetes-hard-way-datadog/]
- A traditional architecture approach is to have all Kubernetes master
  components in one server, and have at least three servers for high
  availability. However, these components have different
  responsibilities and can’t or don't need to scale in the same way.
  For instance, the scheduler and the controller are stateless
  components, making them easy to rotate. But the etcd component is
  stateful and needs to have redundant copies of the data. Also,
  components like the scheduler work with an election mechanism where
  only one of their instances is active. Bernaille said that it
  doesn’t make sense to scale out the scheduler.
[[}]]

● Falco Security [[{security.101,security.governance.policies,_doc_has.comparative,01_PM.TODO]]
@[https://www.infoq.com/news/2020/01/falco-security-cncf/]
- Sysdig Falco provides intrusion and abnormality
  (app|container|VM, ...) detection for Kubernetes, Mesosphere,
  Cloud Foundry, ...  protecting from malicious activity.
- 2018: CNCF Sandbox , 2020-01: CNCF Incubation-level.
- It leverages open source Linux kernel instrumentation to monitor
  the stream of system calls from the kernel.
  Running in user-space, it is able to augment the kernel
  data with other input streams such as container runtime metrics and
  Kubernetes metrics alerting on any behaviour that makes Linux system
  calls.
- alerting rules make use of Sysdig's filtering expressions
  to identify potentially suspicious activity.
  (process starting a shell inside a container, container
   running in privileged mode, or an unexpected read of a sensitive
   file).
- Falco can notify via Slack, Fluentd, and NATS.

- Falco rule:
  · condition field: filter applied to each system call.
    e.g.: attempts to start a shell process within a container:
    - rule: shell_in_container
      desc: notice shell activity within a container
      condition: container.id != host and proc.name = bash
      output: shell in a container (user=%user.name
    container_id=%container.id container_name=%container.name
    shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)
      priority: WARNING

- Falco 0.4.0 communication with Kubernetes and Mesos servers is
  possible allowing for creation of policies using the context of those
  frameworks. This allows specifying properties such as container ids,
  image names, Kubernetes namespaces, services, deployments, or Mesos
  frameworks.

- While similar to other tools that allow for declaring security   [comparative]
  policies such as SELinux, AppArmor, or auditd, Falco has some
  differences. As Mark Stemm, software engineer at Sysdig, notes:
  - Tools like seccomp, seccomp-bpf, SELinux, and AppArmor fall into the
    enforcement category in that they will alter the behaviour of
    processes if they are found to violate the defined rules. Falco and
    other tools, such as auditd, fall in the auditing category as they
    will notify when detecting a violation.
  - Falco Falco runs in user space, using a kernel module to
    obtain system calls, while other tools perform system call
    filtering/monitoring at the kernel level." This allows Falco to have
    more available data to be used within its policies as noted
    previously.
[[}]]

● BrewOPA: Admin Policy made easy [[{security.governance,01_PM.low_code,01_PM.TODO]]
@[https://www.zdnet.com/article/kubernetes-administration-policy-made-easy-with-brewopa/]

- Kubernetes administration policy made easy with brewOPA

- Administering policies across Kubernetes and other cloud native environments
  isn't easy. Now, Cyral wants to take the trouble out of the job with brewOPA.
[[}]]

● Kured reboot daemon [[{01_PM.TODO]]
Kured, an open-source reboot daemon for Kubernetes.
Kured runs as a [DaemonSet][aks-daemonset] and monitors each node for the presence of
a file indicating that a reboot is required.
[[}]]

● Harbor Operator [[{01_PM.TODO]]
https://www.infoq.com/news/2020/03/harbor-kubernetes-operator/
[[}]]

● k8s the hard way [[{01_PM.TODO]]
kubernetes-the-hard-way-virtualbox/README.md
https://github.com/sgargel/kubernetes-the-hard-way-virtualbox/blob/master/README.md
[[}]]

● WebAssembly meets Kubernetes with Krustlet [[{01_PM.TODO]]
https://cloudblogs.microsoft.com/opensource/2020/04/07/announcing-krustlet-kubernetes-rust-kubelet-webassembly-wasm/
[[}]]

● App Manager for GCP [[{cloud.gcp,01_PM.TODO]]
New Application Manager Brings GitOps to Google Kubernetes Engine
https://www.infoq.com/news/2020/03/Kubernetes-application-manager/
[[}]]

● K8s operators [[{01_PM.TODO]]
Other common misunderstandings, according to Deo: That Operators are
set-it-and-forget-it propositions (as I mentioned above), and that
Operators require a lot of heavy-duty development effort. For the
latter, yes, someone has to write the Operator, but it doesn’t
necessarily need to be you, thanks to repositories like
OperatorHub.io. Moreover, toolkits and SDKs like Operator Framework
and Kubebuilder can cut down development effort.

Gadi Naor, founder and CTO at Alcide, gives an example, from a
security perspective, of where an Operator might not be the ideal
fit: When a cron job will do the trick just fine.

By design, Naor explains, Operators can provision, update, and delete
Kubernetes resources - which means they are in-cluster privileged
components. “This means that Operators represent a persistent
in-cluster risk; therefore these are components that we must treat
appropriately as far as our threat and risk modeling,” Naor says.

According to Naor, if a task can be appropriately handled with a cron
job, that may be the better choice.

“This reduces the time a privileged component is running without
compromising on the required functionality,” Naor explains. “The
key questions before choosing an Operator are: Do we really need to
implement certain workflows or functionality as an Operator? Can we
achieve the same using a cron job, or use cluster external automation
[[}]]

● Cilium [[{network,security,01_PM.TODO]]
https://github.com/cilium/cilium
Cilium is open source software for providing and transparently securing network connectivity and loadbalancing between application workloads such as application containers or processes
[[}]]

● limits/request by example [[{101,01_PM.TODO]]
Understanding Kubernetes limits and requests by example | Sysdig
https://sysdig.com/blog/kubernetes-limits-requests/
[[}]]

● Okteto [[{01_PM.low_code,01_PM.TODO]]
A Tool to Develop Applications in Kubernetes

https://github.com/okteto/okteto/blob/master/README.md

https://www.infoq.com/news/2020/01/qa-okteto-kubernetes-development/

We created a few guides to get you started with your favorite programming language:
  - ASP.NET Core
  - Golang
  - Java Gradle
  - Java Maven
  - Node
  - PHP
  - Python
  - Ruby
[[}]]

● k8s new features: [[{01_PM.TODO]]
https://lwn.net/Articles/806896/
[[}]]

● Kogito [[{01_PM.TODO]]
https://kogito.kie.org/get-started/
Kogito is a cloud-native business automation technology for building
cloud-ready business applications. The name Kogito derives from the
Latin "Cogito", as in "Cogito, ergo sum" ("I think, therefore I am"),
and is pronounced [ˈkoː.d͡ʒi.to] (KO-jee-to). The letter K has
reference to Kubernetes, the base for OpenShift as the target cloud
platform for Kogito, and to the Knowledge Is Everything (KIE) open
source business automation project from which Kogito originates.

Kogito is designed specifically to excel in a hybrid cloud
environment and to be adaptable to your domain and tooling needs. The
core objective of Kogito is to help you mold a set of business
processes and decisions into your own domain-specific cloud-native
set of services.
Image of business assets moving to cloud services
Figure 1. Business processes and decisions to cloud services

When you are using Kogito, you are building a cloud-native
application as a set of independent domain-specific services,
collaborating to achieve some business value. The processes and
decisions that you use to describe the target behavior are executed
as part of the services that you create. The resulting services are
highly distributed and scalable with no centralized orchestration
service, and the runtime that your service uses is optimized for what
your service needs.

Kogito includes components that are based on well-known business
automation KIE projects, specifically Drools, jBPM, and OptaPlanner,
to offer dependable, open source solutions for business rules,
business processes, and constraint solving.

The following list describes some of the examples provided with Kogito:

    dmn-quarkus-example and dmn-springboot-example: A decision
service (on Quarkus or Spring Boot) that uses DMN to determine driver
penalty and suspension based on traffic violations

    rules-quarkus-helloworld: A Hello World decision service on
Quarkus with a single DRL rule unit

    ruleunit-quarkus-example and ruleunit-springboot-example: A
decision service (on Quarkus or Spring Boot) that uses DRL with rule
units to validate a loan application and that exposes REST operations
to view application status

    process-quarkus-example and process-springboot-example: A process
service (on Quarkus or Spring Boot) for ordering items and that
exposes REST operations to create new orders or to list and delete
active orders

    onboarding-example: A combination of a process service and two
decision services that use DMN and DRL for onboarding new employees

    kogito-travel-agency: A combination of process services and
decision services that use DRL and XLS for travel booking, intended
for deployment on OpenShift
[[}]]

● Cluster Access [[{cluster_admin.kubeconfig,security.aaa,01_PM.TODO.clean]]
@[https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/]
● Kustomization [[{01_PM.TODO}]]
@[https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/]
[[}]]

● Manage Mem./CPU/API Resrc.  [[{101,application.quotas,01_PM.TODO.clean]]
@[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/] Config. Default Memory Requests and Limits for a Namespace
@[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/] Configure Default CPU Requests and Limits for a Namespace
@[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/] Configure Minimum and Maximum Memory Constraints for a Namespace
@[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/] Configure Minimum and Maximum CPU Constraints for a Namespace
@[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/] Configure Memory and CPU Quotas for a Namespace
@[https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/] Configure a Pod Quota for a Namespace
[[}]]

● k8s+okHTTP Load balancing [[{dev_lang.java,network.balancing,application.knative,01_PM.TODO]]
@[https://medium.com/wandera-engineering/kubernetes-network-load-balancing-using-okhttp-client-54a2db8fc668]
[[}]]

● lightbitslab [[{storage.101,storage.nvme,01_PM.TODO]]
- Persistent storage for Kubernetes is readily available but persistent
  storage that performs like local NVMe flash, not so much.
[[}]]

● git-sync [[{qa.gitops,application.sidecar,01_PM.TODO]]
@[https://github.com/kubernetes/git-sync]
- git-sync is a simple command that pulls a git HEAD|tag|hash repo into
  a local dir. It is a perfect "sidecar" container in Kubernetes - it can
  periodically pull files down from a repository so that an application
  can consume them.
  It will only re-pull if the target of the run has changed in
  the upstream repository. When it re-pulls, it updates the destination
  directory atomically. In order to do this, it uses a git worktree in
  a subdirectory of the --root and flips a symlink.
- A webhook call upon successful git repo synch is allowed after the
  symlink is updated.
[[}]]

● Q⅋A with K8s...  [[{101,qa,01_PM.TODO]]
@[https://www.infoq.com/news/2018/02/dist-system-patterns-burns]
- Distributed Systems programming is not for the faint of heart, and despite
  the evolution of platforms and tools from COM, CORBA, RMI, Java EE, Web
  Services, Services Oriented Architecture (SOA) and so on, it's more of an art
  than a science.
- Brendan Burns outlined many of the patterns that enables distributed systems
  programming in the blog he wrote in 2015. He and David Oppenheimer, both
  original contributors for Kubernetes, presented a paper at Usenix based
  around design patterns and containers shortly after.
  InfoQ caught up with Burns, who recently authored an ebook titled Designing
  Distributed Systems, Patterns and Paradigms for Scaleable Microservices. He
  talks about distributed systems patterns and how containers enable it.
[[}]]

● Atlassian escalator [[{101,qa.scalability,01_PM.TODO]]
@[https://www.infoq.com/news/2018/05/atlassian-kubernetes-autoscaler]
- In Kubernetes, scaling can mean different things to different users. We
  distinguish between two cases:

- Cluster scaling, sometimes called infrastructure-level scaling,
  refers to the (automated) process of adding or removing worker nodes
  based on cluster utilization.
- Application-level scaling, sometimes called pod scaling, refers to
  the (automated) process of manipulating pod characteristics based on
  a variety of metrics, from low-level signals such as CPU utilization
  to higher-level ones, such as HTTP requests served per second, for a
  given pod. Two kinds of pod-level scalers exist:
- Horizontal Pod Autoscalers (HPAs), which increase or decrease the
  number of pod replicas depending on certain metrics.
- Vertical Pod Autoscalers (VPAs), which increase or decrease the
  resource requirements of containers running in a pod.

- Atlassian released their in-house OOSS Escalator tool providing
  configuration-driven preemptive scale-up and faster scale-down for
  k8s nodes.
@[https://developers.atlassian.com/blog/2018/05/introducing-escalator/]
@[https://github.com/atlassian/escalator/]

- Kubernetes has two autoscalers:
  -ºhorizontal pod autoscalerº:
  @[https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/]
  Bºpods can scale down very quicklyº.
  -ºcluster autoscalerº to scales the compute infrastructure itself.
  @[https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler]
    Understandably, it takes a longer time to scale up and down since
    VMs must be created/recycled.
  RºDelays in cluster autoscaler translate to delays in the pod autoscalerº.
- Atlassian's problem was very specific to batch workloads, with a low tolerance
  for delay in scaling up and down forcing to write their own autoscaling functionality
  to solve these problems on top of Kubernetes.

- "Escalator" configurable thresholds for upper and lower capacity of
  the compute VMs.
 @[https://github.com/atlassian/escalator/blob/master/docs/configuration/advanced-configuration.md]
- Some of the configuration properties work by modifying a Kubernetes
  feature called º"taint"º:
[[}]]

● ArgoCD [[{application.development,qa.gitops,01_PM.TODO]]
@[https://argoproj.github.io/argo-cd/]
- Argo CD: declarative, GitOps continuous delivery tool for Kubernetes.
  Application definitions, configurations, and environments should be declarative
  and version controlled. Application deployment and lifecycle management should
  be automated, auditable, and easy to understand.
[[}]]

● Multi-Zone Clusters: [[{cluster_admin.multizone,cloud,01_PM.TODO]]
@[https://kubernetes.io/docs/setup/best-practices/multiple-zones/]

- See also:
  Kubernetes 1.12 introduces topology-aware dynamic provisioning beta,
  which aims to improve the regional cluster experience for stateful
  workloads. It means Kubernetes now understands the inherent zonal
  restrictions of Compute Engine Persistent Disks (PDs) and Regional
  PD, and provisions them in the zone that is best suited to run the
  pod. Another addition to topology is the Container Storage Interface
  (CSI) plugin, which is intended to make it easier for third party
  developers to write and deploy volume plugins exposing new storage
  systems in Kubernetes.
[[}]]

● Kubelet TLS Bootstrap [[{cluster_admin.bootstrap.tls,01_PM.TODO]]
(k8s 1.12+)
@[https://github.com/kubernetes/enhancements/issues/43]
kubelet generates a private key and a CSR for submission to a
cluster-level certificate signing process.
[[}]]

● Volume Snapshot,Restore [[{storage.101,01_PM.TODO]]
- Volume Snapshot/Restore: k8s 1.12+
  @[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support]


- TODO: Storage/Persistence: @ma
@[https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/]
@[https://kubernetes.io/docs/concepts/storage/storage-classes/]
@[https://kubernetes.io/docs/concepts/storage/volume-snapshot-classes/]
@[https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/]
@[https://kubernetes.io/docs/concepts/storage/storage-limits/]
[[}]]


● What's New [[{01_PM.ext_resource,qa.scalability,01_PM.TODO]]
● 1.12
@[https://www.infoq.com/news/2018/10/kubernetes-1-12]
- Kubernetes 1.12 Brings:
  - Horizontal Pod Autoscaler: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    The Horizontal Pod Autoscaler automatically scales the number of pods
    in a replication controller, deployment, replica set or stateful set
    based on observed CPU utilization (or, with custom metrics support,
    on some other application-provided metrics). Note that Horizontal Pod
    Autoscaling does not apply to objects that can't be scaled, for
    example, DaemonSets.

  - Support for the Container Storage Interface (CSI) plugin:
  @[https://kubernetes.io/blog/2018/04/10/container-storage-interface-beta/]

[[}]]

● k8s on a Pine64 ARM (4GB RAM) [[{cluster_admin.embedded,01_PM.TODO]]
@[https://itnext.io/building-an-arm-kubernetes-cluster-ef31032636f9]
[[}]]


● Monitoring .NET Apps [[{dev_lang.c#,application.knative,qa,monitoring,01_PM.TODO]]
@[https://developers.redhat.com/blog/2020/08/05/monitoring-net-core-applications-on-kubernetes/]
[[}]]

● JKube: Deploying Java Apps [[{dev_lang.java,01_PM.TODO]]
Deploy your Java web application into the cloud using Eclipse JKube
@[https://developers.redhat.com/blog/2020/07/27/deploy-your-java-web-application-into-the-cloud-using-eclipse-jkube/]
[[}]]

● Best Practices @ma [[{qa.best_practices,_doc_has.comparative,01_PM.TODO,]]
@[https://dzone.com/articles/best-practices-for-advanced-deployment-patterns]
(Rolling, Blue/Green, Canary, BigBan,...)
[[}]]

● 5 things to remember [[{101,01_PM.TODO]]
Managing Kubernetes resources: 5 things to remember | The Enterprisers Project
@[https://enterprisersproject.com/article/2020/8/managing-kubernetes-resources-5-things-remember]
[[}]]

● flagger: Progressive k8s operator [[{application.operators,01_PM.low_code,01_PM.TODO]]
@[https://github.com/weaveworks/flagger ]
weaveworks/flagger: Progressive delivery Kubernetes operator
Canary → A/B Testing → Blue/Green deployments

Flagger is a progressive delivery tool that automates the release
process for applications running on Kubernetes.It reduces the risk of
introducing a new software version in productionby gradually shifting
traffic to the new version while measuring metrics and running
conformance tests
[[}]]

● AWS Controllers k8s [[{cloud.aws,01_PM.TODO]]
Amazon Announces the Preview of AWS Controllers for Kubernetes (ACK)
https://www.infoq.com/news/2020/09/aws-controllers-k8s-preview
[[}]]

● kubervisor [[{qa,01_PM.TODO]]
https://github.com/AmadeusITGroup/kubervisor
AmadeusITGroup/kubervisor: The Kubervisor allow you to control which
pods should receive traffic or not based on anomaly detection.
It is a new kind of health check system.
[[}]]

● Canary Automation: [[{qa,01_PM.TODO]]
@[https://github.com/AmadeusITGroup/kanary]

The goal of Kanary project is to bring full automation of Canary
Deployment to kubernetes. So far only rolling update is automated and
fully integrated to Kubernetes. Using CRD (Custom Resource
Definition) and an associated controller, this project will allow you
to define your Canary deployment and chain it with a classic rolling
update in case of success.
[[}]]

● Namespace ("Virtual Clusters") HOW-TO summary [[{101.namespace]]
@[https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/]
@[https://kubernetes.io/docs/tasks/administer-cluster/namespaces/]
- Namespace: scope for names allowing for "virtual clusters"
- Can be assigned max Memory/CPU Quotas
- Useful to divide 1 Kubernetes into N logical ones isolated by
  teams and/or projects and/or clients and/or ...
- use labels inside a given Namespace to further group by
  sub-team, sub-project, role-in-project ....
- Services are created with DNS entry
  "service-name"."namespace".svc.cluster.local

  $ kubectl get namespaces
  NAME          STATUS    AGE
  default       Active    1d
  kube-system   Active    1d

  $  kubectl get pods --all-namespaces   <··· by default only pods in active ns is listed

  $ kubectl create namespace namespc01
·
  $ kubectl --namespace=namespc01 run \ ← Run on given N.S.
            nginx --image=nginx

  $ kubectl config set-context \        ← permanently save NS
    $(kubectl config current-context) \ ← $(...) bash syntax
    --namespace=MyFavouriteNS

  $ kubectl config view | \
    grep namespace: # Validate

- See alos: kubens (by Ahmet Alp Balkan)
  script wrapper to easily switch between Kubernetes namespaces.
  $ kubens foo # activate namespace.
  $ kubens -   # back to previous value.
 @[https://github.com/ahmetb/kubectx]


[[}]]

● Knative [[{01_PM.low_code,qa.gitops,01_PM.TODO]]
https://cloud.google.com/knative/
Kubernetes-based platform to build, deploy, and manage modern serverless workloads.
Essential base primitives for all

- set of middleware components to build source-centric+container-based apps
  that can run anywhere: on premises or cloud applying best practices shared
  by successful real-world Kubernetes-based frameworks.
- It enables developers to focus on writing interesting code, without worrying [low_code]
  about the “boring but difficult” parts of building, deploying, and managing
  Askingan application.
- solve mundane but difficult tasks such as orchestrating source-to-container workflows,
  routing and managing traffic during deployment, auto-scaling your workloads,
  or binding running services to eventing ecosystems.
- It supports common development patterns such GitOps, DockerOps, ManualOps, ...Django,
  Ruby on Rails, Spring, ...
- Operator-friendly
[[}]]

● kubectl recipes [[{101.kubectl,01_PM.TODO]]
  $ kubectl get events --all-namespaces
NAMESPACE     LAST SEEN   TYPE      REASON                    OBJECT                              MESSAGE
default       29d         Normal    NodeNotReady              node/node1                          Node node1 status is now: NodeNotReady
default       7d4h        Normal    Starting                  node/node1                          Starting kubelet.
default       7d4h        Normal    NodeHasSufficientMemory   node/node1                          Node node1 status is now: NodeHasSufficientMemory
default       7d4h        Normal    NodeHasNoDiskPressure     node/node1                          Node node1 status is now: NodeHasNoDiskPressure
...
kube-system   7d4h        Normal    Pulled                    pod/calico-kube-controllers-...     Container image ".../kube-controllers:v3.15.1" already present ...
kube-system   7d4h        Normal    Created                   pod/calico-kube-controllers-...     Created container calico-kube-controllers
kube-system   7d4h        Normal    Started                   pod/calico-kube-controllers-...     Started container calico-kube-controllers
...
kube-system   7d4h      RºWarningº  FailedCreatePodSandBox    pod/dns-autoscaler-66498f5c5f-dn458 Failed to create pod sandbox: rpc error: code ...
...
kube-system   7d4h        Normal    LeaderElection            endpoints/kube-controller-manager   node1_e4a58997-c39f-430d-a942-31d53124c5d5 became leader
kube-system   7d4h        Normal    LeaderElection            lease/kube-controller-manager       node1_e4a58997-c39f-430d-a942-31d53124c5d5 became leader
...
kube-system   34m       RºWarningº  FailedMount               pod/kubernetes-dashboard-...        MountVolume.SetUp failed for volume "kubernetes-dashboard-certs"...


$ kubectl get endpoints --all-namespaces
  NAMESPACE     NAME                        ENDPOINTS                                           AGE
  default       kubernetes                  192.168.1.2:6443                                    202d
  kube-system   coredns                     10.233.90.23:53,10.233.90.23:9153,10.233.90.23:53   202d
  kube-system   dashboard-metrics-scraper   10.233.90.24:8000                                   202d
  kube-system   kube-controller-manager     <none>                                              202d
  kube-system   kube-scheduler              <none>                                              202d
  kube-system   kubernetes-dashboard        10.233.90.22:8443                                   202d

$ kubectl get deployments --all-namespaces
  NAMESPACE     NAME                         READY   UP-TO-DATE   AVAILABLE   AGE
  kube-system   calico-kube-controllers      1/1     1            1           202d
  kube-system   coredns                      1/2     2            1           202d
  kube-system   dns-autoscaler               1/1     1            1           202d
  kube-system   kubernetes-dashboard         1/1     1            1           202d
  kube-system   kubernetes-metrics-scraper   1/1     1            1           202d
[[}]]

● Dexter OpenId [[{security.AAA,01_PM.low_code,01_PM.TODO]]
@[https://github.com/gini/dexter]
  low_code tool for authenticating kubectl users with OpenId Connect.
@[https://github.com/tldr-pages/tldr/blob/master/pages/common/dexter.md]
  $ dexter auth  \     ← Create and authenticate a user
   -i $client_id \       with Google OIDC.
   -s $client_secret     Use --kube-config dir1/config to
                         override default kube config location
[[}]]

● network policy providers [[{cloud.sdn,01_PM.TODO]]
- declare network policy
  cilium-network-policy
  kube-router-network-policy
  romana-network-policy
  weave-network-policy
[[}]]


● Install Service Catalog [[{cluster_admin.helm.service_catalog,01_PM.TODO]]
- Install Service Catalog using Helm:
@[https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/]
- Install Service Catalog using SC:
@[https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/]
[[}]]


● Compute, Storage, and Networking Extensions [[{cluster_admin,01_PM.TODO]]
@[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/]
@[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/]
@[https://kubernetes.io/docs/concepts/extend-kubernetes/service-catalog/]
[[}]]


● Admin Tasks [[{cluster_admin,01_PM.TODO]]
@[https://kubernetes.io/docs/reference/#config-reference]
@[https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/]
@[https://kubernetes.io/docs/concepts/cluster-administration/certificates/]
@[https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/]
@[https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/]
@[https://kubernetes.io/docs/concepts/cluster-administration/networking/]
@[https://kubernetes.io/docs/concepts/cluster-administration/logging/]
@[https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/]
@[https://kubernetes.io/docs/concepts/cluster-administration/proxies/]
@[https://kubernetes.io/docs/concepts/cluster-administration/controller-metrics/]
@[https://kubernetes.io/docs/concepts/cluster-administration/addons/]

● Config. files
  (documented in the Reference section of the online documentation, under each binary:)
  @[https://kubernetes.io/docs/admin/kubelet/]
  @[https://kubernetes.io/docs/admin/kube-apiserver/]
  @[https://kubernetes.io/docs/admin/kube-controller-manager/]
  @[https://kubernetes.io/docs/admin/kube-scheduler/]
[[}]]

● Garbage Collection:
@[https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/]

● https://github.com/tldr-pages/tldr/blob/master/pages/common/kompose.md
  A tool to convert docker-compose applications to Kubernetes. More
  information: https://github.com/kubernetes/kompose.

● KOPS: Deploy on Cloud [[{cluster_admin,cloud.aws,cloud.gce,01_PM.low_code,01_PM.TODO]]
@[https://github.com/tldr-pages/tldr/blob/master/pages/common/kops.md]  [TODO]

@[https://github.com/kubernetes/kops/blob/master/README.md]
@[https://github.com/kubernetes/kops/blob/master/README.md]
- Kops cli-tool allows the automation of production-grade k8s cluster
  (creation/destruction, upgrade, maintenance).
- AWS is currently officially supported, with GCE in beta support,
  and VMware vSphere in alpha, and other platforms planned.
- It does NOT support standard linux distros on bare-metal/VMs.
  Kubespray is prefered in this case.
[[}]]

● https://github.com/tldr-pages/tldr/blob/master/pages/common/k8s-unused-secret-detector.md

  Command line interface tool for detecting unused Kubernetes secrets.
  More information: https://github.com/dtan4/k8s-unused-secret-detector.

  https://github.com/tldr-pages/tldr/blob/master/pages/common/k8sec.md

  Command line interface tool to manage Kubernetes secrets.
  More information: https://github.com/dtan4/k8sec.

● k0s: [[{01_PM.TODO.NOW]]
@[https://docs.k0sproject.io/]

    all-inclusive Kubernetes distribution, which is configured with all
  of the features needed to build a Kubernetes cluster. Due to its
  simple design, flexible deployment options and modest system
  requirements, k0s is well suited for
  - Any cloud
  - Bare metal
  - Edge and IoT

  k0s drastically reduces the complexity of installing and running a
  CNCF certified Kubernetes distribution. With k0s new clusters can be
  bootstrapped in minutes and developer friction is reduced to zero.
  This allows anyone with no special skills or expertise in Kubernetes
  to easily get started.

  k0s is distributed as a single binary with zero host OS dependencies
  besides the host OS kernel. It works with any Linux without
  additional software packages or configuration. Any security
  vulnerabilities or performance issues can be fixed directly in the
  k0s distribution that makes it extremely straightforward to keep the
  clusters up-to-date and secure.
[[}]]

● https://github.com/kubernetes-sigs/kind  [[{101,01_PM.TODO]]
  https://github.com/tldr-pages/tldr/blob/master/pages/common/kind.md
  Tool for running local Kubernetes clusters using Docker container
  "nodes". Designed for testing Kubernetes itself, but may be used for
  local development or continuous integration. More information:
  https://github.com/kubernetes-sigs/kind. [[}]]

● Cluster Federation [[{cluster_admin.federation,01_PM.TODO]]
@[https://kubernetes.io/docs/concepts/cluster-administration/federation/]

 Federation API:
 - @[https://kubernetes.io/docs/reference/federation/extensions/v1beta1/definitions/]
 - @[https://kubernetes.io/docs/reference/federation/extensions/v1beta1/operations/]
 - @[https://kubernetes.io/docs/reference/federation/v1/definitions/]
 - @[https://kubernetes.io/docs/reference/federation/v1/operations/]

 EXTERNAL REFERENCES:
- The K8s Bible for Beginners and developers:
@[https://docs.google.com/document/d/1O-BwDTuE4qI0ASE7iFp6qFpTj8uIVrl9F0HUrC4u_GQ/]

- https://kubernetes.io/docs/reference/command-line-tools-reference/federation-apiserver/
- https://kubernetes.io/docs/reference/command-line-tools-reference/federation-controller-manager/
- https://kubernetes.io/docs/tasks/administer-federation/cluster/
- https://kubernetes.io/docs/tasks/administer-federation/configmap/
- https://kubernetes.io/docs/tasks/administer-federation/daemonset/
- https://kubernetes.io/docs/tasks/administer-federation/deployment/
- https://kubernetes.io/docs/tasks/administer-federation/events/
- https://kubernetes.io/docs/tasks/administer-federation/hpa/
- https://kubernetes.io/docs/tasks/administer-federation/ingress/
- https://kubernetes.io/docs/tasks/administer-federation/job/
- https://kubernetes.io/docs/tasks/administer-federation/namespaces/
- https://kubernetes.io/docs/tasks/administer-federation/replicaset/
- https://kubernetes.io/docs/tasks/administer-federation/secret/
- https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/
- https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/

 SECRETS
- @[https://kubernetes.io/docs/tasks/administer-federation/secret/]

● kubefed(eration):
Controls cluster federation
- Cross-cluster Service Discovery using Federated Services
  https://kubernetes.io/docs/tasks/federation/federation-service-discovery/
- https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/
- https://kubernetes.io/docs/tasks/federation/set-up-coredns-provider-federation/
- https://kubernetes.io/docs/tasks/federation/set-up-placement-policies-federation/
REFERENCE:
@[https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed/]
@[https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-options/]
@[https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-init/]
@[https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-join/]
@[https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-unjoin/]
@[https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-version/]
● Federated Services
@[https://kubernetes.io/docs/tasks/federation/federation-service-discovery/]
● Using the k8s API
https://kubernetes.io/docs/reference/using-api/api-overview/
● Accessing the API
https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/
https://kubernetes.io/docs/reference/access-authn-authz/authentication/
https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/
https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/
https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/
https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/
https://kubernetes.io/docs/reference/access-authn-authz/authorization/
https://kubernetes.io/docs/reference/access-authn-authz/rbac/
https://kubernetes.io/docs/reference/access-authn-authz/abac/
https://kubernetes.io/docs/reference/access-authn-authz/node/
https://kubernetes.io/docs/reference/access-authn-authz/webhook/
[[}]]

● https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/  [[{02_doc_has.diagram]]
  ┌──────┐                   ┌──────────┐
  │docker│                   │kubernetes│
  └──┬───┘                   └────┬─────┘
     │                            │
     │                            │
     │                       ┌────▼─────┐   CRI: K8s API
     │                       │ Container│
     │                       │   Runtime│
     │                       │ Interface│
     │                       └─┬──┬─────┘
     │                         │  │
     │        ┌────────────────┘  │
     │        │                   │
     │    ┌───▼──┐                │
     │    │  CRI │                │         CRI "Implementation"
     │    │plugin│                │         - containerd: From docker Comp.
   ┌─▼────┴─────┬┘             ┌──▼──┐        manages storage and network,
   │ containerd │              │CRI-O│        supervises running containers
   └──────┬─────┘              └──┬──┘      - CRI-O: From IBM/RedHat/ ...
          │                       │
          │                       │
          │                       │
┌─────────▼───────────────────────▼───┐     Standard spec for container
│ Open Container Initiative (OCI) spec│     *Images* and running container
└───────────┬─────────────────────────┘
            │
            │
         ┌──▼───┐                           OCI reference tool for spawing
         │ runc │                           and running containers.
         └──┬───┘
            │
       ┌────┴──────┐
       │           │
       │           │
   ┌───▼─────┐  ┌──▼──────┐
   │container│  │container│                 Containers
   └─────────┘  └─────────┘
                                                                       [[02_doc_has.diagram}]]

● Identity and Access Management Made Easier With D2iQ
  AAA,governance,multi-cloud
  Kubernetes Identity and Access Management Made Easier With D2iQ

As teams expand their usage of Kubernetes, clusters and workloads
will exist in different environments. One team may be building their
stack on cloud provider “A,” while another team is building a
stack on cloud provider “B.” Even on a single public cloud
service, clusters may exist in different environments, and IT isn’t
even aware of these things. This makes tracking all of the individual
logins and permissions across the organization next to impossible,
especially when there are multiple accounts and access levels to
manage. And the problem only grows in complexity as more people
on-board, off-board, or change teams, and projects multiply.

● ... you might consider using DNS records relying on a round-robin
  namere solution ... However, this approach is not the best fit
  for cloud environments because the  TOPOLOGY CHANGES TOO OFTEN.
  Some DNS implementations cache the result for too long....
  there’s a high chance of using a hostname/IPaddress resolution
  no longer valid.
  Service discovery requires a different solution (etcd and proxies).

  A Service object is an abstraction targeting a set of Pods (typically
  using labels) and defining the access policy.
  → The Service name is then resolved to the IP address of the Service
    itself by a local DNS server running in the Kubernetes Control Plane.
    → After resolving the Service name to its IP address, Kubernetes
      relies on kube-proxies (installed on each work-node) which intercept
      the request to the Service object and forwards it to one of the
      Pods targeted by the Service. (by updating the info from etcd).
    BºNOTE: there is no DNS resolution involved in this step.º

K8s: Trigger a Kubernetes HPA with Prometheus metrics
Sysdig https://sysdig.com/blog/kubernetes-hpa-prometheus/ 
Keda is an open source project that allows using Prometheus queries,
along with multiple other scalers, to scale Kubernetes pods. 

● Karmada: multicloud k8s low_code
https://www.infoq.com/news/2021/08/karmada-kubernetes-orchestration/
Karmada (Kubernetes Armada) is designed for multi-cloud and multi-cluster
Kubernetes orchestration with a Kubernetes-native implementation. It
provides centralized multi-cloud management, high availability,
failure recovery, and traffic scheduling, which enables users to run
their cloud-native applications across multiple Kubernetes clusters
and clouds with no changes to applications.


Kubernetes Federation is the only official multi-cluster management
solution that the SIG Multicluster initiated and maintained. After
two major versions, KubeFed has a resilient design and architecture.
Karmada is developed in continuation of Kubernetes Federation v1 and
v2 and inherited some basic concepts from these two versions.
However, Federation v2 (KubeFed) is still in beta and some of the
features are still in alpha. Its lack of maturity and community
activity are blocking a lot of users from adopting it in production.


Karmada is fully compatible with the Kubernetes native API as it
evolved from KubeFed, so users can deploy their cloud-native
applications from single-cluster to multi-cluster with zero changes.

It can be installed within minutes.

● Kubestriker: A security auditing tool for Kubernetes clusters - Help Net Security
  https://www.helpnetsecurity.com/2021/05/04/security-kubernetes/ 

● Kustomize vs Helm vs Kubes: Kubernetes Deploy Tools
https://blog.boltops.com/2020/11/05/kustomize-vs-helm-vs-kubes-kubernetes-deploy-tools 
"...Kubes is another tool that handles deployment. Kubes has some similar concepts to both Kustomize and Helm and improves on them."

...Kubes introduces a conventional folder structure. Conventions takes you a long way." 

With Helm, you can also add templating methods with custom helpers.
The helper method definitions are awkward looking, though.
Example:templates/_helpers.tpl
  {{- define "demo.serviceAccountName" -}}
    {{- if .Values.serviceAccount.create }}
    {{- default (include "demo.fullname" .) .Values.serviceAccount.name }}
  {{- else }}
   {{- default "default" .Values.serviceAccount.name }}
  {{- end }}
  {{- end }}

  With Kubes, custom template helper definitions is just Ruby code.
  Example:.kubes/helpers/my_helpers.rbmodule MyHelpers
  def database_endpoint
    case Kubes.env
    when "dev"
      "dev-db.cbuqdmc3nqvb.us-west-2.rds.amazonaws.com"
    when "prod"
      "prod-db.cbuqdmc3nqvb.us-west-2.rds.amazonaws.com"
    end

  Helm supports packaging, Kubes no.
   Kubes supports OCI Image builds, helm doesn't

● Kubernetes Failure Stories
  https://k8s.af/

● The Evolution of Distributed Systems on Kubernetes
  https://www.infoq.com/articles/distributed-systems-kubernetes/

● simplenetes-io/simplenetes: The sns tool is used to manage the
  full life cycle of your Simplenetes clusters. It integrates with
  the Simplenetes Podcompiler project podc to compile pods.
  https://github.com/simplenetes-io/simplenetes

● datawire/ambassador: open source Kubernetes-native API gateway for
  microservices built on the Envoy Proxy
  https://github.com/datawire/ambassador

● Kubernetes configuration patterns, Part 2: Patterns for Kubernetes controllers | Red Hat Developer
  https://developers.redhat.com/blog/2021/05/05/kubernetes-configuration-patterns-part-2-patterns-for-kubernetes-controllers

● AWS Releases Multi-Cloud Kubernetes Autoscaler Karpenter
  https://www.infoq.com/news/2022/01/karpenter-kubernetes-autoscaler/
  This improves upon their Kubernetes Cluster Autoscaler by providing a
  easily configurable, fully automated scheduler. Karpenter is able to
  monitor for unscheduled pods and launch new nodes as well as
  terminate unneeded infrastructure. Karpenter is designed to work with
  any Kubernetes cluster in any environment.

  Karpenter is able to observe the aggregate resource requests of
  unscheduled pods. Using this it makes decisions to launch or
  terminate as needed to optimize cluster performance and cost.
  According to Channy Yun, Principal Developer Advocate with AWS,
  Karpenter "performs particularly well for use cases that require
  rapid provisioning and deprovisioning large numbers of diverse
  compute resources quickly".

● https://www.infoq.com/news/2022/01/ebpf-wasm-service-mesh/
  By Dec 2021, the WasmEdge community of contributors demonstrated that
  WasmEdge-based microservices can work with Dapr and Linkerd sidecars
  as an alternative to heavyweight full-blown Linux containers with
  guest OS and full software stacks. The WebAssembly microservices
  consume 1% of resources, and cold starts in 1% of the time compared
  with Linux container applications.

● https://github.com/Liquid-Reply/kind-crun-wasm
This repository shows how to run WASM on KinD. It is build on the
awesome work of https://github.com/second-state/wasmedge-containers-examples

- To get started:
  $ IMAGE=ghcr.io/liquid-reply/kind-crun-wasm:v1.23.0
  $ kind create cluster --image $IMAGE   ← Create "WASM in KinD" Cluster
                                           kind-crun-wasm replaces runc with crun,
                                           allowing to run wasm workloads.

  $ kubectl run -it --rm --restart=Never \                ← Test WASM capabilities
    wasi-demo \
    --image=hydai/wasm-wasi-example:with-wasm-annotation \
    --annotations="module.wasm.image/variant=compat" \
    /wasi_example_main.wasm 50000000

- To create new WASM containersstart here:
   https://wasmedge.org/book/en/kubernetes/demo.html


@[https://www.infoq.com/news/2022/01/ebpf-wasm-service-mesh/]:
   - Kubernetes
     KubeEdge   SuperEdge   OpenYurt
    ºk8sº  k3s  minikube    kind  microk8s

   - High-level container runtimes (e.g, CRI runtimes)
    ºCRI-Oº containerd  Podman  docker

   - Low-level container runtimes (OCI runtimes)
    ºcrunº runc  gVisor   kata-runtime


   - Linux container images    ºWebAssembly app Imagesº

● Developping and Debugging locally
https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/"

● https://www.infoq.com/news/2019/03/airbnb-kubernetes-workflow
  airbnb workflow

● audits log events: (1.11+)

● k8s DNS

● private Reg.  [[{cluster_admin.image_registry,security.secret_mng,network,01_PM.TODO]]
- using a private (image) registry:
@[https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry]
- Pull Image from a Private Registry:
@[https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/]
- imagePullSecrets:
@[https://kubernetes.io/docs/concepts/configuration/secret/]
- method to pass a secret that contains a Docker image registry password
  to the Kubelet so it can pull a private image on behalf of your Pod.
[[}]]

Recheck: cluster_admin....  cluster_admin.bootstrap

● Kubernetes-native apps: K8s offers a simple Endpoints API
- non-native        apps: K8s offers virtual-IP-based-2-Service bridge

● To get External IPs of all nodes:
  $ kubectl get nodes -o jsonpath=\
    '{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'

● There are lot of components in Kubernetes but the most   [[{101,01_PM.TODO]]
  important components that are most used would be
  https://medium.com/geekculture/howgT-to-deploy-spring-boot-and-mongodb-to-kubernetes-minikube-71c92c273d5e
[[}]]

● @[https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/]

● Core Kubernetes Types:
  https://github.com/kubernetes/api/blob/master/core/v1/types.go

● https://kubernetes.io/docs/concepts/extend-kubernetes/operator/

● Dynamic Volume Provisioning [[{101,storage.101,01_PM.TODO.clean]]
@[https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/]

● Node-specific Volume Limits
@[https://kubernetes.io/docs/concepts/storage/storage-limits/]
[[}]]

● Cluster Debug/Troubleshoot [[{cluster_admin.troubleshooting,01_PM.TODO.clean]]
See also:
- @[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/]
- @[https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/]
[[}]]


@[https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/]
  - A VM node can be ‘tainted’ (marked) with a certain value so
    that pods with a related marker are not scheduled onto it. Unused
    nodes would be brought down faster by the Kubernetes standard cluster
    autoscaler when they are marked. The scale-up configuration parameter
    is a threshold expressed as a percentage of utilization, usually less
    than 100 so that there is a buffer.  Escalator autoscales the compute
    VMs when utilization reaches the threshold, thus making room for
    containers that might come up later, and allowing them to boot up
    fast.

● [{]]
NON-CLASSIFIED                   CLUSTER: NODE RELATED:
==============                   ======================
DownwardAPIProjection            Node
ExecAction                       NodeAddress
KeyToPath                        NodeAffinity
LocalObjectReference             NodeCondition
ObjectFieldSelector              NodeConfigSource
ObjectReference                  NodeConfigStatus
Preconditions                    NodeDaemonEndpoints
ResourceFieldSelector            NodeList
ResourceRequirements             NodeProxyOptions
ScopedResourceSelerRequirement   NodeResources
ScopeSelector                    NodeSelector
SecurityContext                  NodeSelectorRequirement
SELinuxOptions                   NodeSelectorTerm
SerializedReference              NodeSpec
Sysctl                           NodeStatus
TypedLocalObjectReference        NodeSystemInfo
WindowsSecurityContextOptions    Lifecycle

SCHEDULING-POD:                   CLUSTER-NAMESPACE
===============                   ==================
PreferredSchedulingTerm           Namespace
TopologySelectorLabelRequirement  NamespaceList
TopologySelectorTerm              NamespaceSpec
Event                             NamespaceStatus
EventList                         -- quotas ---
EventSeries                       ResourceQuota
EventSource                       ResourceQuotaList
ComponentCondition                ResourceQuotaSpec
ComponentStatus                   ResourceQuotaStatus
ComponentStatusList
ReplicationController
ReplicationControllerCondition
ReplicationControllerList
ReplicationControllerSpec
ReplicationControllerStatus
Taint
Toleration

VOLUME RELATED:                      NETWORK-RELATED:
==============================       ================
Binding                              DaemonEndpoint
Volume                               EndpointAddress
VolumeSource                         EndpointPort
VolumeNodeAffinity                   Endpoints
PersistentVolume                     EndpointsList
PersistentVolumeClaim                EndpointSubset
PersistentVolumeClaimCondition       Service
PersistentVolumeClaimList            ServiceAccount
PersistentVolumeClaimSpec            ServiceAccountList
PersistentVolumeClaimStatus          ServiceAccountTokenProjection
PersistentVolumeClaimVolumeSource    ServiceList
PersistentVolumeList                 ServicePort
PersistentVolumeSource               ServiceProxyOptions
PersistentVolumeSpec                 ServiceSpec
PersistentVolumeStatus               ServiceStatus
HostPathVolumeSource                 ClientIPConfig
EmptyDirVolumeSource                 HTTPGetAction
GCEPersistentDiskVolumeSource        HTTPHeader
FCVolumeSource                       TCPSocketAction
Flex(Persistent)VolumeSource         HostAlias
GitRepoVolumeSource                  LoadBalancerIngress
SecretVolumeSource                   LoadBalancerStatus
NFSVolumeSource
QuobyteVolumeSource
DownwardAPIVolumeSource
DownwardAPIVolumeFile
VsphereVirtualDiskVolumeSource
PhotonPersistentDiskVolumeSource
PortworxVolumeSource
ConfigMapVolumeSource
ProjectedVolumeSource
VolumeMount
VolumeDevice
AttachedVolume
Glusterfs* RBD* Cinder*
CephFSV* Flocker* Azure*
ScaleIO* StorageOS* CSI*
AWSElasticBlock* ISCSI*


POD RELATED            CONFIG RELATED:            CONTAINER RELATED          Limits
===========            ==============             =================          =============
Pod                    ConfigMapProjection        ContainerPort              LimitRange
PodAffinity            ConfigMapKeySelector       ContainerStateWaiting      LimitRangeItem
PodAffinityTerm        ConfigMapEnvSource         ContainerStateRunning      LimitRangeList
PodAntiAffinity        SessionAffinityConfig      ContainerStateTerminated   LimitRangeSpec
Affinity               ConfigMapNodeConfigSource  ContainerState             RangeAllocation
PodAttachOptions       ConfigMap                  ContainerStatus
PodCondition           ConfigMapList              ContainerImage
PodDNSConfig           EnvFromSource              Capabilities
PodDNSConfigOption     EnvVar                     Handler (Container Life-cicle)
PodExecOptions         EnvVarSource               Probe
PodList                ── secret related ───
PodLogOptions          SecretProjection
PodPortForwardOptions  SecretReference
PodProxyOptions        SecretKeySelector
PodReadinessGate       SecretEnvSource
PodSecurityContext     Secret
PodSignature           SecretList
PodSpec
PodStatus
PodStatusResult
PodTemplate
PodTemplateList
PodTemplateSpec
WeightedPodAffinityTerm
PreferAvoidPodsEntry
AvoidPods
[[}]]

● pod scheduling:
  At present, there are two kinds of affinity/anti-affinity: required
  and preferred. Required affinities will stop a pod from scheduling if
  the affinity rules cannot be met by any node. With preferred
  affinities, on the other hand, a pod will still be scheduled even if
  no node was found that matches the affinity rules.

  The combination of these two types of affinity allows you to place
  your pods according to your specific requirements, for example:


  If possible, run my pod in an availability zone without other
  pods labelled app=nginx-ingress-controller.

● Note: resource quota feature can be configured to limit the total amount of resources
  that can be consumed. In conjunction namespaces, it can prevent one team from
  hogging all the resources.

● advertise extended resources for a node
change the reclaim policy of a persistentvolume
change the default storageclass
configure multiple schedulers
configure out of resource handling
configure quotas for api objects
control cpu management policies on the node
customizing dns service
debugging dns resolution
developing cloud controller manager
guaranteed scheduling for critical add-on pods
ip masquerade agent user guide
kubernetes cloud controller manager
limit storage consumption
operating etcd clusters for kubernetes
reconfigure a node's kubelet in a live cluster
reserve compute resources for system daemons
safely drain a node while respecting application slots
securing a cluster
set kubelet parameters via a config file
set up high-availability kubernetes masters
static pods
storage object in use protection
using coredns for service discovery
using a kms provider for data encryption
using sysctls in a kubernetes cluster

● Multi-Container Design Patterns:
https://www.linkedin.com/pulse/multi-container-init-container-pod-kubernetes-prachika-kanodia-
  Sidecar Container
     shared fs ···┐
     ^            v
     App Cont.   sidecar
     ---------   -------

  Adapter Container
  App Cont.      Adapter
  ---------      -------
  Writes         Simplifies  ←···· External Monitoring Service
  Complex        monitoring
  monit.output   output


  Ambassador Container
  App Cont. ···→ Ambassador   ┌··→ Local
  needs DB       Provides DB ─┼··→ Test
  connection     connection   └··→ Production


@[https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/]
@[https://banzaicloud.com/blog/k8s-sidecars/]

● [[{]]
securityContext
apiVersion: v1
  kind: Pod
  metadata:
    name: security-context-demo
  spec:
    securityContext:             ←    $ kubectl exec $runningPod -it -- /bin/bash
      runAsUser: 1000                 # id
      runAsGroup: 3000                uid=1000 gid=3000 groups=2000º
      fsGroup: 2000              ←┐   ( 0/0 if sec.ctx left blank)
                                  └ file-system owned/writable by fsGroup GID
                                    (when supported by volume)
    volumes:
    - name: sec-ctx-vol           ← Volumes will be relabeled with provided
      emptyDir: {}                  seLinuxOptions values
    containers:
    - name: sec-ctx-demo
      ...
      volumeMounts:
      - name: sec-ctx-vol
        mountPath: /data/demo
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add: ["NET_ADMIN", "SYS_TIME"] ← Provides a subset of 'root' capabilities: *1
       seLinuxOptions:
         level: "s0:c123,c456"

  @[https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h]

- SecurityContext holds security configuration that will be applied to a container.

- SecurityContext settings takes precedence over PodSecurityContext.

-  PodSecurityContext holds pod-level security attributes and
   common container settings
[[}]]

● StatefulSets:

● pod-lifecycle: restart-policy
  Pod Security Policies
  Resource Quotas

● 101, https://kubectl.docs.kubernetes.io/
  Kustomize lets you customize raw, template-free YAML
  files for multiple purposes, leaving the original YAML untouched and usable as is.

    /Pod patch.yaml
    /Dev patch.yaml    → kustomize → Kubernetes
    /Stagin patch.yaml

● scheduling: Pod Priority and Preemption

● REF:@[https://kubernetes.io/docs/concepts/architecture/cloud-controller/]

● What Is Kubernetes Security Posture Management?
  https://sysdig.com/learn-cloud-native/kubernetes-security/kspm/
  Kubernetes security posture management, or KSPM, is the use of
  security automation tools to discover and fix security and compliance
  issues within any component of Kubernetes.

  For example, KSPM could detect misconfigurations in a Kubernetes RBAC
  Role definition that grants a non-admin user permissions that he or
  she should not have, like the ability to create new pods. Or, a KSPM
  tool could alert you to an insecure Kubernetes network configuration
  that allows communication between pods in different namespaces, which
  would typically not be a setting you would want to enable.

● ValidKube Aims to Help Enforce Kubernetes YAML Best Practices
  https://www.infoq.com/news/2022/02/validkube-kubernetes-yaml/ 
    ValidKube is a browser-based tool, which means it is immediately
  accessible to anyone willing to try it out without needing to install
  the individual tools.

● Kubernetes 1.24 - What's new? - New features and deprecations
  https://sysdig.com/blog/kubernetes-1-24-whats-new/

● K8s: 1.23 Released with Improved Events, gRPC Probes, and Support for Dual-Stack
https://www.infoq.com/news/2021/12/kubernetes-1-23/

● Kubernetes Cluster API v1.0, Production Ready
https://www.infoq.com/news/2021/11/kubernetes-cluster-api/

● Virtual Kubernetes clusters: A new model for multitenancy
  https://opensource.com/article/22/3/virtual-kubernetes-clusters-new-model-multitenancy
 If you speak to people running Kubernetes in production, one of the
complaints you'll often hear is how difficult multitenancy is.
Organizations use two main models to share Kubernetes clusters with
multiple tenants, but both present issues

● : Thread by @devruso on Thread Reader App
https://threadreaderapp.com/thread/1452617538257494017.html

● A visual map of a Kubernetes deployment | Opensource.com
  https://opensource.com/article/22/3/visual-map-kubernetes-deployment

● kubernetes patterns OOSS book:
  https://github.com/k8spatterns/examples

● https://github.com/bitnami-labs/sealed-secrets

Problem: "I can manage all my K8s config in git, except Secrets."
Solution: Encrypt your Secret into a SealedSecret, which is safe to
store - even to a public repository. The SealedSecret can be
decrypted only by the controller running in the target cluster and
nobody else (not even the original author) is able to obtain the
original Secret from the SealedSecret.

Sealed Secrets is composed of two parts:

    A cluster-side controller / operator
    A client-side utility: kubeseal

The kubeseal utility uses asymmetric crypto to encrypt secrets that only the controller can decrypt.

These encrypted secrets are encoded in a SealedSecret resource, which you can see as a recipe for creating a secret. Here is how it looks:

apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: mysecret
  namespace: mynamespace
spec:
  encryptedData:
    foo: AgBy3i4OJSWK+PiTySYZZA9rO43cGDEq.....

Once unsealed this will produce a secret equivalent to this:

apiVersion: v1
kind: Secret
metadata:
  name: mysecret
  namespace: mynamespace
data:
  foo: bar  # <- base64 encoded "bar"



[[$undordered_notes }]]

● tags [[{]]
../tags.sh kubernetes.txt
01_PM.BACKLOG
01_PM.EXT_RESOURCE
01_PM.LOW_CODE
01_PM.TODO
01_PM.TODO.CLEAN
01_PM.TODO.NOW
02_DOC_HAS.COMPARATIVE
02_DOC_HAS.DIAGRAM
02_DOC_HAS.DIAGRAM.DECISSION_TREE
101
101.JOBS
101.K8S_INTERNALS
101.KUBECTL
APPLICATION
APPLICATION.DAEMONSET
APPLICATION.DEVELOPMENT
APPLICATION.HELM
APPLICATION.KNATIVE
APPLICATION.KNATIVE.GW
APPLICATION.OBSERVABILITY
APPLICATION.OPERATORS
APPLICATION.QUOTAS
APPLICATION.SIDECAR
CLOUD
CLOUD.AWS
CLOUD.BILLING
CLOUD.GCE
CLOUD.GCP
CLOUD.IAC
CLOUD.SDN
CLOUD.STORAGE
CLUSTER_ADMIN
CLUSTER_ADMIN.BACKUP
CLUSTER_ADMIN.BOOTSTRAP
CLUSTER_ADMIN.BOOTSTRAP.TLS
CLUSTER_ADMIN.EMBEDDED
CLUSTER_ADMIN.FEDERATION
CLUSTER_ADMIN.GPU
CLUSTER_ADMIN.HELM.SERVICE_CATALOG
CLUSTER_ADMIN.IAS
CLUSTER_ADMIN.IMAGE_REGISTRY
CLUSTER_ADMIN.KUBEADMIN
CLUSTER_ADMIN.KUBECONFIG
CLUSTER_ADMIN.MULTIZONE
CLUSTER_ADMIN.SSH
CLUSTER_ADMIN.TROUBLESHOOTING
DEBUG
DEV_LANG.C#
DEV_LANG.JAVA
DEV_LANG.PYTHON
_DOC_HAS
_DOC_HAS.COMPARATIVE
K8S_API
MONITORING
NETWORK
NETWORK.101.KUBELET
NETWORK.BALANCING
NETWORK.HYBRID
NETWORK.INGRESS
NETWORK.IPVS
NETWORK.SERVICE
NETWORK.SERVICES
QA
QA.BEST_PRACTICES
QA.DONTS
QA.GITOPS
QA.GOVERNANCE
QA.IDE
QA.SCALABILITY
QA.TESTING
QA.TESTING.CHAOS
QA.VENDOR_LOCKIN
SECURITY
SECURITY.101
SECURITY.AAA
SECURITY.AAA.2FA
SECURITY.AAA.RBAC
SECURITY.AUDITING
SECURITY.BACKUP
SECURITY.GOVERNANCE
SECURITY.GOVERNANCE.POLICIES
SECURITY.SECRET_MNG
SECURITY.STANDARDS
STORAGE
STORAGE.101
STORAGE.DISTRIBUTED
STORAGE.DISTRIBUTED.101
STORAGE.DISTRIBUTED.GLUSTER
STORAGE.KUBEDB
STORAGE.NVME
STORAGE.OBJECT.S3
TROUBLESHOOTING
TROUBLESHOOTING.101
[[}]]

● Clean: [[{]]
- monitoring → monitoring.*
- ⅋ ··> &
- Zip Pod notes
- Zip Labels notes
- Zip StatefulSet and Deployment notes
[[}]]
