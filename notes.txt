●  Apropos:
- Visit next Web site for a great experience:
  https://earizon.github.io/txt_world_domination/viewer.html?payload=../DevOps/notes.txt
- If you want to contribute to great gistory of this
  document you can take the next flight to:
@[https://www.github.com/earizon/DevOps]
  Your commits and pull-request will be immortalized
  in the Pantheon of the Unicode Gods.
────────────────────────────────────────────────────────────────────────────────

#############################################
●        Shell Scripting                    # [[{dev_language.shell_script]]
#############################################

● REFERENCE SCRIPT:  [[{dev_language.shell_script.101]] #[bash_summary]

  #!/bin/bash
  # NOTE: The Bash "syntax sugar" VAR1=$("some command") executes "some command"
  #  and assigns execution (STDOUT) output as effective value to VAR1

  # SETUP STDERR/STDOUT logging to file and console {{{
  readonly LOG_DIR="LOGS.gitingore"
  if [ ! -d ${LOG_DIR}.gitignore ] ; then
     mkdir ${LOG_DIR}.gitignore
  fi
  # $(whoami) will avoid collisions  among different users even if writing to the
  # same directory and serves as audit trail. # This happens frequently in DevOps when
  # executing in sudo/non-sudo contexts.
  readonly OUTPUT="${LOG_DIR}.gitignore/$(basename $0).$(whoami).$(date +%Y%m%d_%Hh%Mm%Ss).log"
  ln -sf ${OUTPUT} link_last_log.$(basename $0).gitignore  # (opinionated) Improve UX, create link to latest log
  exec 3>&1
  exec 4>&2
  echo "Cloning STDOUT/STDERR to ${PWD}/${OUTPUT}"
                                           # (Opnionated) Redirect to STDOUT and file REF:
  exec &> >(tee -a "$OUTPUT")              # Comment to disable (Ussually not needed in Kubernetes/AWS-EC2/...
                                           # since console output is direcly saved to files/S3 by some external mechanism.
                                           # https://unix.stackexchange.com/questions/145651↩
                                           #   /using-exec-and-tee-to-redirect-logs-to-stdout-and-a-log-file-in-the-same-time

  exec 2>&1                                # (Opinionated). Mix errors (STDERR) with STDOUT.
                                           # Recommended to see errors in the context of normal execution.
  echo "message logged to file & console"
  # }}}

  global_exit_status=0
  readonly WD=$(pwd)    # Best Practice: write down current work dir are use it
                        # to avoid problems when changing dir ("cd")
                        # randomnly throughout the script execution

  readonly FILE_RESOURCE_01="${WD}/temp_data.csv"    # <- readonly: inmutable value      [[qa]]


  readonly LOCK=$(mktemp)                 # <- Make temporal file. Assign ro constant LOCK.
                                          #    Use TMP_DIR=$(mktemp --directory) to create temporal dir.

  function funCleanUpOnExit() {
    rm ${LOCK}
  }
  trap funCleanUpOnExit EXIT              # ← Clean any temporal resource (socket, file, ...) on exit

  function XXX(){
    set +e                                # <- Disable "exit on any error" for the body of function.
                                          #  REF: @[https://en.wikipedia.org/wiki/Fail-fast]
    local -r name = ${HOME}               # local: scoped to function!!!                      [[qa]]
    echo "Cleaning resource and exiting"
    rm -fO ${FILE_RESOURCE_01}
    set -e                                # <- Re-enable fail-fast.
  }

  ERR_MSG=""
  function funThrow {
      if [[ $STOP_ON_ERR_MSG != false ]] ; then
        echo "ERR_MSG DETECTED: Aborting now due to "
        echo -e ${ERR_MSG}
        if [[ $1 != "" ]]; then
            global_exit_status=$1 ;
        elif [[ $global_exit_status == 0 ]]; then
            global_exit_status=1 ;
        fi
        exit $global_exit_status
      else
        echo "ERR_MSG DETECTED: "
        echo -e ${ERR_MSG}
        echo "WARN: CONTINUING WITH ERR_MSGS "

        global_exit_status=1 ;
      fi
      ERR_MSG=""
  }

  exec 100>${LOCK}                        # Simple linux-way to use locks.
  flock 100                               # First script execution will hold the lock
  if [[ $? != 0 ]] ; then                 # Next ones will have to wait. Use -w nSecs
      ERR_MSG="HOME ENV.VAR NOT DEFINED"  # to fail after timeout or -n to fail-fast
      funThrow 10 ;                       # lock will automatically be liberated on
  fi                                      # exit. (no need to unlock manually)
                                          # <a href="https://www.putorius.net/lock-files-bash-scripts.html">REF</a>

  # SIMPLE WAY TO PARSE/CONSUME ARGUMENTS WITH while-loop.
  while [  $#  -gt 0 ]; do  # $#  number of arguments
    case "$1" in
      -l|--list)
        echo "list arg"
        shift 1                       # <- consume arg      ,  $# = $#-1
        ;;
      -p|--port)
        export PORT="${2}:"
        shift                         # <- consume arg+value,  $# = $#-2
        ;;
      -h|--host)
        export HOST="${2^^}:"         # <-  ^^ suffix: Convert ${2} to upper case
        shift                         # <-  consume arg+value, $# = $#-2
        ;;
      *)
        echo "non-recognised option '$1'"
        shift                         # <- consume arg       , $# = $#-1
    esac
  done

  set -e                                   # At this point all variable must be defined. Exit on any error.

  function preChecks() {
    # Check that ENV.VARs and parsed arguments are in place
    if [[ ! ${HOME} ]] ; then ERR_MSG="HOME ENV.VAR NOT DEFINED" ; funThrow 41 ; fi
    if [[ ! ${PORT} ]] ; then ERR_MSG="PORT ENV.VAR NOT DEFINED" ; funThrow 42 ; fi
    if [[ ! ${HOST} ]] ; then ERR_MSG="HOST ENV.VAR NOT DEFINED" ; funThrow 43 ; fi
    set -u # From here on, ANY UNDEFINED VARIABLE IS CONSIDERED AN ERROR.
  }

  function funSTEP1 {
    echo "STEP 1: $HOME, PORT:$PORT, HOST: $HOST"
  }
  function funSTEP2 { # throw ERR_MSG
    ERR_MSG="My favourite ERROR@funSTEP2"
    funThrow 2
  }

  cd $WD ; preChecks
  cd $WD ; funSTEP1
  cd $WD ; funSTEP2

  echo "Exiting with status:$global_exit_status"
  exit $global_exit_status

● INIT VARS AND CONSTANTS:
  # complete Shell parameter expansion list available at:
  # - @[http://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html]
  var1=$1 # init var $1 with first param
  var2=$# # init var $1 with number of params
  var3=$! # init var with PID of last executed command.
  var4=${parameter:-word} # == $parameter if parameter set or 'word' (expansion)
  var5=${parameter:=word} # == $parameter if parameter set or 'word' (expansion), then parameter=word
  var6=${parameter:?word} # == $parameter if parameter set or 'word' (expansion) written to STDERR, then exit.
  var7=${parameter:+word} # == var1       if parameter set or 'word' (expansion).
  var8=${var1^^}          # init var2 as var1 UPPERCASE.
  var9=${parameter:offset}         #  <- Substring Expansion. It expands to up to length characters of the value
  varA=${parameter:offset:length}  |     of parameter starting at the character specified by offset.
                                   |     If parameter is '@', an indexed array subscripted by '@' or '*', or an
                                   |     associative array name, the results differ.
  readonly const1=${varA}

● CONCURRENT PROCESS BARRIER SYNCHRONIZATION:
  Wait for background jobs to complete example:
  (
    ( sleep 3 ; echo "job 1 ended" ) &
    ( sleep 1 ; echo "job 2 ended" ) &
    ( sleep 1 ; echo "job 3 ended" ) &
    ( sleep 9 ; echo "job 4 ended" ) &
    wait ${!}       # alt.1: Wait for all background jobs to complete
  # wait %1 %2 %3   # alt.2: Wait for jobs 1,2,3. Do not wait for job 4
    echo "All subjobs ended"
  ) &

● READ-EVALUATE-PARSE-LOOP (REPL) IN BASH:
  REPL stands for Read-eval-print loop: More info at:
  @[https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop]
      # Define the list of a menu item
      select   language  in C# Java PHP Python Bash Exit
      do
        #Print the selected value
        if [[   $language  == "Exit" ]] ; then
          exit 0
        fi
        echo "Selected language is $language"
      done

● 'test' CONDITIONAL BRANCHING: [[{]]
  (man test summary from GNU coreutils for more info)

  test   EXPRESSION  # ← EXPRESSION true/false sets the exit status.
  test [ EXPRESSION ]

  -n STRING                  # STRING length >0
                             # (or just STRING)
  -z STRING                  #  STRING length == 0
  STRING1 = STRING2          # String equality
  STRING1 != STRING2         # String in-equality


  INTEGER1 -eq INTEGER2      # ==
  INTEGER1 -ge INTEGER2      # <=
  INTEGER1 -gt INTEGER2
  INTEGER1 -le INTEGER2
  INTEGER1 -lt INTEGER2
  INTEGER1 -ne INTEGER2
  ^^^^^^^^
    NOTE:  INTEGER can be -l STRING (length of STRING)

   FILE TEST/COMPARISION
    WARN:  Except -h/-L, all FILE-related tests dereference symbolic links.
  -e FILE                    # FILE exists
  -f FILE                    # FILE exists and is a1regular file
  -h FILE                    # FILE exists and is   symbolic link  (same as -L)
  -L FILE                    #                                     (same as -h)
  -S FILE                    # FILE exists and is   socket
  -p FILE                    # FILE exists and is a named pipe
  -s FILE                    # FILE exists and has   size greater than zero


  -r FILE                    # FILE exists and read  permission is granted
  -w FILE                    # FILE exists and write permission is granted
  -x FILE                    # FILE exists and exec  permission is granted

  FILE1  -ef FILE2           # ← same device and inode numbers
  FILE1 -nt FILE2            # FILE1 is newer (modification date) than FILE2
  FILE1 -ot FILE2            # FILE1 is older (modification date) than FILE2
  -b FILE                    # FILE exists and is block special
  -c FILE                    # FILE exists and is character special
  -d FILE                    # FILE exists and is a directory
  -k FILE                    # FILE exists and has its sticky bit set


  -g FILE                    # FILE exists and is set-group-ID
  -G FILE                    # FILE exists and is owned by the effective group ID
  -O FILE                    # FILE exists and is owned by the effective user ID
  -t FD   file descriptor FD is opened on a terminal
  -u FILE FILE exists and its set-user-ID bit is set
  • BOOLEAN ADITION:
    WARN : inherently ambiguous.  Use
    EXPRESSION1 -a EXPRESSION2 # AND # 'test EXPR1 && test EXPR2' is prefered
    EXPRESSION1 -o EXPRESSION2 # OR  # 'test EXPR1 || test EXPR2' is prefered
  [[}]]

    WARN,WARN,WARN : your shell may have its own version of test and/or '[',
                     which usually supersedes the version described here.
                     Use /usr/bin/test to force non-shell ussage.

  Full documentation at: @[https://www.gnu.org/software/coreutils/]
[[}]]

● (KEY/VALUE) MAPS (Bash 4+) #[bash_dictionary]
  (also known as associative array or hashtable)

  Bash Maps can be used as "low code" key-value databases.
  Very useful for daily config/devops/testing task.
  Ex:
  #!/bin/bash            # ← /bin/sh will fail. Bash 4+ specific

  declare -A map01       # ← STEP 1) declare Map

  map01["key1"]="value1" # ← STEP 2) Init with some elements.
  map01["key2"]="value2" #   Visually map01 will be a table similar to:
  map01["key3"]="value3" #   key  │ value
                         #   ─────┼───────
                         #   key1 │ value1  ← key?, value? can be any string
                         #   key2 │ value2
                         #   key3 │ value3

    keyN="key2"          # ← STEP 3) Example Ussage
    ${map01[${key_var}]} #   ← fetch value for key "key2"
    ${!map01[@]}         #   ← fetch keys  . key2 key3 key1
    ${map01[@]}          #   ← fetch values. (value2 value3 value1)

    for keyN in "${!map01[@]}";      # ← walk over keys:
    do                               # (output)
      echo "$keyN : ${map01[$keyN]}" # key1 : value1
    done                             # key2 : value2
                                     # key3 : value3

● Curl (network client swiss Army nkife)  Summary: [[{networking.curl,troubleshooting,qa.testing]] #[curl_summary]
- Suport for DICT, FILE, FTP, FTPS, GOPHER, HTTP GET/POST, HTTPS, HTTP2, IMAP,
             IMAPS, LDAP, LDAPS, POP3, POP3S, RTMP, RTSP, SCP, SFTP, SMB, SMBS,
             SMTP, SMTPS, TELNET,  TFTP, unix socket protocols.
- Proxy support.
- kerberos support.
- HTTP  cookies, etags
- file transfer resume.
- Metalink
- SMTP / IMAP Multi-part
- HAProxy PROXY protocol
- ...

  HTTP Example
  $ curl http://site.{one,two,three}.com  \
         --silent                         \    ← Disable progress meter
         --anyauth                        \    ← make curl figure out auth. method
                                                 (--basic, --digest, --ntlm, and --negotiate)
                                                 not recommended if uploading from stdin since
                                                 data can be sent 2+ times
                                                 - Used together with -u, --user.
         --cacert file_used_to_verify_peer \   ← Alt: Use CURL_CA_BUNDLE
                                                 - See also --capath dir, --cert-status,  --cert-type PEM|DER|...
         --cert certificate[:password]     \   ← Use cert to indentify curl client
         --ciphers list of TLS_ciphers     \
         --compressed                      \   ← (HTTP) Request compressed response. Save uncompressed response.
         --config text_file_with_curl_args \
         --connect-timeout sec_number      \
         --create-dirs                     \   ← When using --output
         --data-binary data                \   ← HTTP POST alt 1: posts data with no extra processing whatsoever.
                                                 Or @data_file
         --data-urlencode data             \   ← HTTP POST alt 2
         --data           data             \   ← HTTP POST alt 3: post data in the same way that a browser  does for formats
                                                 (content-type application/x-www-form-urlencoded)
         --header ...
         --limit-rate speed
         --location                        \  ← follow redirects
         --include                         \  ← Include the HTTP response headers in the output.
                                                             See also -v, --verbose.
        --oauth2-bearer ...                \  ← (IMAP POP3 SMTP)
        --fail-early                       \  ← Fail as soon as possible
        --continue-at -                    \  ← Continue a partial download
        --output out_file                  \  ← Write output to file (Defaults to stdout)


        curl --list-only https://..../dir1/ ← List contents of remote dir
[[}]]

● Kapow!: Shell Script to HTTP API: [[{web_hook,01_PM.low_code,git.*,monitoring.prometheus,dev_stack.kubernetes,]]
@[https://github.com/BBVA/kapow]    [[notifications.jira,InfraAsCode.ansible,git.*,git.github,notifications.slack,]]
  (by BBVA-Labs Security team members)
  " If you can script it, you can HTTP it !!!!"

 Ex:
 Initial Script:
   $ cat /var/log/apache2/access.log | grep 'File does not exist'

 To expose it as HTTP:

   $ cat search-apache-errors
   #!/usr/bin/env sh
   kapow route add /apache-errors - <-'EOF'
       cat /var/log/apache2/access.log | grep 'File does not exist' | kapow set /response/body
   EOF

 Run HTTP Service like:

 $ kapow server search-apache-errors  ← Client can access it like
                                        curl http://apache-host:8080/apache-errors
                                        [Fri Feb 01 ...] [core:info] File does not exist: ../favicon.ico
                                        ...

    We can share information without having to grant SSH access to anybody.


  Recipe: Run script as a given user:
  # Note that `kapow` must be available under $PATH relative to /some/path
  kapow route add /chrooted\
      -e 'sudo --preserve-env=KAPOW_HANDLER_ID,KAPOW_DATA_URL \
          chroot --userspec=sandbox /some/path /bin/sh -c' \
          -c 'ls / | kapow set /response/body'

● WebHook  [[{01_PM.TODO]]
@[https://github.com/adnanh/webhook]
  - lightweight incoming webhook server to run shell commands
    You can also pass data from the HTTP request (such as headers,
    payload or query variables) to your commands. webhook also allows you
    to specify rules which have to be satisfied in order for the hook to
    be triggered.

  - For example, if you're using Github or Bitbucket, you can use webhook
    to set up a hook that runs a redeploy script for your project on your
    staging server, whenever you push changes to the master branch of
    your project.

  - Guides featuring webhook:
    - Webhook and JIRA by @perfecto25                                      [[jira]]
    - Trigger Ansible AWX job runs on SCM (e.g. git) commit by @jpmens     [[ansible]]
    - Deploy using GitHub webhooks by @awea [git][github]
    - Setting up Automatic Deployment and Builds Using Webhooks by Will
      Browning
    - Auto deploy your Node.js app on push to GitHub in 3 simple steps by  [[git.github]]
      Karolis Rusenas
    - Automate Static Site Deployments with Salt, Git, and Webhooks by     [[git]]
      Linode
    - Using Prometheus to Automatically Scale WebLogic Clusters on         [[prometheus,k8s,weblogic]]
      Kubernetes by Marina Kogan
    - Github Pages and Jekyll - A New Platform for LACNIC Labs by Carlos
      Martínez Cagnazzo
    - How to Deploy React Apps Using Webhooks and Integrating Slack on     [[slack]]
      Ubuntu by Arslan Ud Din Shafiq
    - Private webhooks by Thomas
    - Adventures in webhooks by Drake
    - GitHub pro tips by Spencer Lyon [github]
    - XiaoMi Vacuum + Amazon Button = Dash Cleaning by c0mmensal
    - Set up Automated Deployments From Github With Webhook by Maxim Orlov
      VIDEO: Gitlab CI/CD configuration using Docker and adnanh/webhook
      to deploy on VPS - Tutorial #1 by Yes! Let's Learn Software          [[}]]
[[}]]

● Bash-it: https://www.tecmint.com/bash-it-control-shell-scripts-aliases-in-linux/
- bundle of community Bash commands and scripts for Bash 3.2+,
  which comes with autocompletion, aliases, custom functions, ....
- It offers a useful framework for developing, maintaining and
  using shell scripts and custom commands for your daily work.
[[dev_language.shell_script}]]

####################################
● GIT                              # [[{git]]
####################################

• External Links:
  - @[https://git-scm.com/book/en/v2]
  - @[https://learnxinyminutes.com/docs/git/]
  - @[https://learngitbranching.js.org/?demo]
  - Related:
    See UStore: Distributed Storage with rich semantics!!!
    @[https://arxiv.org/pdf/1702.02799.pdf]
• Who-is-who:
  (Forcibly incomplete but still quite pertinent list of core people and companies)
  - Linus Torvald:
     L.T. initiated the project to fix problems with distributed
     development of the Linux Kernel.
  - Junio C. Hamano:  lead git maintainer (+8700 commits)
    @[https://git-blame.blogspot.com/]

● GIT: What's new: [[{]]
- 2.28:
@[https://github.blog/2020-07-27-highlights-from-git-2-28/]

 - Git 2.28 takes advantage of 2.27 commit-graph optimizations to  [[performance]]
   deliver a handful of sizeable performance improvements.


- 2.27:
 - commit-graph file format was extended to store changed-path Bloom
   filters. What does all of that mean? In a sense,
   this new information helps Git find points in history that touched a
   given path much more quickly (for example, git log -- <path>, or git [[performance]]
   blame).

- 2.25:
@[https://www.infoq.com/news/2020/01/git-2-25-sparse-checkout/]
  500+ changes since 2.24.

  Sparse checkouts are one of several approaches Git supports to improve   [scalability]
  performance when working with big(huge or monolithic) repositories.      [monolitic]
   They are useful to keep working directory clean by specifying which     [performance]
  directories to keep. This is useful, for example, with repositories
  containing thousands of directories.

  See also: http://schacon.github.io/git/git-read-tree.html#_sparse_checkout

- 2.23:
  https://github.blog/2019-08-16-highlights-from-git-2-23
[[}]]


● GIT "Full Journey" [[{git.101,1_doc_type.diagram,01_PM.WiP]] #[git_summary]
  #########################################################################
  # Non-normative Git server setup for N projects with M teams of L users #
  #########################################################################
 · CONTEXT:
   · ssh access has been enabled to server (e.g: $ $ sudo apt install openssh-server   )
   · Ideally ssh is protected. See for example:
   @[../DevOps/linux_administration_summary.html#knockd_summary]
   (Alternatives include using GitHub,GitLab,BitBucket, AWS/Scaleway/Azure/... )

 · We want to configure linux users and groups to match a "permissions layout" similar to:
    GIT_PROJECT1 ···→ Linux Group teamA ····→ R/W permssions to /var/lib/teamA/project01 *1
    GIT_PROJECT2 ···→ Linux Group teamA ····→ R/W permssions to /var/lib/teamA/project02
                      └───────┬────────┘
                       alice, bob,...
    GIT_PROJECT3 ···→ Linux Group teamB ····→ R/W permssions to /var/lib/teamB/project03
    GIT_PROJECT4 ···→ Linux Group teamB ····→ R/W permssions to /var/lib/teamB/project04
                      └───────┬────────┘
                       franc, carl, ...
    GIT_PROJECT5 ···→ ...
  *1: setup /var/lib/teamA/project01 like:
  $ $ sudo mkdir -p /var/lib/teamA/project01                 ← create directory
  $ $ cd /var/lib/teamA/project01
  $ $ sudo git init --bare                                   ← INIT NEW GIT BARE DIRECTORY !!!!
                                                               (GIT OBJECT DATABASE for
                                                                commits/trees/blogs, ...)
  $ $ DIR01=/var/lib/teamA/project01
  $ $ sudo find ${DIR01} -exec chown -R nobody:teamA {} \;   ← Fix owner:group_owner for dir. recursively
  $ $ sudo find ${DIR01} -type d -exec chmod g+rwx {} \;     ← enable read/wr./access perm. for dirs.
  $ $ sudo find ${DIR01} -type f -exec chmod g+rw  {} \;     ← enable read/write      perm. for files

    Finally add the desired linux-users to the 'teamA' linux-group at will. More info at:
    @[../DevOps/linux_administration_summary.html#linux_users_groups_summary] )

  ######################################################################
  # Non-normative ssh client access to Git remote repository using ssh #
  ######################################################################
• PRE-SETUP) Edit ~/.bashrc to tune the ssh options for git ading lines similar to:
  + GIT_SSH_COMMAND=""
  + GIT_SSH_COMMAND="${GIT_SSH_COMMAND} -oPort=1234 "                 ← connect to port 1234 (22 by default)
  + GIT_SSH_COMMAND="${GIT_SSH_COMMAND} -i ~/.ssh/privKeyServer.pem " ← private key to use when authenticating to server
  + GIT_SSH_COMMAND="${GIT_SSH_COMMAND} -u myUser1 "                  ← ssh user and git user are the same when using ssh.
  + GIT_SSH_COMMAND="${GIT_SSH_COMMAND} ..."                          ← any other suitable ssh options (-4, -C, ...)

    Optionally add your GIT URL like:
  $ + export GIT_URL="myRemoteSSHServer"
  $ + export GIT_URL="${GIT_URL}/var/lib/my_git_team   ← Must match path in server (BASE_GIT_DIR)
  $ + export GIT_URL="${GIT_URL}/ourFirstProject"      ← Must match name in server (PROJECT_NAME)

• PRE-SETUP) Modify PS1 prompt (Editing $HOME/.bashrc) to look like:
  PS1="\h[\$(git branch 2>/dev/null | grep ^\* | sed 's/\*/branch:/')]"               $( ... ) ==   exec. ... as script.
           └─────────────     show git branch    ───────────────────┘                            + Assign STDOUT to var.
  export PS1="${PS1}@\$(pwd |rev| awk -F / '{print \$1,\$2}' | rev | sed s_\ _/_) \$ "           (bash "syntax sugar")
               └────────────── show current and parent dir. only ────────┘

  host1 $                           ← PROMPT BEFORE
  host01[branch: master]@dir1/dir2  ← PROMPT AFTER

• Finally clone the repo like:
  $ $ git clone   myUser1 @${GIT_URL}                  ← execution will warn about cloning empty directory.
  $ $ cd ourFirstProject                               ← move to local clone.
  $ $ ...                                              ← Continue with standards git work flows.
  $ $ git add ...
  $ $ git commit ...

  #############################
  # COMMON (SIMPLE) GIT FLOWS #
  #############################
  ┌─ FLOWS 1: (Simplest flow) no one else pushed changes before our push.────────────────────────────────────
  │           NO CONFLICTS CAN EXISTS BETWEEN LOCAL AND REMOTE WORK
  │  local ─→ git status ─→ git add . ─→ git commit ··································→ git push \
  │  edit     └───┬────┘    └───┬────┘   └───┬────┘                                     origin featureX
  │               │         add file/s       │                                         └─────┬─────────┘
  │       display changes   to next commit   │                                       push to featureX branch
  │       pending to commit              commit new file history                     at remote repository.

  ┌─ FLOWS 2: someone else pushed changes before us,  ───────────────────────────────────────────────────────
  │           BUT THERE ARE NO CONFLICTS (EACH USER EDITED DIFFERENT FILES)
  │  local → git status → git add . → git commit ─→ git pull ·························→ git push \
  │  edit                                          └───┬───┘                            origin featureX
  │               • if 'git pull' is ommitted before 'git push', git will abort warning about remote changes
  │                 conflicting with our local changes. 'git pull' will download remote history and since
  │                 different files have been edited by each user, an automatic merge is done  (local changes
  │                  + any other user's remote changes). 'git pull' let us see other's people work locally.

  ┌─ FLOW 3: someone else pushed changes before our push,  ──────────────────────────────────────────────────
  │          BUT THERE ARE  CONFLICTS (EACH USER EDITED ONE OR MORE COMMON FILES)
  │  local → git status → git add . → git commit ─→ git pull ┌→ git add  → git commit → git push \
  │  edit                                              ↓     ↑  └──┬──┘                 origin featureX
  │                                             "fix conflicts" mark conflicts as
  │                                             └─────┬──────┘  resolved
  │                                  manually edit conflicting changes (Use git status to see conflicts)

  ┌─ FLOW 4: Amend local commit ─────────────────────────────────────────────────────────────────────────────
  │  local → git status → git add . → git commit  → git commit ─amend ...→ git commit → git push \
  │  edit                                                                               origin featureX

  ┌─ GIT FLOW: Meta-flow using WIDELY ACCEPTED BRANCHES RULES  ──────────────────────────────────────────────
  │  to manage common issues when MANAGING AND VERSIONING SOFTWARE RELEASES
  │  ┌───────────────────┬──────────────────────────────────────────────────────────────────────────────────┐
  │  │ Standarized       │ INTENDED USE                                                                     │
  │  │ branch names      │                                                                                  │
  │  ├───────────────────┼──────────────────────────────────────────────────────────────────────────────────┤
  │  │ feature/...       │ Develop new features here. Once developers /QA tests are "OK" with new code      │
  │  │                   │ merge back into develop. If asked to switch to another task just commit changes  │
  │  │                   │to this branch and continue later on.                                             │
  │  ├───────────────────┼──────────────────────────────────────────────────────────────────────────────────┤
  │  │develop            │ RELEASE STAGING AREA: Merge here feature/... completed features NOT YET been     │
  │  │                   │ released in other to make them available to other dev.groups.                    │
  │  │                   │ Branch used for QA test.                                                         │
  │  ├───────────────────┼──────────────────────────────────────────────────────────────────────────────────┤
  │  │release/v"X"       │ stable (release tagged branch). X == "major version"                             │
  │  ├───────────────────┼──────────────────────────────────────────────────────────────────────────────────┤
  │  │hotfix branches    │ BRANCHES FROM A TAGGED RELEASE. Fix quickly, merge to release and tag in release │
  │  │                   │ with new minor version. (Humor) Never used, our released software has no bugs    │
  │  └───────────────────┴──────────────────────────────────────────────────────────────────────────────────┘
  │
  │  • master branch ← "Ignore".
  │  ├─ • develop (QA test branch) ·················• merge ·················· • merge ┐  ...• merge ┐
  │  │  │                                           ↑ feat.1                   ↑       ·     ↑       ·
  │  │  ├→ • feature/appFeature1 • commit • commit ─┘  ························│·······↓ ... ┘       ·
  │  │  │     (git checkout -b)                                                │       ·             ·
  │  │  │                                                                      │       ·             ·
  │  │  ├→ • feature/appFeature2 • commit • commit • commit • commit • commit ─┘       ·  ┌··········┘
  │  │                                                                                 ·  ·(QA test "OK")
  │  │                      ┌··········←·(QA Test "OK" in develop, ready for release)··┘  ·
  │  │  ...                 v                                                             v
  │  ├─ • release/v1 ·······• merge⅋tag ┐         • merge⅋tag ┐         • merge⅋tag ······• merge⅋tag
  │  │                        v1.0.0    ·         ↑ v1.0.1    ·         ↑ v1.0.2            v1.1.0
  │  │                        *1        ↓         · *1        ·         · *1                *1
  │  │                                  └ hotfix1 •           └ hotfix2 •
  │  │                                  (git checkout -b)
  │  ├─ • release/v2 ····

  *1 Each merge into release/v"N" branch can trigger deployments to acceptance and/or production enviroments
     triggering new Acceptance tests. Notice also that deployments can follow different strategies
     (canary deployments to selected users first, ...).

<hr/>
<span xsmall>RECIPES</span>
                                            PLAYING WITH BRANCHES
  $ git checkout    newBranch             ← swith to local branch (use -b to create if not yet created)
  $ git branch -av                        ←  List (-a)ll existing branches
  $ git branch -d branchToDelete          ← -d: Delete branch
  $ git checkout --track "remote/branch"  ← Create new tracking branch (TODO)
  $ git checkout v1.4-lw                  ← Move back to (DETACHED) commit. ('git checkout HEAD' to reattach)
 $ git remote update origin --prune       ← Update local branch to mirror remote branches in 'origin'

                                            VIEW COMMIT/CHANGES HISTORY
  $ git log -n 10                         ← -n 10. See only 10 last commits.
  $ git log -p path_to_file               ← See log for file with line change details (-p: Patch applied)
  $ git log --all --decorate \            ← PRETTY BRANCH PRINT Alt.1
    --oneline --graph                       REF @[https://stackoverflow.com/questions/1057564/pretty-git-branch-graphs]
  $ git log --graph --abbrev-commit \     ← PRETTY BRANCH PRINT  Alt.2
     --decorate --date=relative --all


                                            UPDATE ALL REMOTE BRANCHES TO LOCAL REPO
  (REF: https://stackoverflow.com/questions/10312521/how-to-fetch-all-git-branches=
  for remote in `git branch -r`;             # ← add remote branches on server NOT yet tracked locally
     do git branch \                         #   (pull only applies to already tracked branches)
        --track ${remote#origin/} $remote;
  done

  $ git fetch --all                            # ← == git remote update.
                                               #    updates local copies of remote branches
                                               #    probably unneeded?. pull already does it.
                                               #    It is always SAFE BUT ...
                                               #    - It will NOT update local branches (tracking remote branches)
                                               #    - It will NOT create local branches (tracking remote branches)
  $ git pull --all                             # ← Finally update all tracked branches.

                                            TAGS:
  $ git tag                               ← List tags
  → v2.1.0-rc.2
  → ...
  $ git tag -a v1.4 -m "..." 9fceb..      ← Create annotated tag (recomended), stored as FULL OBJECTS.
                                            It contains tag author/mail/date, tagging message (-m).
                                            can be checksummed and optionally SIGNED/VERIFIED with GPG.
                                            if commit ommited (9fceb..) HEAD is used)
  $ git tag v1.4-lw                       ← Create lightweight tag ("alias" for commit-checksum)

                                            SHARING/PUSHING TAGS
                                            WARN : 'git push' does NOT push tags automatically
  $ git push origin v1.5                  ← Share 'v1.5' tag to remote 'origin' repo
  $ git push origin --tags                ← Share all tags
  $ git tag -d v1.4-lw                    ← Delete local tag (remote tags will persist)
  $ git push origin --delete v1.4-lw      ← Delete remote tag. Alt 1
  $ git push origin :refs/tags/v1.4-lw    ← Delete remote tag. Alt 2
                    └────────────────┴─── ← null value before the colon is being pushed to the
                                             remote tag name, effectively deleting it.
  $ git show-ref --tags                   ← show mapping tag ←→ commit
  → 75509731d28d... refs/tags/v2.1.0-rc.2
  → 8fc0a3af313d... refs/tags/v2.1.1
  → ...
                                            REVERTING CHANGES
 $ git reset --hard HEAD~1                ← revert to last-but-one (~1) local commit (Not yet pushed)
                                            (effectively removing last commit from local history)
 $ git checkout path/fileN                ← revert file not yet "git-add"ed or removed from FS to last commited ver.
 $ git checkout HEAD^ -- path/...         ← revert commited file to last-but-one commit version
 $ git revert ${COMMIT_ID}                ← add new commit cancelling changes in $COMMIT_ID. Previous
                                            commit is not removed from history. Both are kept on history.
 $ git clean -n                           ← Discard new "git-added" files. -n == -dry-run, -f to force
 $ git reset path/fileA                   ← Remove from index file "git-added" by mistake (with 'git add .')
                                            (probably must be added to .gitignore)

 $ git checkout N -- path1                ← Recover file at commit N (or tag N)
 $ git checkout branch1 -- path1          ← Recover file from branch1
                                            origin/branch1 to recover from upstream -vs local- branch.


                                            CLONING REMOTES
  $ git clone --depth=1  \                ← Quick/fast-clone (--depth=1) with history truncated to last N commits.
                                            Very useful in CI/CD tasks.
        --single-branch \                 ← Clone only history leading to tip-of-branch (vs cloning all branches)
                                            (implicit by previous --depth=... option)
        --branch '1.3' \                  ← branch to clone (defaults to HEAD)
        ${GIT_URL}                        To clone submodules shallowly, use also --shallow-submodules.
<hr/>
<span xsmall>Track code changes</span>
REF: @[https://git-scm.com/book/en/v2/Appendix-C:-Git-Commands-Debugging]

Methods to track who changed and/or when a change(bug) was introduced include:
• git bisect : find first commit introducing a change(bug, problem, ...) through automatic binary search .
@[https://git-scm.com/book/en/v2/Git-Tools-Debugging-with-Git#_binary_search]
  · git-blame helps to find recently introduced bugs.
  · git-bisect helps find bugs digged many commits down in history.
  · Ussage example:

                               MANUAL BISECT SEARCH
    $ git bisect start       ← start investigating issue.
    (run tests)
    $ git bisect bad         ← tell git that current commit is broken
    (run tests)
    $ git bisect good v1.0   ← tell git that current commit is OK
    Bisecting: 6 revisions↲  ← Git counts about 12 commits between
    left to test after this    "good" and "bad" and checks out middle one
    (run tests)
    ...
    b04... is 1st bad commit ← repeat git bisect good/bad until reaching
                               1st bad commit.
    $ git bisect reset       ← DON'T FORGET: reset after finding commit.

                               AUTOMATING BISECT SEARCH
  $ git bisect start HEAD v1.0
  $ git bisect run test.sh   ← test.sh must return 0 for "OK" results
                               non-zero otherwise.

• git blame : annotates lines-of-files with:
@[https://git-scm.com/book/en/v2/Git-Tools-Debugging-with-Git#_file_annotation]
  $ git blame -L 69,82 -C path2file  ← display last commit+committer for a given line
                                       -C: try to figure out where snippets of code
                                           came from (copied/file moved)
  b8b0618cf6fab (commit_author1 2009-05-26 ... 69) ifeq
  b8b0618cf6fab (commit_author1 2009-05-26 ... 70)
  ^1da177e4c3f4 (commit_author2 2005-04-16 ... 71) endif
  ^                                            ^^
  prefix '^' marks initial commit for line    line




• git grep : find any string/regex in any file in any commit, working directory (default) or index.
@[https://git-scm.com/book/en/v2/Git-Tools-Searching#_git_grep]
  •  Much faster than standard 'grep' UNIX command!!!
  $ git grep -n      regex01  ← display file:line   matching regex01 in working dir.
                                -n/--line-number: display line number
                                Use -p / --show-functions to display enclosing function
                                (Quick way to check where something is being called from)
  $ git grep --count regex01  ← Summarize file:match_count matching regex01 in working dir.

  $ git grep                  ← display file:line matching
      -n -e '#define' --and \ ← '#define' and
       \( -e ABC -e DEF \)      ( ABC or DEF )
      --break --heading \     ← split up output into more readable format
      "v1.8.0"                      ← search only in commit with tag "v1.8.0"

• Git Log Searching:
  $ git log -S ABC --oneline ← log only commits changing the number-of-occurrences of "ABC"
  e01503b commit msg ...       Replace -S by -G for REGEX (vs string).
  ef49a7a commit msg ...

• Line history Search:
  $ git log -L :funABC:file.c ← git will try to figure out what the bounds of
                                function funABC are, then look through history and
                                display every change made.
                                If programming lang. is not supported, regex can be
                                used like {: -L '/int funABC/',/^[}]/:file.c
                                range-of-lines or single-line-number can be used to
                                filter out non interesting results.
<hr/>
<span xsmall>Plumbings Summary</span>
• Summary extracted from:
  @[https://alexwlchan.net/a-plumbers-guide-to-git/1-the-git-object-store/]
  @[https://alexwlchan.net/a-plumbers-guide-to-git/2-blobs-and-trees/]
  @[https://alexwlchan.net/a-plumbers-guide-to-git/3-context-from-commits/]
  @[https://alexwlchan.net/a-plumbers-guide-to-git/4-refs-and-branches/]

$ $ git init   ← creates an initial layout containing (Alternatively $ $git clone ... ª from existing remote repo )
  ✓.git/objects/, ✓ .git/refs/heads/master, ✓ .git/HEAD (pointing to heads/master initially)
  ✓.git/description (used by UIs), ✓.git/info/exclude (local/non-commited .gitignore),
  ✓.git/config, ✓.git/hooks/

   ~/.git/index  ←············  binary file with staging area data (files 'git-added' but not yet commited)
                                Use (porcelain) $ $ git ls-files   to see indexes files (git blobs)
        ┌─.git/objects/  (  GIT OBJECT STORE  ) ─────────┐      $ $ echo "..." > file1.txt
        │                                                │      $ $ git hash─object ─w file1.txt
┌········→ • /af/3??? •···→ •/a1/12???                   │          └───────────┬────────────────┘
·       │  │(2nd commit)                      ┌············ save to Object Store┘content─addressable File System.
·       │  v                                  v          │  WARN : Original file name lost. We need to add a mapping
· ┌··┬···→ • /5g/8... •···→ • /32/1... •┬··→• /a3/7... • │   (file_name,file_attr) ←→ hash to index like:
· ·  ·  │   (1st commit)    ┌··········┘├··→• /23/4... • │   $ $ git update-index --add file1.txt (git Plumbing)
· ·  ·  │                   ·           └···┐            │┌· Finally an snapshot of the index is created like a tree:
· ·  ·  │                   ·               ·            │·$ $ git write-tree                 .git/index snapshot to tree
· ·  ·  │                   ├─····························┘  ( /af/9???... tree object will be added to Obj.Store)
· ·  ·  │                   ·               ·            │ $ $ git cat-file -p ${tree_hash}
· ·  ·  │                   ·               ·            │     100644 blob b133......  file1.txt  ← Pointer+file_name to blob
· ·  ·  │                   ·               ·            │     040000 tree 8972......  subdir...  ← Pointer+"dirname" to (sub)tree
· ·  ·  │                   ·               ·            │     ...
· ·  ·  │                   ·               ·            │     ^^^^^^ ^^^^ ^^^^^^^^^^  ^^^^^^^^^^^^
· ·  ·  │                   ·               ·            │     file   type content     Name of file
· ·  ·  │                   ·               ·            │     permis.     address.ID
· ·  ·  │                   ·               ·            │ ☞KEY-POINT:☜
· ·  ·  │                   v               v            │  Starting at a tree, we can rebuild everything it points to
· ·  ·  │                   • /af/9... •┬··→• /12/d... • │  All that rest is mapping trees to some "context" in history.
· ·  ·  │                               └··→• /34/2... • │
· ·  ·  │  ^^^^^^^^^^^^     ^^^^^^^^^^^^    ^^^^^^^^^^^^ │  Plumbing "workflow" summary:
· ·  ·  │    commits          Trees           Blobs      │← Add 1+ Blobs → Update Index → Snapshot to Tree
· ·  ·  └────────────────────────────────────────────────┘  → create commit pointing to tree of "historical importance"
· ·  └·······················─┬──────────────────────┐        → create friendly refs to commits
· · $ $ echo "initial commit" | git commit-tree 321.....   ← create commit pointing to tree of "historical importance"
· ·     af3...                                               Use also flag '-p $previous_commit' to BUILD A LINEAR ORDERED
· ·                                                          HISTORY OF COMMITS !!!
· ·
· · $ $ git cat-file -p af3....                            -p: show parent of given commit
· ·   tree 3212f923d36175b185cfa9dcc34ea068dc2a363c   ← Pointer to tree of interest
· ·   author    Alex Chan ... 1520806168 +0000        ← Context with author/commiter/
· ·   committer Alex Chan ... 1520806168 +0000          creation time/ ...
· ·   ...
· · "DEBUGGING" TIP: Use 'git cat-file -p 12d...' for pretty print ('-t' to display/debug object type)
· ·
· └ ~/.git/refs/heads/dev     ←  ~/.git/HEAD (pointer to active ref)
└·· ~/.git/refs/heads/master af3...  ← Create friendly "master" alias to a commit like:
                      ^^^^^^         $ $ git update-ref refs/heads/master   ← With plumbing each new commit requires a new
    refs in heads/ folder are        $ $ cat  .git/refs/heads/master          git update-ref.
    COMMONLY CALLED BRANCHS            af3... (pointer to commit)
                                       Now $ $ git cat-file -p master   "==" $ $ git cat-file -p af3...

                                     $ $ git rev-parse master               ← check value of ref
                                       af3...

                                     $ $ git update-ref refs/heads/dev     ← Create second branch (ref in heads/ folder)
                                     $ $ git branch
                                       dev
                                     * master ←·························  current branch is determined by (contents of)
                                     $ $ cat .git/HEAD                     ~/.git/HEAD. Using plumbing we can change it like
                                     ref: refs/heads/master                $ git symbolic-ref HEAD refs/heads/dev
<hr/>
<span title>merge/rebase/cherry-pick</span>
• REF:
@[https://stackoverflow.com/questions/9339429/what-does-cherry-picking-a-commit-with-git-mean]
@[https://git-scm.com/docs/git-cherry-pick]

 ┌ INITIAL STATE ──────────────────────────────────────────────────────────
 │ • → • → • → • →H1          ← refs/heads/branch01
 │         │
 │         └─→ •x1→ •x2→ •H2  ← refs/heads/branch02
 └────────────────────────────────────────────────────────────────────────
 ┌ MERGE @[https://git-scm.com/docs/git-merge]────────────────────────────
 │ add changes for other branch as single "M"erge commit
 │ $ $ git checkout branch01 ⅋⅋ git merge branch02
 │ • → • → • → • → •H1 → •M  : M = changes of ( x1+x2+H2 )
 │         │             ↑
 │         └─→ •x1→ •x2→ •H2
 └────────────────────────────────────────────────────────────────────────
 ┌ REBASE @[https://git-scm.com/docs/git-rebase]──────────────────────────
 │ "replay" full list of commits to head of branch
 │ $ $ git checkout branch01 ⅋⅋ git rebase branch02
 │ • → • → • → • →H1 •x1→ •x2→ •H2
 │         │
 │         └─→ •x1→ •x2→ •H2
 └────────────────────────────────────────────────────────────────────────
 ┌ Squash N last commit into single one  (rebase interactively) ──────────
 │
 │ • → • → • → • →H1      ← refs/heads/branch01
 │         │
 │         └─→ • H2' (x1 + x2)
 │
 │ $ $ git rebase --interactive HEAD~2

 │   pick 64d03e last-but-2 commit comment ← Different interesing actions are available
 │   pick 87718a last-but-1 commit comment   Replace "pick" by "s"(quash) to mark commit
 │   pick 83118f HEAD       commit comment   to be squashed into single commit.
 │                                            ·
 │   s 64d03e last-but-2 commit comment     ←·┘
 │   s 87718a last-but-1 commit comment     (Save and close editor. Git will combine all
 │   s 83118f HEAD       commit comment     commits into first in list)
 │                                          The editor will "pop up" again asking to enter
 │                                          a commit message.
 └────────────────────────────────────────────────────────────────────────

 ┌ CHERRY-PICK @[https://git-scm.com/docs/git-cherry-pick]────────────────
 │ "Pick" unique-commits from branch and apply to another branch
 │ $ $ git checkout branch02 ⅋⅋ git cherry-pick  -x branch02
 │ ··· • → • →H1 → ...                          └┬┘
 │     │                  • Useful if "source" branch is public, generating
 │     └─→ • → • →H2 →      standardized commit message allowing co-workers
 │                          to still keep track of commit origin.
 │                        • Notes attached to the commit do NOT follow the
 │                          cherry-pick. Use $ $ git notes copy "from" "to"
 └────────────────────────────────────────────────────────────────────────
[[}]]

● GPG signed commits @ma [[{security.secret_management,security.signed_content,01_PM.TODO]]
@[https://git-scm.com/book/en/v2/Git-Tools-Signing-Your-Work]

  GPG PRESETUP

  See @[General/cryptography_map.html?id=pgp_summary] for a summary on
  how to generate and manage pgp keys.

  GIT PRESETUP
  $ git config --global \
        user.signingkey 0A46826A  ← STEP 1: Set default key for tags+commits sign.

  $ git tag -s v1.  -m 'my signed 1.5 tag'  ←   Signing tags
            └──┬──┘                           (follow instructions to sign)
          replaces -a/--anotate

  $ git show v1.5
  tag v1.5
  Tagger: ...
  Date:   ...

  my signed 1.5 tag
  -----BEGIN PGP SIGNATURE-----
  Version: GnuPG v1

  iQEcBAABAgAGBQJTZbQlAAoJEF0+sviABDDrZbQH/09PfE51KPVPlanr6q1v4/Ut
  ...
  =EFTF
  -----END PGP SIGNATURE-----

  commit ...

  $ git tag -  v1.4.2.1  ←   Verify tag
            └┘             Note: signer’s pub.key must be in local keyring
  object 883653babd8ee7ea23e6a5c392bb739348b1eb61
  type commit
  ...
  gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID
  F3119B9A
  gpg: Good signature from "Junio C Hamano <junkio@cox.net>"
  gpg:                 aka "[jpeg image of size 1513]"
  Primary key fingerprint: 3565 2A26 2040 E066 C9A7  4A7D C0C6 D9A4
  F311 9B9A
  └──────────────────────────┬────────────────────────────────────┘
   Or error similar to next one will be displayed:
     gpg: Can't check signature: public key not found
   error: could not verify the tag 'v1.4.2.1'

  $ git commit -  -  -m 'Signed commit'  ←   Signing Commits (git 1.7.9+)

  $ git log --show-signature -1          ←   Verify Signatures
  commit 5c3386cf54bba0a33a32da706aa52bc0155503c2
  gpg: Signature made Wed Jun  4 19:49:17 2014 PDT using RSA key ID
  0A46826A
  gpg: Good signature from "1stName 2ndName (Git signing key)
  <user01@gmail.com>"
  Author: ...
  ...
$ $ git log --pretty="format:%h %G? %aN  %s"
                                ^^^
                                check and list found signatures
         Ex. Output:
    5c3386cG   1stName 2ndName  Signed commit
    ca82a6dR   1stName 2ndName  Change the version number
    085bb3bR   1stName 2ndName  Remove unnecessary test code
    a11bef0R   1stName 2ndName  Initial commit


You can also use the -S option with the git merge command to sign the
resulting merge commit itself. The following example both verifies
that every commit in the branch to be merged is signed and
furthermore signs the resulting merge commit.


$ git merge \             ←   # Verify signature at merge time
  --verify-signatures \
  -S \                    ← Sign merge itself.
  signed-branch-to-merge  ← Commit must have been signed.

$ git pull  \             ←   # Verify signature at pull time
  --verify-signatures
[[}]]

●  Client Hooks [[{01_PM.TODO.now]]
@[https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks]

  Client-Side Hooks
  - not copied when you clone a repository
    - to enforce a policy do on the server side
  - committing-workflow hooks:
    - pre-commit hook:
      - First script to be executed.
      - used to inspect the snapshot that's about to be committed.
        - Check you’ve NOT forgotten something
        - make sure tests run
        - Exiting non-zero from this hook aborts the commit
      (can be bypassed with git commit --no-verify flag)
    - prepare-commit-msg hook:
      - Params:
        - commit_message_path (template for final commit message)
        - type of commit
        - commit SHA-1 (if this is an amended commit)
      - run before the commit message editor is fired up
        but after the default message is created.
      - It lets you edit the default message before the
        commit author sees it.
      - Used for non-normal-commits with auto-generated messages
        - templated commit messages
        - merge commits
        - squashed commits
        - amended commits
    - commit-msg hook:
        - commit_message_path (written by the developer)
    - post-commit hook:
      - (you can easily get the last commit by running git log -1 HEAD)
      - Generally, this script is used for notification or something similar.

  - email-workflow  hooks:
    - invoked by  git am
                  ^^^^^^
                  Apply a series of patches from a mailbox
                  prepared by git format-patch

    - applypatch-msg :
      - Params:
        - temp_file_path containing the proposed commit message.
    - pre-applypatch :
      - confusingly, it is run after the patch is
        applied but before a commit is made.
      - can be used it to inspect the snapshot before making the commit,
        run tests,  inspect the working tree with this script.
    - post-applypatch :
      - runs after the commit is made.
      - Useful to notify a group or the author of the patch
        you pulled in that you’ve done so.

  - Others:
    - pre-rebase hook:
      - runs before you rebase anything
      - Can be used to disallow rebasing any commits
        that have already been pushed.
    - post-rewrite hook:
      - Params:
        - command_that_triggered_the_rewrite:
          - It receives a list of rewrites on stdin.
      - run by commands that replace commits
        such as 'git commit --amend' and 'git rebase'
        (though not by git filter-branch).
      - This hook has many of the same uses as the
        post-checkout and post-merge hooks.
    - post-checkout hook:
      - Runs after successful checkout
      - you can use it to set up your working directory
        properly for your project environment.
        This may mean moving in large binary files that
        you don't want source controlled, auto-generating
        documentation, or something along those lines.
    - post-merge hook:
      - runs after a successful merge command.
      - You can use it to restore data in the working tree
        that Git can't track, such as permissions data.
        It can likewise validate the presence of files
        external to Git control that you may want copied
        in when the working tree changes.
    - pre-push hook:
      - runs during git push, after the remote refs
        have been updated but before any objects have
        been transferred.
      - It receives the name and location of the remote
        as parameters, and a list of to-be-updated refs
        through stdin.
      - You can use it to validate a set of ref updates before
        a push occurs (a non-zero exit code will abort the push).
[[}]]

● Server-Side Hooks [[{01_PM.TODO.now]]
(system administrator only)
- Useful to enforce nearly any kind of policy in repository.

- exit non-zero to rollback/reject push
  and print error message back to the client.

 pre-receive hook :
 - first script to run
 - INPUT: STDIN reference list
 - Rollback all references on non-zero exit

 - Ex.
   - Ensure none of the updated references are non-fast-forwards.
   - do access control for all the refs and files being modifying
     by the push.

 update hook :
 - similar to pre-receive hook.but  run once for each branch the
   push is trying to update  (ussually just one branch is updated)

 - INPUT arguments:
   - reference name (for branch),
   - SHA-1
   - SHA-1
     refname= ARGV[0] ← ref.name for current branch
     oldrev = ARGV[1] ←  (SHA-1)  original (current-in-server)      ref. *1
     newrev = ARGV[2] ←  (SHA-1)  new      (intention to)      push ref. *1
     user   = $USER   ← "Injected" by git when using ssh.

     *1: We can run over all commit from $oldrev to $newrev like
         git rev-list \                        ← display a (sha1)commit per line to STDOUT
             oldrev..$newrev \                 ← from $oldrev to $newrev
             while read SHA_COMMIT ; do
            git cat-file commit $SHA_COMMIT \  ← *1
            | sed '1,/^$/d'                    ← Delete from line 1 to first match of
                                                 empty-line (^$).



  *1 output format is similar to:
  | tree      ...
  | parent    ...
  | committer ...
  |
  | My commit Message
  | tree      ...

  - user-name (if accesing through ssh) based on ssh public-key.
 - Exit  0: Update
   Exit !0: Rollback reference, continue with next one.

 post-receive
 - can be used to update other services or notify users.
 - INPUT: STDIN reference list
 - Useful for:
   - emailing a list.
   - trigger CI/CD.
   - update ticket system
     (commit messages can be parsed for "open/closed/..."
 -   WARN : can't stop the push process.
   client  will block until completion.
[[}]]

● GIT Commit Standard Emojis: [[{]]
@[https://gist.github.com/parmentf/035de27d6ed1dce0b36a]
 Commit type               Emoji                    Graph
 Initial commit           :tada:                      🎉
 Version tag              :bookmark:                  🔖
 New feature              :sparkles:                  ✨
 Bugfix                   :bug:                       🐛
 Metadata                 :card_index:                📇
 Documentation            :books:                     📚
 Documenting src          :bulb:                      💡
 Performance              :racehorse:                 🐎
 Cosmetic                 :lipstick:                  💄
 Tests                    :rotating_light:            🚨
 Adding a test            :white_check_mark:          ✅
 Make a test pass        :heavy_check_mark:           ✔️
 General update           :zap:                       ⚡️
 Improve format           :art:                       🎨
 /structure
 Refactor code            :hammer:                    🔨
 Removing stuff           :fire:                      🔥
 CI                       :green_heart:               💚
 Security                 :lock:                      🔒
 Upgrading deps.         :arrow_up:                   ⬆️
 Downgrad. deps.         :arrow_down:                 ⬇️
 Lint                     :shirt:                     👕
 Translation              :alien:                     👽
 Text                     :pencil:                    📝
 Critical hotfix          :ambulance:                 🚑
 Deploying stuff          :rocket:                    🚀
 Work in progress         :construction:              🚧
 Adding CI build system   :construction_worker:       👷
 Analytics|tracking code  :chart_with_upwards_trend:  📈
 Removing a dependency    :heavy_minus_sign:          ➖
 Adding a dependency      :heavy_plus_sign:           ➕
 Docker                   :whale:                     🐳
 Configuration files      :wrench:                    🔧
 Package.json in JS       :package:                   📦
 Merging branches         :twisted_rightwards_arrows: 🔀
 Bad code / need improv.  :hankey:                    💩
 Reverting changes        :rewind:                    ⏪
 Breaking changes         :boom:                      💥
 Code review changes      :ok_hand:                   👌
 Accessibility            :wheelchair:                ♿️
 Move/rename repository  :truck:                      🚚
[[}]]

● GitHub: Custom Bug/Feature-request templates [[{git.github]]
  WARN : Non standard (Vendor lock-in) Microsoft extension.
 $ cat .github/ISSUE_TEMPLATE/bug_report.md
 | ---
 | name: Bug report
 | about: Create a report to help us improve
 | title: ''
 | labels: ''
 | assignees: ''
 |
 | ---
 |
 | **Describe the bug**
 | A clear and concise description of what the bug is.
 |
 | **To Reproduce**
 | Steps to reproduce the behavior:
 | 1. Go to '...'
 | 2. Click on '....'
 | 3. Scroll down to '....'
 | 4. See error
 |
 | **Expected behavior**
 | A clear and concise description of what you expected to happen.
 |
 | ...

 $ cat .github/ISSUE_TEMPLATE/feature_request.md
  | ---
  | name: Feature request
  | about: Suggest an idea for this project
  | title: ''
  | labels: ''
  | assignees: ''
  |
  | ---
  |
  | **Is your feature request related to a problem? Please describe.**
  | A clear and concise description of what the problem is....
  |
  | **Describe the solution you'd like**
  | A clear and concise description of what you want to happen.
  |
  | **Describe alternatives you've considered**
  | A clear and concise description of any alternative solutions or features you've considered.
  |
  | **Additional context**
  | Add any other context or screenshots about the feature request here.

 $ cat ./.github/pull_request_template.md
 ...

 $ ./.github/workflows/*
  WARN : Non standard (Vendor lock-in) Microsoft extension.
 @[https://docs.github.com/en/free-pro-team@latest/actions/learn-github-actions]
[[}]]

● Unbreakable Branches: [[{git.bitbucket,qa,jenkins.troubleshooting]]
@[https://github.com/AmadeusITGroup/unbreakable-branches-jenkins]

- plugins for Bitbucket and Jenkins trying to fix next problem:

  Normal Pull Request workflow:
  Open pull-request (PR) to merge changes in target-branch
    → (build automatically triggered)
      → build OK
        repo.owner merges PR
         → second build triggered on target-branch
           →  second build randomnly fails
              leading to broken targeted branch
              └───────────────┬───────────────┘
               Reasons include:
               - Race condition: Parallel PR was merged in-between
               - Environment issue (must never happens)
               - lenient dependency declaration got another version
                 leading to a build break

  - If the Jenkins job is eligible to unbreakable build
    (by having environment variables such as UB_BRANCH_REF)
    at the end of the build a notification to Bitbucket is
    sent according to the build status.
    (or  manually through two verbs: ubValidate|ubFail)

- Difference stashnotifier-plugin:
  - stashplugin reports status-on-a-commit
  - unbreakable build a different API is dedicated on Bitbucket.

- On the Bitbucket side:
  - GIT HEAD@target-branch moved to  top-of-code to be validated in PR
    (target-branch can then always have a successful build status).

- Security restrictions added to Bitbucket:
  (once you activate the unbreakable build on a branch for your repository)
  -  merge button replaced by merge-request-button to queue the build.
  -  The merge will happen automatically at the end of the build if the build succeeds
  -  direct push on the branch is forbidden
  -  Merge requests on different PRs will process the builds sequentially

- Prerequisites to run the code locally:
  - Maven (tested agains 3.5)
  - Git should be installed

- PRE-SETUP:
  - Install UnbreakableBranch plugin at Bitbucket
  - bitbucketBranch source plugin Jenkins plugin should be
    a patched so that mandatory environment variables are
    injected.   Note that this plugin hasn't been released yet


@[https://github.com/newren/git-filter-repo/]
- Create new repository from old ones, keeping just the
  history of a given subset of directories.

(Replace: (buggy)filter-branch @[https://git-scm.com/docs/git-filter-branch])
- Python script for rewriting history:
  - cli for simple use cases.
  - library for writing complex tools.

- Presetup:
  - git 2.22.0+  (2.24.0+ for some features)
  - python 3.5+

  $ git filter-repo \
       --path src/ \                         ← commits not touching src/ removed
       --to-subdirectory-filter my-module \  ← rename  src/** → my-module/src/**
       --tag-rename '':'my-module-'            add 'my-module-' prefix to any tags
                                               (avoid any conflicts later merging
                                                into something else)

  Design rationale behind filter-repo :
  - None existing tools with similr features.
  - [Starting report] Provide analysis before pruning/renaming.
  - [Keep vs. remove] Do not just allow to remove selected paths
                      but to keep certain ones.
    (removing all paths except a subset can be painful.
     We need to specify all paths that ever existed in
     any version of the repository)
  - [Renaming] It should be easy to rename paths:
  - [More intelligent safety].
  - [Auto shrink] Automatically remove old cruft and repack the
    repository for the user after filtering (unless overridden);
  - [Clean separation] Avoid confusing users (and prevent accidental
    re-pushing of old stuff) due to mixing old repo and rewritten repo
    together.
  - [Versatility] Provide the user the ability to extend the tool
    ... rich data structures (vs hashes, dicts, lists, and arrays
        difficult to manage in shell)
    ... reasonable string manipulation capabilities
  - [Old commit references] Provide a way for users to use old commit
    IDs with the new repository.
  - [Commit message consistency] Rewrite commit messages pointing to other
    commits by ID.
  - [Become-empty pruning] empty commits should be pruned.
  - [Speed]

- Work on filter-repo and predecessor has driven
  improvements to fast-export|import (and occasionally other
  commands) in core git, based on things filter-repo needs to do its
  work:

  Manual Summary :
@[https://htmlpreview.github.io/?https://github.com/newren/git-filter-repo/blob/docs/html/git-filter-repo.html]
- Overwrite entire repository history using user-specified filters.
  (WARN: deletes original history)
  - Use cases:
    - stripping large files (or large directories or large extensions)
    - stripping unwanted files by path (sensitive secrests) [secret]
    - Keep just an interesting subset of paths, remove anything else.
    - restructuring file layout. Ex:
      - move all files subdirectory
      - making subdirectory as new toplevel.
      - Merging two directories with independent filenames.
      - ...
    - renaming tags
    - making mailmap rewriting of user names or emails permanent
    - making grafts or replacement refs permanent
    - rewriting commit messages
[[}]]

● TODO: [[{01_PM.TODO]]
• GitOps uses Git (DVCS) as single source of truth  for declarative infrastructure and [[{ci/cd.gitops]]
  applications. Every developer within a team can issue pull requests against a
  Git repository, and when merged, a "diff and sync" tool detects a difference
  between the intended and actual state of the system. Tooling can then be
  triggered to update and synchronise the infrastructure to the intended state.
 @[https://www.weave.works/blog/gitops-operations-by-pull-request]             [[}]]

• Scaling Git with LFS/VFS [[{git.scalability,scalability.storage]]
• VFS is designed to scale for projects with many files/branches and long commit history
 with Git Object data store up to hundreds of Gigabytes .
@[https://github.com/Microsoft/VFSForGit]
@[https://vfsforgit.org/]
• Git Large File Storage (LFS)targets the problem of large individual files (1GB video files, ML training data, ...).
  replacing them with text pointers inside Git, while storing the file contents on a remote server.
@[https://git-lfs.github.com/]
[[}]]

• 4 secrets encryption tools [[{security.secret_management}]]
@[https://www.linuxtoday.com/security/4-secrets-management-tools-for-git-encryption-190219145031.html]
@[https://www.atareao.es/como/cifrado-de-repositorios-git/]

• Garbage Collector:[[{performance]]
-  Git occasionally does garbage collection as part of its normal operation,
by invoking git gc --auto. The pre-auto-gc hook is invoked just before the
garbage collection takes place, and can be used to notify you that this is
happening, or to abort the collection if now isn’t a good time.
[[}]]


• sparse-checkout (Git v2.25+) allows to checkout just a subset [[{scalability]]
  of a given monorepo, speeding up commands like git pull and
  git status.
@[https://github.blog/2020-01-17-bring-your-monorepo-down-to-size-with-sparse-checkout/] [[}]]

• Advanced Git:
  - revert/rerere:
  - Submodules:
  - Subtrees:
    - TODO: how subtrees differ from submodules
    - how to use the subtree to create a new project from split content
  - Interactive rebase:
    - how to rebase functionality to alter commits in various ways.
    - how to squash multiple commits down into one.
  - Supporting files:
    - Git attributes file and how it can be used to identify binary files,
      specify line endings for file types, implement custom filters, and
      have Git ignore specific file paths during merging.
  - Cregit token level blame:
  @[https://www.linux.com/blog/2018/11/cregit-token-level-blame-information-linux-kernel]
  cregit: Token-Level Blame Information for the Linux Kernel
  Blame tracks lines not tokens, cgregit blames on tokens (inside a line)

• Gitea painless self-hosted Git service(Gogs) [[{01_PM.TODO,01_PM.low_code]]
@[https://gitea.io/]
- Fork of gogs, since it was unmaintained.

• Gerrit (by Google)</span>
@[https://www.gerritcodereview.com/index.html]
Gerrit is a Git Server that provides:
- Code Review:
  - One dev. writes code, another one is asked to review it.
    (Goal is cooperation, not fauilt-finding)
  @[https://docs.google.com/presentation/d/1C73UgQdzZDw0gzpaEqIC6SPujZJhqamyqO1XOHjH-uk/]
  - UI for seing changes.
  - Voting pannel.


- Access Control on the Git Repositories.
- Extensibility through Java plugins.
@[https://www.gerritcodereview.com/plugins.html]


Gerrit does NOT provide:
- Code Browsing
- Code SEarch
- Project Wiki
- Issue Tracking
- Continuous Build
- Code Analyzers
- Style Checkers
[[}]]

• Git Secrets: [[{qa,security.secret_management}]]
https://github.com/awslabs/git-secrets#synopsis
- Prevents you from committing passwords and other sensitive
  information to a git repository.

• Forgit: Interactive Fuzzy Finder:[[{dev_stack.forgit,qa.UX,01_PM.TODO]]
@[https://www.linuxuprising.com/2019/11/forgit-interactive-git-commands-with.html]
- It takes advantage of the popular "fzf" fuzzy finder to provide
  interactive git commands, with previews. [[}]]

• Isomorphic Git: 100% JS client [[{security.gpg]]
@[https://isomorphic-git.org/] !!!

- Features:
  - clone repos
  - init new repos
  - list branches and tags
  - list commit history
  - checkout branches
  - push branches to remotes
  - create new commits
  - git config
  - read+write raw git objects
  - PGP (GPG) signing
  - file status
  - merge branches
[[}]]

• Git Monorepos: [[{qa.UX]]
  (Big) Monorepos in Git:
  https://www.infoq.com/presentations/monorepos/
  https://www.atlassian.com/git/tutorials/big-repositories [[}]]


• Git: Symbolic Ref best-patterns
@[https://stackoverflow.com/questions/4986000/whats-the-recommended-usage-of-a-git-symbolic-reference]

• GitHub: Search by topic: [[{git.github}]]
  https://help.github.com/en/github/searching-for-information-on-github/searching-topics
  Ex:search by topic ex "troubleshooting" and language "java"
  https://github.com/topics/troubleshooting?l=java


• Gitsec: [[{security.secret_management,qa]]
  @[https://github.com/BBVA/gitsec]
  gitsec is an automated secret discovery service for git that helps
  you detect sensitive data leaks.
  gitsec doesn't directly detect sensitive data but uses already
  available open source tools with this purpose and provides a
  framework to run them as one.
[[}]]

[[}]]
[[git}]]

#############################################################
● Containerization ("Docker") Summary [[{containerization]] #
#############################################################

@[https://reproducible-builds.org/]
  Containerization == "Reproducible Builds"
  == set of software development practices that create an
     independently-verifiable path from source to binary code.

• OCI Standards Specification:  [[{containerization.runtime,standards,1_doc_type.comparative]]
@[https://www.opencontainers.org/faq]

- OCI mission: promote a set of common, minimal, open standards and specifications
  around container technology focused on creating formal specification for
  container image formats and runtime

- VALUES: (mostly adopted from the appc founding values)
  - Composable: All tools for downloading, installing, and running
    containers should be well integrated, but independent and composable.
  - Portable: runtime standard should be usable across different hardware,
    operating systems, and cloud environments.
  - Secure: Isolation should be pluggable, and the cryptographic primitives
    for strong trust, image auditing and application identity should be solid.
  - Decentralized: Discovery of container images should be simple and
    facilitate a federated namespace and distributed retrieval.
  - Open: format and runtime should be well-specified and developed by
    a community.
  - Code leads spec, rather than vice-versa. !!!
  - Minimalist: do a few things well, be minimal and stable, and
  - Backward compatible.

- Docker donated both a draft specification and a runtime and code
  associated with a reference implementation of that specification:
  It includes entire contents of the libcontainer project, including
  "nsinit" and all modifications needed to make it run independently
  of Docker.  This codebase, called runc, can be found at
  https://github.com/opencontainers/runc

- the responsibilities of the Technical Oversight Board (TOB)
  can be followed at https://github.com/opencontainers/tob:
  - Serving as a source of appeal if the project technical leadership
    is not fulfilling its duties or is operating in a manner that is
    clearly biased by the commercial concerns of the technical
    leadership’s employers.
  - Reviewing the tests established by the technical leadership for
    adherence to specification
  - Reviewing any policies or procedures established by the technical leadership.

- The OCI seeks rough consensus and running code first.

- What is the OCI’s perspective on the difference between a standard and a specification?
  The v1.0.0 2017-07-19.
  - Adopted by:
    - Cloud Foundry community by embedding runc via Garden
    - Kubernetes is incubating a new Container Runtime Interface (CRI)
      that adopts OCI components via implementations like CRI-O and rklet.
    - rkt community is adopting OCI technology already and is planning
      to leverage the reference OCI container runtime runc in 2017.
    - Apache Mesos.
    - AWS announced OCI image format in its Amazon EC2 Container Registry (ECR).

  - Will the runtime and image format specs support multiple platforms?

  - How does OCI integrate with CNCF?
      A container runtime is just one component of the cloud native
    technical architecture but the container runtime itself is out of
    initial scope of CNCF (as a CNCF project), see the charter Schedule A
    for more information.
[[}]]

● Container Ecosystem:  [[{containerization.runtime,containerization.performance]] #[runtimes_summary]
                        [[02_doc_has.comparative,standards]]
- Image creation:
  Alt 1) Dockerfile -> docker build
  Alt 2) buildah: Similar to docker build, it also allow to
         add image-lyaer manually from the host command line.
         (removing the need for a Dockerfile).
         (RedHat rootless 'podman' is based on buildah)
  Alt 3) Java source code → jib → OCI image
  Alt 4) Google Kaniko
  ...

- Runtimes: @[#runtimes_summary]
  Alt 1) runC       (OOSS, Go-based, maintained by docker and others)
  Alt 2) Crun       (OOSS, C-based, faster than runC )
  Alt 2) containerd (OOSS, maintained by IBM and others)
  Alt 3) CRI-O: very lightweight alterantive for k8s
  Alt 3) rklet

- Registries and Repositories
  repository: "storage" for OCI binary images.
  registry:   index of 1+ repositories (ussually its own repo)

- Container Orchestration "==" Kubernetes, AWS EC2, ...
────────────────────────────────────────────────────────────────────────────────
• runc runtime:
@[https://github.com/opencontainers/runc]
- Reference runtime and cli tool donated by Docker for spawning and
  running containers according to the OCI specification:
@[https://www.opencontainers.org/]

- Based on Go.

- *It reads a runtime specification and configures the Linux kernel.*
  - Eventually it creates and starts container processes.
   *Go might not have been the best programming language for this task*.
   *since it does not have good support for the fork/exec model of computing.*
   *- Go's threading model expects programs to fork a second process      *
   *  and then to exec immediately.                                       *
   *- However, an OCI container runtime is expected to fork off the first *
   *  process in the container.  It may then do some additional           *
   *  configuration, including potentially executing hook programs, before*
   *  exec-ing the container process. The runc developers have added a lot*
   *  of clever hacks to make this work but are still constrained by Go's *
   *  limitations.                                                        *
   *crun, C based, solved those problems.*

- reference implementation of the OCI runtime specification.

• crun ────────────────────────────────────────────────────────────────────────
@[https://github.com/containers/crun/issues]
@[https://www.redhat.com/sysadmin/introduction-crun]
- fast, low-memory footprint container runtime by Giuseppe Scrivanoby
  (RedHat).
- C based: Unlike Go, C is not multi-threaded by default, and was built
  and designed around the fork/exec model.
  It could handle the fork/exec OCI runtime requirements in a much cleaner
  fashion than 'runc'. C also interacts very well with the Linux kernel.
  It is also lightweight, with much smaller sizes and memory than runc(Go):
  compiled with -Os, 'crun' binary is ~300k (vs ~15M 'runc')
  "" We have experimented running a container with just  *250K limit set*.""
 *or 50 times smaller.* and up to  *twice as fast.

- cgroups v2 ("==" Upstream kernel, Fedora 31+) compliant from the scratch
  while runc -Docker/K8s/...-  *gets "stuck" into cgroups v1.*
  (experimental support in 'runc' for v2 as of v1.0.0-rc91, thanks to
   Kolyshkin and Akihiro Suda).

- feature-compatible with "runc" with extra experimental features.

- Given the same Podman CLI/k8s YAML we get the same containers "almost
  always" since  *the OCI runtime's job is to instrument the kernel to*
 *control how PID 1 of the container runs.*
 *It is up to higher-level tools like conmon or the container engine to*
 *monitor the container.*

- Sometimes users want to limit number of PIDs in containers to just one.
  With 'runc' PIDs limit can not be set too low, because the Go runtime
  spawns several threads.
  'crun', written in C, does not have that problem.
  Ex:
  $ RUNC="/usr/bin/runc" , CRUN="/usr/bin/crun"
  $ podman --runtime $RUNC run --rm --pids-limit 5 fedora echo it works
                                    └────────────┘
  →  Error: container create failed (no logs from conmon): EOF
  $ podman --runtime $CRUN run --rm --pids-limit 1 fedora echo it works
                                    └────────────┘
  →  it works

- OCI hooks supported, allowing the execution of specific programs at
  different stages of the container's lifecycle.

- runc/crun comparative:
  $ CMD_RUNC="for i in {1..100}; do runc run foo < /dev/null; done"
  $ CMD_CRUN="for i in {1..100}; do crun run foo < /dev/null; done"
  $ time -v sh -c "$CMD_RUNC"
  → User time (seconds): 2.16
  → System time (seconds): 4.60
  → Elapsed (wall clock) time (h:mm:ss or m:ss): 0:06.89
  → Maximum resident set size (kbytes): 15120
  → ...
  $ time -v sh -c "$CMD_CRUN"
  → ...
  → User time (seconds): 0.53
  → System time (seconds): 1.87
  → Elapsed (wall clock) time (h:mm:ss or m:ss): 0:03.86
  → Maximum resident set size (kbytes): 3752
  → ...

- Experimental features:
  - redirecting hooks STDOUT/STDERR via annotations.
    - Controlling stdout and stderr of OCI hooks
      Debugging hooks can be quite tricky because, by default,
      it's not possible to get the hook's stdout and stderr.
    - Getting the error or debug messages may require some yoga.
    - common trick: log to syslog to access hook-logs via journalctl.
                     (Not always possible)
    - With 'crun' + 'Podman':
    $*$ podman run --annotation run.oci.hooks.stdout=/tmp/hook.stdout*
                                └───────────────────────────────────┘
                                 executed hooks will write:
                                  STDOUT → /tmp/hook.stdout
                                  STDERR → /tmp/hook.stderr
    *(proposed fo OCI runtime spec)*

  - crun supports running older versions of systemd on cgroup v2 using
    --annotation run.oci.systemd.force_cgroup_v1,
    This forces a cgroup v1 mount inside the container for the name=systemd hierarchy,
    which is enough for systemd to work.
    Useful to run older container images, such as RHEL7, on a cgroup v2-enabled system.
    Ej:
  $*$ podman run --annotation run.oci.systemd.force_cgroup_v1=/sys/fs/cgroup \ *
  $*  centos:7 /usr/lib/systemd/systemd                                        *

  - Crun as a library:
    "We are considering to integrate it with  *conmon, the container monitor used by*
    *Podman and CRI-O, rather than executing an OCI runtime."*

- 'crun' Extensibility:
  """... easily to use all the kernel features, including syscalls not enabled in Go."""
  -Ex: openat2 syscall protects against link path attacks (already supported by crun).

- 'crun' is more portable: Ex: Risc-V.
[[}]]

● Container Network Iface (CNI): [[{containerization.networking,101,01_PM.TODO]]
@[https://github.com/containernetworking/cni]
- specification and libraries for writing plugins to configure network interfaces
  in Linux containers, along with a number of supported plugins.
- CNI concerns itself only with network connectivity of containers
  and removing allocated resources when the container is deleted.
- <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI Spec</a>

- CNI concerns itself only with network connectivity of
  containers and removing allocated resources when container
  are deleted.
- specification and libraries for writing plugins
  to configure network interfaces in Linux containers,
  along with a number of supported plugins:
  - libcni, a CNI runtime implementation
  - skel, a reference plugin implementation
    github.com/cotainernetworking/cni
- Set of reference and example plugins:
  - Inteface plugins:  ptp, bridge,macvlan,...
  - "Chained" plugins: portmap, bandwithd, tuning,
    github.com/cotainernetworking/pluginds

    NOTE: Plugins are executable programs with STDIN/STDOUT
                                  ┌ Network
                ┌─────→(STDIN)    │
  Runtime → ADD JSON    CNI ···───┤
   ^        ^^^         executable│
   │        ADD         plugin    └ Container(or Pod)
   │        DEL         └─┬──┘      Interface
   │        CHECK         v
   │        VERSION    (STDOUT)
   │                 └────┬──────┘
   │                      │
   └──── JSON result ─────┘

 *Runtimes*            *3rd party plugins*
  K8s, Mesos, podman,   Calico ,Weave, Cilium,
  CRI-O, AWS ECS, ...   ECS CNI, Bonding CNI,...

- The idea of CNI is to provide common interface between
  the runtime and the CNI (executable) plugins through
  standarised JSON messages.

  Example cli Tool  executing CNI config:
@[https://github.com/containernetworking/cni/tree/master/cnitool]
   INPUT_JSON
   {
     "cniVersion":"0.4.0",   ← Standard attribute
     "name": *"myptp"*,
     "type":"ptp",
     "ipMasq":true,
     "ipam": {               ← Plugin specific attribute
       "type":"host-local",
       "subnet":"172.16.29.0/24",
       "routes":[{"dst":"0.0.0.0/0"}]
     }
   }
   $ echo $INPUT_JSON | \                  ← Create network config
     sudo tee /etc/cni/net.d/10-myptp.conf   it can be stored on file-system
                                             or runtime artifacts (k8s etcd,...)

   $ sudo ip netns add testing             ← Create network namespace.
                       └-----┘

   $ sudo CNI_PATH=./bin \                 ← Add container to network
     cnitool add  *myptp*  \
     /var/run/netns/testing

   $ sudo CNI_PATH=./bin \                ← Check config
     cnitool check myptp \
     /var/run/netns/testing


   $ sudo ip -n testing addr               ← Test
   $ sudo ip netns exec testing \
     ping -c 1 4.2.2.2

   $ sudo CNI_PATH=./bin \                 ← Clean up
     cnitool del myptp \
     /var/run/netns/testing
   $ sudo ip netns del testing

 *Maintainers (2020):*
  - Bruce Ma (Alibaba)
  - Bryan Boreham (Weaveworks)
  - Casey Callendrello (IBM Red Hat)
  - Dan Williams (IBM Red Hat)
  - Gabe Rosenhouse (Pivotal)
  - Matt Dupre (Tigera)
  - Piotr Skamruk (CodiLime)
  - "CONTRIBUTORS"

 *Chat channels*
  - https.//slack.cncf.io  - topic #cni
[[}]]

● Docker  summary: [[{containerization.docker,containerization.storage.host,containerization.image.registry,]]
                   [[containerization.runtime,containerization.security,TODO,containerization.orchestration.swarn]]
  - External Links:
  - @[https://docs.docker.com/]
  - @[https://github.com/jdeiviz/docker-training] D.Peman@github
  - @[https://github.com/jpetazzo/container.training] container.training@Github
  - @[http://container.training/]

  - Docker API:
    - @[https://docs.docker.com/engine/api/])
    - @[https://godoc.org/github.com/docker/docker/api]
    - @[https://godoc.org/github.com/docker/docker/api/types]

  - docker help summary:
  Usage: docker COMMAND

  A self-sufficient runtime for containers
  Options:
  --config string      Location of client config files (default "/root/.docker")
  --debug              Enable debug mode
  --host list          Daemon socket(s) to connect to
  --log-level $level   := "debug"|"info"*|"warn"|"error"|"fatal"
  --tls                Use TLS; implied by --tlsverify
  --tlscacert ...      Trust certs signed only by this CA (default "/root/.docker/ca.pem")
  --tlscert string     Path to TLS certificate file       (default "/root/.docker/cert.pem")
  --tlskey string      Path to TLS key file               (default "/root/.docker/key.pem")
  --tlsverify          Use TLS and verify the remote
  --version            Print version information and quit

• Management Commands:
              Manage ...
  config      Docker configs
  container   containers
  network     networks
  node        Swarm nodes
  plugin      plugins
  secret      Docker secrets
  service     services
  swarm       Swarm
  system      Docker
  trust       trust on
              Docker images
  volume      volumes


• docker  running containers commands :
| · attach      Attach local STDIN/OUT/ERR streams to a running container
| · cp          Copy files/folders between a container and the local filesystem
| · create      Create a new container
| · exec        Run a command in a running container
| · kill        Kill one or more running containers
| · logs        Fetch the logs of a container
| · pause       Pause all processes within one or more containers
| · restart     Restart one or more containers
| · rm          Remove 1+ containers
| · run         Run a command in a new container
| · start       Start 1+ stopped containers
| · stop        Stop 1+ running containers
| · top         Display running processes for container
|               NOTE: '$ docker stats' is really what most people want
|               when searching for a tool similar to UNIX "top".
  · port        List port mappings or specific mapping for container
| · unpause     Unpause all processes within 1+ containers
| · wait        Block until 1+ containers stop, print their exit codes
| · rename      Rename a container
| · stats       Display a live stream of container(s) resource usage statistics
| · update      Update configuration of 1+ containers
|               (cpu-quota, shared, kernel-memory, pids-limit, ...)

• docker Image Build/Image Management Commands :
| · image       ls current (local) images.
| · build       Build an image from a Dockerfile
| · commit      Create a new image from a container's changes
| · diff        Inspect changes to files or directories on a container's filesystem
| · export      Export a container's filesystem as a tar archive
| · history     Show the history of an image
| · images      List images
| · import      Import the contents from a tarball to create a filesystem image
| · load        Load an image from a tar archive or STDIN
| · save        Save one or more images to a tar archive (streamed to STDOUT by default)
| · tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE

• docker Throubleshoot/Debug Commands :
| · events      Get real time events from the server
  ┌─ $ sudo docker info ─(Global Info) ────────
  │ Containers: 23
  │    Running: 10
  │     Paused: 0
  │    Stopped: 1
  │ Images: 36
  │ Server Version: 17.03.2-ce
  │ *Storage Driver: devicemapper*
  │  Pool Name: docker-8:0-128954-pool
  │  Pool Blocksize: 65.54 kB
  │  Base Device Size: 10.74 GB
  │  Backing Filesystem: ext4
  │  Data file: /dev/loop0
  │  Metadata file: /dev/loop1
  │      Data Space Used      : 3.014 GB *
  │      Data Space Total     : 107.4 GB *
  │      Data Space Available : 16.11 GB *
  │  Metadata Space Used      : 4.289 MB *
  │  Metadata Space Total     : 2.147 GB *
  │  Metadata Space Available : 2.143 GB
  │ *Thin Pool Min. Free Space: 10.74 GB*
  │  Udev Sync Supported: true
  │  Deferred Removal Enabled: false
  │  Deferred Deletion Enabled: false
  │  Deferred Deleted Device Count: 0
  │      Data loop file: /var/...devicemapper/data
  │  Metadata loop file: /var/.../devicemapper/metadata
  │  Library Version: 1.02.137 (2016-11-30)
  │  Logging Driver: json-file
  │  Cgroup Driver: cgroupfs
  │ Plugins:
  │  Volume: local
  │  Network: bridge host macvlan null overlay
  │ Swarm: inactive
  │ Runtimes: runc
  │ Default Runtime: runc
  │ Init Binary: docker-init
  │ containerd version: 4ab9917febca...
  │ runc       version: 54296cf40ad8143b62...
  │ init       version: 949e6fa
  │ Security Options:    [[{security]]
  │  seccomp
  │   Profile: default   [[}]]
  │ Kernel Version: 4.17.17-x86_64-linode116
  │ Operating System: Debian GNU/Linux 9 (stretch)
  │ OSType: linux
  │ Architecture: x86_64
  │ CPUs: 4
  │ Total Memory: 3.838 GiB
  │ Name: MyLaptop01
  │ ID: ZGYA:L4MN:CDCP:DANS:IEHQ:...
  │ *Docker Root Dir: /var/lib/docker*
  │ *Debug Mode (client): false*
  │ *Debug Mode (server): false*
  │ *Registry: https://index.docker.io/v1/*
  │ Experimental: false
  │ Insecure Registries:
  │  127.0.0.0/8
  │ Live Restore Enabled: false
  └───────────────────────────────────────────────────
  · inspect     Return low-level information on Docker objects
| · ps          List running containers summary
|               -a: (all) show also exited containers.
| · version     Show the Docker version information


● /var/run/docker.sock:
@[https://medium.com/better-programming/about-var-run-docker-sock-3bfd276e12fd]
  - Unix socket the Docker daemon listens on by default,
    used to communicate with the daemon from within a container.
  - Can be mounted on containers to allow them to control Docker:
    This is potentially a security hole and must be restricted to
    "special" container (e.g: Kubernetes controlers,...)
    $ docker run -v /var/run/docker.sock:/var/run/docker.sock ....

● Docker Networks: [[{containerization.networking,101]]
  $ docker network create  network01     <-  Create new network

  $ docker run      \
    --network network01 \                <- Use it.
    -h redis-server -p 6379
    -name redis-server01

  $ docker run --rm -ti \
    --network  network01 \               <- Use it. Now client can connect
    -d redis                                to server in shared network01.

  $ docker network ls                    <- List existing networks

  $ docker disconnect \                  <- Disconnect server from network
    network01 redis-server01

  $ docker connect --alias db \          <- Connect server to network
    network01  redis-server01

[[}]]

● VOLUMES [[{containerization.storage]]

                                            REUSING VOLUMES:
  $ docker run -it  --name alpha \       <- Create new container mounting
    -v $(pwd)/log:/var/log  \               host directoy as volume
    ubuntu bash

  $ docker run \
    --volumes-from alpha                 <- Share volumes from alpha container
    --name beta
    ubuntu


                                            CREAR REUSABLE VOLUME
  $ docker volume create --name=www_data <- Create Volume
  $ docker run -d -p 8888:80 \
    -v www_data:/var/www/html            <- reuse in new (nginx) container
    -v logs:/var/log/nginx nginx

  $ docker run \
    -v  www_data:/website                <- reuse in (vi) container
    -w /website \                           <- Make it writable to container
    -it alpine vi index.html
[[}]]

● docker-compose: [[{containerization.orchestration.docker-compose]]
- "Self documenting" YAML file defining services, networks and volumes.
  Full ref: @[https://docs.docker.com/compose/compose-file/]
  Ideal to create development/production enviroments running just on a single host
  (vs a pool of workers, like it's the case with Kubernetes, AWS EC2, ...)

- Best Patterns: https://docs.docker.com/compose/production/  [[01_PM.TODO]]

- Docker compose example:
  C&P from https://github.com/bcgov/moh-prime/blob/develop/docker-compose.yml

  ---
  version: "3"

  services:
  ######################################################### Database #
    postgres:
      restart: always                       # <- "always", "on-failure:5", "no"
      container_name: primedb
     *image: postgres:10. *                 # use pre-built image
      environment:
        POSTGRES_PASSWORD: postgres
        ...
      ports:
        - "5432:5432"
      volumes:
        - local_postgres_data:/var/lib/postgresql/data
     *networks:*                            # ← Networks to connect to
     *  - primenet*
  ########################################################## MongoDB #
    mongo:
      restart: always
      container_name: primemongodb
      image: mongo:3
      environment:
        MONGO_INITDB_ROOT_USERNAME: root
        ...
      ports:
        - 8081:8081
      volumes:
        - local_mongodb_data:/var/lib/mongodb/data
     *networks:*
     *  - primenet*
  ############################################################## API #
    dotnet-webapi:
      container_name: primeapi
      restart: always
     *build:*                               # ← use Dockerfile to build image
        context: prime-dotnet-webapi/   *WARN*: remember to rebuild image and recreate
                                              app’s containers like:
                                            ┌───────────────────────────────────────────────┐
                                            │ $ docker-compose build dotnet-webapi          │
                                            │                                               │
                                            │ $ docker-compose up \ ← stop,destroy,recreate │
                                            │   --no-deps           ← prevents from also    │
                                            │   -d dotnet-webapi      recreating any service│
                                            │                         primeapi depends on.  │
                                            └───────────────────────────────────────────────┘
      command: "..."
      environment:
        ...
     *ports:          *  ← Exposed ports outside private "primenet" network
     *  - "5000:8080" *  ← Map internal port (right) to "external" port
     *  - "5001:5001" *
     *expose:*          ←   Expose ports without publishing to host machine
     *   - "5001"*          (only accessible to linked services).
                             Use internal port.
     *networks:*
     *  - primenet*
      depends_on:
        - postgres
  ##################################################### Web Frontend #
    nginx-angular:
      build:
           context: prime-angular-frontend/
      ...
  ################################################ Local SMTP Server #
    mailhog:
      container_name: mailhog
      restart: always
      image: mailhog/mailhog:latest
      ports:
        - 25:1025
        - 1025:1025
        - 8025:8025 # visit localhost:8025 to see the list of captured emails
      ...
  ########################################################### Backup #
    backup:
      ...
      restart: on-failure
      volumes:
       *- db_backup_data:/opt/backup*
      ...

  volumes:
    local_postgres_data:
    local_mongodb_data:
    db_backup_data:

 *networks:*
    primenet:
      driver: bridge

 *Example  *
  ---
  version: '3.6'

  # reusable yaml template ############################################
  x-besu-bootnode-def:
    &besu-bootnode-def
    restart: "on-failure"
    image: hyperledger/besu:${BESU_VERSION:-latest}
    environment:
      - LOG4J_CONFIGURATION_FILE=/config/log-config.xml
    entrypoint:
      - /bin/bash
      - -c
      - |
        /opt/besu/bin/besu public-key export --to=/tmp/bootnode_pubkey;
        /opt/besu/bin/besu \
        --config-file=/config/config.toml \
        --p2p-host=$$(hostname -i) \
        --genesis-file=/config/genesis.json \
        --node-private-key-file=/opt/besu/keys/key \
        --min-gas-price=0 \
        --rpc-http-api=EEA,WEB3,ETH,NET,PERM,${BESU_CONS_API:-IBFT} \
        --rpc-ws-api=EEA,WEB3,ETH,NET,PERM,${BESU_CONS_API:-IBFT} ;

  # reusable yaml template ############################################
  x-besu-def:
    &besu-def
    restart: "on-failure"
    image: hyperledger/besu:${BESU_VERSION:-latest}
    environment:
      - LOG4J_CONFIGURATION_FILE=/config/log-config.xml
    entrypoint:
      - /bin/bash
      - -c
      - |
        while [ ! -f "/opt/besu/public-keys/bootnode_pubkey" ]; do sleep 5; done ;
        /opt/besu/bin/besu \
        --config-file=/config/config.toml \
        --p2p-host=$$(hostname -i) \
        --genesis-file=/config/genesis.json \
        --node-private-key-file=/opt/besu/keys/key \
        --min-gas-price=0 \
        --rpc-http-api=EEA,WEB3,ETH,NET,PERM,${BESU_CONS_API:-IBFT} \
        --rpc-ws-api=EEA,WEB3,ETH,NET,PERM,${BESU_CONS_API:-IBFT} ;

  services:
  # using the YAML template ####################################################
    validator1:
      << : *besu-bootnode-def
      volumes:
        - public-keys:/tmp/
        - ./config/besu/config.toml:/config/config.toml
        - ./config/besu/permissions_config.toml:/config/permissions_config.toml
        - ./config/besu/log-config.xml:/config/log-config.xml
        - ./logs/besu:/var/log/
        - ./config/besu/${BESU_CONS_ALGO:-ibft2}Genesis.json:/config/genesis.json
        - ./config/besu/networkFiles/validator1/keys:/opt/besu/keys
      networks:
        quorum-dev-quickstart:
          ipv4_address: 172.16.239.11
  # using the YAML template ####################################################
    validator2:
      << : *besu-def
      volumes:
        - public-keys:/opt/besu/public-keys/
        - ./config/besu/config.toml:/config/config.toml
        - ./config/besu/permissions_config.toml:/config/permissions_config.toml
        - ./config/besu/log-config.xml:/config/log-config.xml
        - ./logs/besu:/var/log/
        - ./config/besu/${BESU_CONS_ALGO:-ibft2}Genesis.json:/config/genesis.json
        - ./config/besu/networkFiles/validator2/keys:/opt/besu/keys
      depends_on:
        - validator1
      networks:
        quorum-dev-quickstart:
          ipv4_address: 172.16.239.12
  # using the YAML template ####################################################
    ...
  volumes:
    public-keys:
    prometheus:
    grafana:

  networks:
    quorum-dev-quickstart:
      driver: bridge
      ipam:
        config:
          - subnet: 172.16.239.0/24

• docker-compose SystemD Integration:

  STEP 1) Create some config at /etc/compose/docker-compose.yml

  STEP 2) Create systemd Service:
  ┌─ /etc/systemd/system/docker-compose.service ────────────┐
  │ (Service unit to start and manage docker compose)       │
  │ [Unit]                                                  │
  │ Description=Docker Compose container starter            │
  │ After=docker.service network-online.target              │
  │ Requires=docker.service network-online.target           │
  │                                                         │
  │ [Service]                                               │
  │ WorkingDirectory=/etc/compose                           │
  │ Type=oneshot                                            │
  │ RemainAfterExit=yes                                     │
  │                                                         │
  │ ExecStartPre=-/usr/local/bin/docker-compose pull --quiet│
  │ ExecStart=/usr/local/bin/docker-compose up -d           │
  │                                                         │
  │ ExecStop=/usr/local/bin/docker-compose down             │
  │                                                         │
  │ ExecReload=/usr/local/bin/docker-compose pull --quiet   │
  │ ExecReload=/usr/local/bin/docker-compose up -d          │
  │                                                         │
  │ [Install]                                               │
  │ WantedBy=multi-user.target                              │
  └─────────────────────────────────────────────────────────┘

  ┌─ /etc/systemd/system/docker-compose-reload.service ──────────────┐
  │ (Executing unit to trigger reload on docker-compose.service)     │
  │                                                                  │
  │ [Unit]                                                           │
  │ Description=Refresh images and update containers                 │
  │                                                                  │
  │ [Service]                                                        │
  │ Type=oneshot                                                     │
  │                                                                  │
  │ ExecStart=/bin/systemctl reload-or-restart docker-compose.service│
  └──────────────────────────────────────────────────────────────────┘

  ┌─ /etc/systemd/system/docker-compose-reload.timer ┐
  │ (Timer unit to plan the reloads)                 │
  │ [Unit]                                           │
  │ Description=Refresh images and update containers │
  │ Requires=docker-compose.service                  │
  │ After=docker-compose.service                     │
  │                                                  │
  │ [Timer]                                          │
  │ OnCalendar=*:0/15                                │
  │                                                  │
  │ [Install]                                        │
  │ WantedBy=timers.target                           │
  └──────────────────────────────────────────────────┘
[[}]]

● Container Registry ("Image repository") Summary [[{containerization.image.registry,01_PM.TODO]]
@[https://docs.docker.com/registry/#what-it-is]
@[https://docs.docker.com/registry/introduction/]
  $ docker run -d -p 5000:5000 \  ← Start registry server
    --restart=always
    --name registry registry:2

  $ docker pull ubuntu            ← Pull (example) image
  $ docker image tag ubuntu \     ← Tag the image to "point"
    localhost:5000/myfirstimage     to local registry
  $ docker push \                 ← Push to local registry
    localhost:5000/myfirstimage
  $ docker pull \                 ← final Check
    localhost:5000/myfirstimage

  NOTE: clean setup testing like:
  $ docker container stop  registry
  $ docker container rm -v registry
[[}]]

● Dockerize (non-friendly container apps): [[{containerization.image.build,01_PM.low_code,containerization.troubleshooting]]
  @[https://github.com/jwilder/dockerize]
  Utility to simplify running applications in docker containers allowing to:
  - generate app config. files AT CONTAINER STARTUP TIME
    from templates and container environment variables.
  - Tail multiple log files to stdout and/or stderr.
  - Wait for other services to be available using TCP, HTTP(S),
    unix before starting the main process.

  Typical use case:
   - application that has one or more configuration files and
     you would like to control some of the values using ENV.VARs.
   - dockerize allows to set an environment variable and update the
     config file before starting the contenerized application
   - other use case: forward logs from harcoded files on the filesystem to stdout/stderr
     (Ex: nginx logs to /var/log/nginx/access.log and /var/log/nginx/error.log by default)
[[}]]

● Managing Containers:

$ docker run \                   <- Create and run container based on imageXX
  --rm \                         <- Remove on exit (remove to see container's logs after exit)
  --name name01 \                <- assign name (or fail if name already assigned)
  --network network01 \          <- attach to network01 (that must have been created previously)
  somerepo/image01


$ docker logs docker             <- Dump full container logs
$ docker logs --tail 3           <- Dump last 3 lines
$ docker logs --tail 1 --follow  <- Dump last 3 lines, then follow future los.

$ docker stop containerXX && \   <- Try to stop container properly.
  sleep 30 && \
  docker kill containerXX        <- Finally if it doesn't respond in 30 secs.

$ docker container prune  <- Prune stopped containers

● Monitoring running containers 101 [[{containerization.monitoring]]
List containers instances:
   $ docker ps     # only running
   $ docker ps -a  # also finished, but not yet removed (docker rm ...)
   $ docker ps -lq # TODO:

"top" containers showing Net IO read/writes, Disk read/writes:
   $ docker stats
   | CONTAINER ID   NAME                    CPU %   MEM USAGE / LIMIT     MEM %   NET I/O          BLOCK I/O      PIDS
   | c420875107a1   postgres_trinity_cache  0.00%   11.66MiB / 6.796GiB   0.17%   22.5MB / 19.7MB  309MB / 257kB  16
   | fdf2396e5c72   stupefied_haibt         0.10%   21.94MiB / 6.796GiB   0.32%   356MB / 693MB    144MB / 394MB  39

   $ docker top 'containerID'
   | UID       PID     PPID    C  STIME  TTY   TIME     CMD
   | systemd+  26779   121423  0  06:11  ?     00:00:00 postgres: ddbbName cache 172.17.0.1(35678) idle
   | ...
   | systemd+  121423  121407  0  Jul06  pts/0 00:00:44 postgres
   | systemd+  121465  121423  0  Jul06  ?     00:00:01 postgres: checkpointer process
   | systemd+  121466  121423  0  Jul06  ?     00:00:26 postgres: writer process
   | systemd+  121467  121423  0  Jul06  ?     00:00:25 postgres: wal writer process
   | systemd+  121468  121423  0  Jul06  ?     00:00:27 postgres: autovacuum launcher process
   | systemd+  121469  121423  0  Jul06  ?     00:00:57 postgres: stats collector process
[[}]]



● Dockviz: [[{containerization.monitoring,gui,containerization.troubleshooting]]
@[https://github.com/justone/dockviz]
  Show a graph of running containers-dependencies and image-dependencies.

Other options:
$*dockviz images - *
└─511136ea3c5a Virtual Size: 0.0 B
  ├─f10ebce2c0e1 Virtual Size: 103.7 MB
  │ └─82cdea7ab5b5 Virtual Size: 103.9 MB
  │   └─5dbd9cb5a02f Virtual Size: 103.9 MB
  │     └─74fe38d11401 Virtual Size: 209.6 MB Tags: ubuntu:12.04, ubuntu:precise
  ├─ef519c9ee91a Virtual Size: 100.9 MB
  └─02dae1c13f51 Virtual Size: 98.3 MB
    └─e7206bfc66aa Virtual Size: 98.5 MB
      └─cb12405ee8fa Virtual Size: 98.5 MB
        └─316b678ddf48 Virtual Size: 169.4 MB Tags: ubuntu:13.04, ubuntu:raring

$ dockviz images -t -l                   #  <- show only labelled images
└─511136ea3c5a Virtual Size: 0.0 B
  ├─f10ebce2c0e1 Virtual Size: 103.7 MB
  │ └─74fe38d11401 Virtual Size: 209.6 MB Tags: ubuntu:12.04, ubuntu:precise
  ├─ef519c9ee91a Virtual Size: 100.9 MB
  │ └─a7cf8ae4e998 Virtual Size: 171.3 MB Tags: ubuntu:12.10, ubuntu:quantal
  │   ├─5c0d04fba9df Virtual Size: 513.7 MB Tags: nate/mongodb:latest
  │   └─f832a63e87a4 Virtual Size: 243.6 MB Tags: redis:latest
  └─02dae1c13f51 Virtual Size: 98.3 MB
    └─316b678ddf48 Virtual Size: 169.4 MB Tags: ubuntu:13.04, ubuntu:raring


$ dockviz images - -i *                 #  <- Show incremental size (vs cumulative)
└─511136ea3c5a Virtual Size: 0.0 B
  ├─f10ebce2c0e1 Virtual Size: 103.7 MB
  │ └─82cdea7ab5b5 Virtual Size: 255.5 KB
  │   └─5dbd9cb5a02f Virtual Size: 1.9 KB
  │     └─74fe38d11401 Virtual Size: 105.7 MB Tags: ubuntu:12.04, ubuntu:precise
  └─02dae1c13f51 Virtual Size: 98.3 MB
    └─e7206bfc66aa Virtual Size: 190.0 KB
      └─cb12405ee8fa Virtual Size: 1.9 KB
        └─316b678ddf48 Virtual Size: 70.8 MB Tags: ubuntu:13.04, ubuntu:raring
[[}]]

● Managing Images:  [[{containerization.image.registry]]
  $ docker images        # ← List local ("downloaded/instaled") images

  $ docker search redis  # ← Search remote images @ Docker Hub:

  $ docker rmi /${IMG_NAME}:${IMG_VER}  # ← remove (local) image
  $ docker image prune                  # ← remove all non used images

  PUSH/PULL Images from Private Registry
  ======================================
  PRE-SETUP) (Optional opinionated, but recomended)
  -  Define ENV. VARS. in  ENVIRONMENT file
    ┌ ENVIRONMENT ────────────────────────────────────────────┐
    │ #  COMMON ENV. PARAMS for PRIVATE/PUBLIC REGISTRY: {{   │
    │ USER=user01                                             │
    │ IMG_NAME="postgres_custom"                              │
    │ IMG_VER="1.0"  # ← Defaults to 'latest'                 │
    │ # }}                                                    │
    │ # PRIVATE REGISTRY ENV. PARAMS ONLY : {{                │
    │ SESSION_TOKEN="dAhYK9Z8..."  # ← Updated Each 'N' hours │
    │ REGISTRY=docker_registry.myCompany.com                  │
    │ # }}                                                    │
    └─────────────────────────────────────────────────────────┘

  UPLOAD IMAGE SCRIPT)
  ┌─ push_image_to_private_registry.sh ──────┐
  │ #!/bin/bash                              │
  │ set -e # ← stop on first error           │
  │ source ENVIRONMENT                       │
  │                                          │
  │ sudo docker login \                      │
  │    -u ${LOGIN_USER} \                    │
  │    -p ${SESSION_TOKEN} \                 │
  │    ${REGISTRY}                           │
  │                                          │
  │ TARGET=""                                │
  │ TARGET="${TARGET}${REGISTRY}/${USER}"    │
  │ TARGET="${TARGET}/${IMG_NAME}:${IMG_VER}"│
  │ sudo docker push ${TARGET}               │
  └──────────────────────────────────────────┘

  DOWNLOAD IMAGE)
  $ docker pull ${REGISTRY}/${USER}/${IMG_NAME}:${IMG_VER}  <- ALT1: DOWNLOAD FROM PRIVATE REGISTRY
  $ docker pull ${IMG_NAME}:${IMG_VER} [[}]]                <- ALT2: DOWNLOAD FROM DOCKER HUB

● BUILD DOCKER/OCI IMAGE 101 [[{containerization.dockerfile,containerization.image.build]]
  ┌─ Dockerfile ───────────────────────────────┐
  │ FROM registry.redhat.io/ubi7/ubi           <- 72.7 MB layer
  │ RUN dnf update                             <- 30.1 MB layer
  │ COPY target/dependencies /app/dependencies <- 10.0 MB layer
  │ COPY target/resources    /app/resources    <-  9.0 MB layer
  │ COPY target/classes      /app/classes      <-  0.5 MB layer
  │                                            │  └──┬──┘
  │ ENTRYPOINT java -cp \                      │ Put most frequently changed
  │   /app/dependencies/*:... \                │ layer down the layer "stack",
  │   my.app.Main                              │ so that when building and/or
  └────────────────────────────────────────────┘ uploading new images only
                                                 them will be affected.
                                     Probably the most frequently changed layer
                                     is also the smaller layer

    Standard Labels are also recomended. They can be added like:
    LABEL org.label-schema.build-date=2022-02-17T05:47Z
    LABEL org.label-schema.description=Ethereum transaction signing application,
    LABEL org.label-schema.name=Ethsigner,
    LABEL org.label-schema.schema-version=1.0,
    LABEL org.label-schema.url=https://docs.ethsigner.consensys.net,
    LABEL org.label-schema.vcs-ref=4c42aec7,
    LABEL org.label-schema.vcs-url=https://github.com/ConsenSys/ethsigner,
    LABEL org.label-schema.vendor=Consensys,
    LABEL org.label-schema.version=22.1.0

   $ docker build \
      --build-arg http_proxy=http://...:8080 \
      --build-arg https_proxy=https://..:8080 \
      -t 'registry_server'/'user_name'/'image_name':'tag' .
         ^^^^^^^^^^^^^^^^^^
         default one if not
         provided


Note: Unless you tell Docker otherwise, it will do as little work as possible when
building an image. It caches the result of each build step of a Dockerfile that
it has executed before and uses the result for each new build.
 *WARN:*
   If a new version of the base image you’re using becomes available that
   conflicts with your app, however, you won’t notice that when running the tests in
   a container using an image that is built upon the older, cached version of the base image.
  *You can force build to look for newer verions of base image "--pull" flag*.
   Because new base images are only available once in a while, it’s not really
   wasteful to use this argument all the time when building images.
   (--no-cache can also be useful)

• Dockerfile : ARG vs ENV:

  ARG : Vaules are consumed/used at build time. Not available at runtime.
  ENV : Values are consumed/used at runtime by final app.
  Both can be combined to provide a default ENV (Runtime) value to apps like:

  ARG buildAppParam1=default_value
  ENV appParam1=$buildAppParam1

• Dockerfile : ENTRYPOINT vs COMMAND

  └ ENTRYPOINT:
    - Defaults to '/bin/sh -c'.
    - It can be changed with ENTRYPOINT or '$ docker run --entrypoint ...'
    - similar to "init" process in Linux. (First command to be executed).
    - It will act, in practice, as the binary executable. Ex:
      $ alias CAT="docker run --entrypoint /bin/cat myImage"
      $ CAT /etc/passwd

  └ COMMAND:
    - Commands are the params passed to ENTRYPOINT.
    - No defaults exits.  The must be indicated as:
      $ docker run -i -t ubuntu command1 command2 ....


  ┌ Dockerfile.base ──────────────────┐
  │ FROM node:7.10-alpine             │
  │                                   │
  │ RUN mkdir /src                    │
  │ WORKDIR /src                      │
  │ COPY package.json /src            ← *1
  │ RUN npm install                   ← *2
  │                                   │
  │ ONBUILD ARG NODE_ENV              ← *4
  │ ONBUILD ENV NODE_ENV $NODE_ENV    │
  │                                   │
  │ CMD [ "npm", "start" ]            │
  └───────────────────────────────────┘
   $ docker build -t node-base  \     <-  STEP 1) Compile base image
     -f Dockerfile.base .

  ┌ Dockerfile.child ─┐
  │ FROM node-base    │
  │                   │
  │ EXPOSE 8000       │
  │ COPY . /src       ← *3
  └───────────────────┘
  $ docker build -t node-child \   <- STEP 2: Compile child image
    -f Dockerfile.child \
      --build-arg NODE_ENV=... .

  $ docker run -p 8000:8000 \        <- STEP 3: Test
   -d node-child *

 *1 Modifications in package.json will force rebuild from there
    triggering a new npm install on next step.
    WARN: If the package.json is put after npm install then no npm
          install will be executed since Docker will not detect any change.

 *2 slow process that doesn't change put before "moving parts" to
    avoid (but after copying any file that indicates that a new npm
    install must be triggered - package.json, package-lock.json, maybe
    "other")

 *3 source code, images, CSS, ...  will change frequently during development.
    Put in last position (top layer in image) so that new modification triggers
    just rebuild of last layer.

 *4 Modify base image adding "ONBUILD" in places that are executed just during
    build in the image extending base image

• Image Build: MultiStage Builds  [[{qa,performance]]
@[https://docs.docker.com/develop/develop-images/multistage-build/]

- Example 1: Go multistage build:

   ┌─ Dockerfile.multistage ───────────────┐ Stage 1:
   │ FROM *golang-1.14:alpine* AS *build*  ← Base Image with compiler, utility libs, ...
   │ ADD . /src                            │ ( Maybe "hundreds" of MBs)
   │ RUN cd /src ; go build  *-o app*      ←  Let's Build final  *executable*
   │                                       │
   │                                       │ Stage 2:
   │ FROM *alpine:1.14*                    ← Clean minimal image (maybe just ~4MB).
   │ WORKDIR /app                          │
   │ COPY*--from=build*  */src/app* /app/  ← Copy  *executable* to final image
   │ ENTRYPOINT ./app                      │
   └───────────────────────────────────────┘

 $*$ docker build . -f Dockerfile.multistage \ *  Build image from multistage Dockerfile
 $*  -t ${IMAGE_TAG}                           *

 $*$ docker run --rm -ti ${IMAGE_TAG}          *  Finally Test it.

- Ex 2: Multi-stage NodeJS Build:
  • PRESETUP:
    - Check with $*$ npm list --depth 3 * duplicated or similar dependencies.
      Try to fix manually in package.json
    - npm audit (See also online services like https://snyk.io,...)
    - Avoid leaking dev ENV.VARs/secrets/...:

        Alt 1:                     Alt 2: (Safer)
      ┌─ .dockerignore ────────┐   ┌─ .dockerignore ────────┐
      │ + node_modules/        │   │ # ignore by default    │
      │ + .npmrc         ← ** *│   │ *                      ← Now it's safe to just:
      │ + .env                 │   │                        │ COPY . /my/workDir
      │ + ....                 │   │ !/docker-entrypoint.sh ← Explicetely mark what we want to copy.
      └────────────────────────┘   │ !/another/file.txt     ←
                                   └────────────────────────┘
  WARN: Sometimes npmrc can contain secrets. [[security.secret_management]]

  ┌─────────────────────────────────────────┐   STAGE 1:
  │ FROM node:14.2.0-alpine3.11 AS build    ← DONT's: Avoid non-deterministic versions (e.g: node, node:14)
  │                                         │ sha256 can also be use to lock to precise version.
  │                                         │ node:lts-alpine@sha256:aFc342a...
  │                                         │
  │ ADD . / app01_src/                      │
  │                                         │ @[https://docs.npmjs.com/cli/v7/commands/npm-ci]
  │ RUN npm ci --only=production            ← ci: == npm install optimized for Continuous Integrations
  │                                         │     Significantly faster when:
  │                                         │     - There is a package-lock.json|npm-shrinkwrap.json file
  │                                         │     - node_modules/ folder is missing|empty.
  │                                         │ --only=production: Skip testing/dev dependencies.
  │                                         │ WARN: Avoid npm install (yarn install)
  │                                         │
  │ FROM node:16.10.0-alpine3.13            ← Use stage1 image. NOTE: In node we still need the
  │                                         │ "big" image, since output artifacts are not self-executables.
  │ RUN mkdir /app                          │
  │ WORKDIR /app                            │ We can still save some space removing un-needed sources.
  │ USER node                               ← Avoid root
  │ COPY *--from=build* --chown=node:node \ ← Forget source, ... Keep only  *"dist/"* executable and
  │      /app01_src/dist  /app              │ (unfortunatelly) keep also the big (tens/hundreds of MBs)
  │ COPY *--from=build* --chown=node:node \ │ node_modules/ folder, still needed in production.
  │      /app01_src/node_modules \          │
  │      /app/node_modules                  │
  │                                         │
  │ ENV NODE_ENV production                 ← Some libs enable optimization  only when PRODUCTION=true
  │                                         │
  │                                         │
  │ ENTRYPOINT ["node", "/app/dist/cli.js"] ← TODO: Check "dumb-init" alternative. *1
  └─────────────────────────────────────────┘

  *1 NOTE: to allow nodeJS handle OS signals add some code like:
     async function handleSigInt(signal) {
         await fastify.close()
         process.exit()
     }
     process.on('SIGINT', handleSigInt)
     (Check alternatives in other languages)

 [[}]]

[[}]]

● https://github.com/googlecontainertools/distroless  [[{containerization.image.build,qa]]
- "distroless" images contain only your application and its runtime dependencies.
  (not package managers, shells,...)
  notice: in kubernetes we can also use init containers with non-light images
          containing all set of tools (sed, grep,...) for pre-setup, avoiding
          any need to include in the final image.

  stable:                      experimental (2019-06)
  gcr.io/distroless/static     gcr.io/distroless/python2.7
  gcr.io/distroless/base       gcr.io/distroless/python3
  gcr.io/distroless/java       gcr.io/distroless/nodejs
  gcr.io/distroless/cc         gcr.io/distroless/java/jetty
                               gcr.io/distroless/dotnet

  ex java multi-stage dockerfile:
  @[https://github.com/googlecontainertools/distroless/blob/master/examples/java/dockerfile]

    from openjdk:11-jdk-slim  as  build-env   <- stage 1) compile using "bloated jdk"
    add . /app/examples
    workdir /app
    run javac examples/*.java
    run jar cfe main.jar \
        examples.hellojava examples/*.class

    from gcr.io/distroless/java:11            <- stage 2) copy compiled jars to distroless
    copy --from= *build-env* /app /app
    workdir /app
    cmd ["main.jar"]
[[}]]

● CNCF buildpacks.io: [[{containerization.image.build,01_pm.low_code,dev_language.java]]
• build oci image directly from JAVA source source (Skipping Dockerfile/docker build)
• highly modular and customizable.
• used, among others, by spring (spring boot 2.3+) with  gradle and maven support.
  e.g.: springboot gradle integration:
  bootbuildimage {
    imagename = "${docker.username}/${project.name}:${project.version}"
    environment = ["bp_jvm_version" : "11.*"]
  }
  • promotes best practices in terms of security;
  • defining cpu and memory limits for jvm containers is critical
    because they will be used to size properly items like jvm thread pools,
    heap memory and  non-heap  memory. tunning manually is challenging,
    fortunately, if using paketo implementation of cloud native buildpacks
    (included for example in spring boot), java memory calculator is included
    automatically and the component  will configure jvm memory based on the
    resource limits assigned to the container. otherwise, results are
    unpredictable.
[[}]]

● rootless buildah: [[{containerization.image.build]]
@[https://opensource.com/article/19/3/tips-tricks-rootless-buildah]
- building containers in unprivileged environments
- library+tool for building oci images.
- complementary to podman.
- build speed: [[[01_PM.TODO]]
  @[https://www.redhat.com/sysadmin/speeding-container-buildah]
  this article will address a second problem with build speed when using dnf/yum
   commands inside containers. note that in this article i will use the name dnf
   (which is the upstream name) instead of what some downstreams use (yum) these
   comments apply to both dnf and yum.
[[}]]

● appsody (prebuilt images)  [[{containerization.image.build,dev_stack.kubernetes,01_PM.low_code,01_PM.todo]]
@[https://appsody.dev/docs]
- pre-configured application stacks for rapid development
  of quality microservice-based applications.               [[qa]]

- Stacks include language runtimes, frameworks, additional libraries and tools
  NEEDED FOR LOCAL DEVELOPMENT, providing consistency and best practices.

- it consists of:
  - base-container-image:
    - local development.
    - it defines the environment and specifies the stack behavior
      during the development lifecycle of the application.

  - project templates:
    - starting point ('hello world')
    - they can be customized/shared.

- stack layout example, my-stack:
  my-stack
  ├── readme.md               # describes stack and how to use it
  ├── stack.yaml              # different attributes and which template
  ├── image/                  # to use by default
  |   ├── config/
  |   |   └── app-deploy.yaml # deploy config using appsody operator
  |   ├── project/
  |   |   ├── php/java/...stack artifacts
  |   |   └── dockerfile      # final   (run) image ("appsody build")
  │   ├── dockerfile-stack    # initial (dev) image and env.vars
  |   └── license             # for local dev.cycle. it is independent
  └── templates/              # of dockerfile
      ├── my-template-1/
      |       └── "hello world"
      └── my-template-2/
              └── "complex application"

- GENERATED FILES:
  -*".appsody-config.yaml"*. generated by $*$ appsody init*
    it specifies the stack image used and can be overridden
    for testing purposes to point to a locally built stack.

- STABILITY LEVELS:
  - experimental ("proof of concept")
    - support  appsody init|run|build

  - incubator : not production-ready.
    - active contributions and reviews by maintainers
    - support  appsody init|run|build|test|deploy
    - limitations described in readme.md

  - stable : production-ready.
    - support all appsody cli commands
    - pass appsody stack 'validate' and 'integration' tests
      on all three operating systems that are supported by appsody
      without errors.
      example:
      - stack must not bind mount individual files as it is
        not supported on windows.
      - specify the minimum appsody, docker, and buildah versions
        required in the stack.yaml
      - support appsody build command with buildah
      - prevent creation of local files that cannot be removed
        (i.e. files owned by root or other users)
      - specify explicit versions for all required docker images
      - do not introduce any version changes to the content
        provided by the parent container images
        (no yum upgrade, apt-get dist-upgrade, npm audit fix).
         - if package contained in the parent image is out of date,
           contact its maintainers or update it individually.
      - tag stack with major version (at least 1.0.0)
      - follow docker best practices, including:
        - minimise the size of production images
        - use the official base images
        - images must not have any major security vulnerabilities
        - containers must be run by non-root users
      - include a detailed readme.md, documenting:
        - short description
        - prerequisites/setup required
        - how to access any endpoints provided
        - how users with existing projects can migrate to
          using the stack
        - how users can include additional dependencies
          needed by their application

- OFFICIAL APPSODY REPOSITORIES:
  https://github.com/appsody/stacks/releases/latest/download/stable-index.yaml
  https://github.com/appsody/stacks/releases/latest/download/incubator-index.yaml
  https://github.com/appsody/stacks/releases/latest/download/experimental-index.yaml

- by default, appsody comes with the incubator and experimental repositories
  ( *warn*: not stable by default). repositories can be added by running :
  $ appsody repo
[[}]]


● Podman (IBM/RedHat)  [[{containerization.podman,02_doc_type.comparative,01_PM.TODO]]
- No system daemon required.
- No daemon required.
- rootless containers.                                         [[containerization.security]]
- Podman is set to be the default container engine for the single-node
  use case in Red Hat Enterprise Linux 8.
  (CRI-O for OpenShift clusters)

- easy to use and intuitive.
  - Most users can simply alias Docker to Podman (alias docker=podman)

- '$ podman generate kube' creates a Pod that can then be exported as Kubernetes-compatible YAML.

- enables users to run different containers in different user namespaces


- Runs at native Linux speeds.
  (no daemon getting in the way of handling client/server requests)


-  OCI compliant Container Runtime (runc, crun, runv, etc)
  to interface with the OS.

- Podman  libpod library manages container ecosystem:
  - pods.
  - containers.
  - container images (pulling, tagging, ...)
  - container volumes.

- Podman Introduction:

  $ podman search busybox
  INDEX       NAME                          DESCRIPTION             STARS  OFFICIAL AUTOMATED
  docker.io   docker.io/library/busybox     Busybox base image.     1882   [OK]
  docker.io   docker.io/radial/busyboxplus  Full-chain, Internet... 30     [OK]
  ...
  $ podman run -it docker.io/library/busybox
  / #

  $ URL="https://raw.githubusercontent.com/nginxinc/docker-nginx"
  $ URL="${URL}/594ce7a8bc26c85af88495ac94d5cd0096b306f7/       "
  $ URL="${URL}/mainline/buster/Dockerfile                      "
  $ podman build -t nginx ${URL}                                   ← build Nginx web server using
                    └─┬─┘                                            official Nginx Dockerfile
                      └────────┐
                             ┌─┴─┐
  $ podman run -d -p 8080:80 nginx                                 ← run new image from local cache
                     └─┬─┘└┘
                       │   ^Port Declared @ Dockerfile
                 Effective
                 (Real)port


- To make it public publish to any other Register compatible with the
 *Open Containers Initiative (OCI) format*. The options are:
  - Private Register:
  - Public  Register:
    - quay.io
    - docker.io

$*$ podman login quay.io                            * ← Login into quay.io
$*$ podman tag localhost/nginx quay.io/${USER}/nginx* ← re-tag the image
$*$ podman push quay.io/${USER}/nginx               * ← push the image
→ Getting image source signatures
→ Copying blob 38c40d6c2c85 done
→ ..
→ Writing manifest to image destination
→ Copying config 7f3589c0b8 done
→ Writing manifest to image destination
→ Storing signatures

$*$ podman inspect quay.io/${USER}/nginx            * ← Inspect image
→ [
→     {
→         "Id": "7f3589c0b8849a9e1ff52ceb0fcea2390e2731db9d1a7358c2f5fad216a48263",
→         "Digest": "sha256:7822b5ba4c2eaabdd0ff3812277cfafa8a25527d1e234be028ed381a43ad5498",
→         "RepoTags": [
→             "quay.io/USERNAME/nginx:latest",
→             ...
→         ]
→     }
→ ]


• Podman commands:
@[https://podman.readthedocs.io/en/latest/Commands.html]
 *Image Management:*
  build        Build an image using instructions from Containerfiles
  commit       Create new image based on the changed container
  history      Show history of a specified image
  image
  └ build   Build an image using instructions from Containerfiles
    exists  Check if an image exists in local storage
    history Show history of a specified image
    prune   Remove unused images
    rm      Removes one or more images from local storage
    sign    Sign an image
    tag     Add an additional name to a local image
    tree    Prints layer hierarchy of an image in a tree format
    trust   Manage container image trust policy

  images       List images in local storage  ( == image list)
  inspect      Display the configuration of a container or image ( == image inspect)
  pull         Pull an image from a registry  (== image pull)
  push         Push an image to a specified destination (== image push)
  rmi          Removes one or more images from local storage
  search       Search registry for image
  tag          Add an additional name to a local image

 *Image Archive/Backups:*
  import       Import a tarball to create a filesystem image (== image import)
  load         Load an image from container archive ( == image load)
  save         Save image to an archive ( == image save)

 *Pod Control:*
  attach       Attach to a running container ( == container attach)
  containers Management
  └ cleanup    Cleanup network and mountpoints of one or more containers
    commit     Create new image based on the changed container
    exists     Check if a container exists in local storage
    inspect    Display the configuration of a container or image
    list       List containers
    prune      Remove all stopped containers
    runlabel   Execute the command described by an image label

 *Pod Checkpoint/Live Migration:*
  container checkpoint Checkpoints one or more containers
  container restore    Restores one or more containers from a checkpoint

  $*$ podman container checkpoint $container_id\ *← Checkpoint and prepare*migration archive*
  $*    -e /tmp/checkpoint.tar.gz                *
  $*$ podman container restore \                 *← Restore from archive at new server
  $*  -i /tmp/checkpoint.tar.gz                  *

  create       Create but do not start a container ( == container create)
  events       Show podman events
  exec         Run a process in a running container ( == container exec)
  healthcheck  Manage Healthcheck
  info         Display podman system information
  init         Initialize one or more containers ( == container init)
  kill         Kill one or more running containers with a specific signal ( == container kill)
  login        Login to a container registry
  logout       Logout of a container registry
  logs         Fetch the logs of a container ( == container logs)
  network      Manage Networks
  pause        Pause all the processes in one or more containers ( == container pause)
  play         Play a pod
  pod          Manage pods
  port         List port mappings or a specific mapping for the container ( == container port)
  ps           List containers
  restart      Restart one or more containers ( == container restart)
  rm           Remove one or more containers ( == container rm)
  run          Run a command in a new container ( == container run)
  start        Start one or more containers ( == container start)
  stats        Display a live stream of container resource usage statistics (== container stats)
  stop         Stop one or more containers ( == container stop)
  system       Manage podman
  top          Display the running processes of a container ( == container top)
  unpause      Unpause the processes in one or more containers ( == container unpause)
  unshare      Run a command in a modified user namespace
  version      Display the Podman Version Information
  volume       Manage volumes
  wait         Block on one or more containers ( == container wait)

 *Pod Control: File system*
  cp           Copy files/folders container ←→ filesystem (== container cp)
  diff         Inspect changes on container’s file systems ( == container diff)
  export       Export container’s filesystem contents as a tar archive ( ==  container export )
  mount        Mount a working container’s root filesystem  ( == container mount)
  umount       Unmounts working container’s root filesystem ( == container mount)


 *Pod Integration*
  generate     Generated structured data
    kube       kube Generate Kubernetes pod YAML from a container or pod
    systemd    systemd Generate a  *SystemD unit file* for a Podman container


• SystemD Integration:
https://www.redhat.com/sysadmin/improved-systemd-podman
- auto-updates help to make managing containers even more straightforward.

- SystemD is used in Linux to  managing services (background long-running jobs listening for client requests) and their dependencies.

 *Podman running SystemD inside a container*
  └ /run               ← tmpfs
    /run/lock          ← tmpfs
    /tmp               ← tmpfs
    /var/log/journald  ← tmpfs
    /sys/fs/cgroup      (configuration)(depends also on system running cgroup V1/V2 mode).
    └───────┬───────┘
     Podman automatically mounts next file-systems in the container when:
     - entry point of the container is either */usr/sbin/init or /usr/sbin/systemd*
     -*--systemd=always*flag is used

 *Podman running inside SystemD services*
  - SystemD needs to know which processes are part of a service so it
    can manage them, track their health, and properly handle dependencies.
  - This is problematic in Docker  (according to RedHat rival) due to the
    server-client architecture of Docker:
    - It's practically impossible to track container processes, and
      pull-requests to improve the situation have been rejected.
    - Podman implements a more traditional architecture by forking processes:
      - Each container is a descendant process of Podman.
      - Features like sd-notify and socket activation make this integration
        even more important.
        - sd-notify service manager allows a service to notify SystemD that
          the process is ready to receive connections
        - socket activation permits SystemD to launch the containerized process
          only when a packet arrives from a monitored socket.

    - Compatible with audit subsystem (track records user actions).
      - the forking architecture allows systemd to track processes in a
        container and hence opens the door for seamless integration of
        Podman and systemd.

  $*$ podman generate systemd --new $container*  ← Auto-generate containerized systemd units:
                              └─┬─┘
                              Ohterwise it will be tied to creating host


     - Pods are also supported in Podman 2.0
       Container units that are part of a pod can now be restarted.
       especially helpful for auto-updates.

 *Podman auto-update  (1.9+)*
  - To use auto-updates:
    - containers must be created with :
      --label "io.containers.autoupdate=image"

    - run in a SystemD unit generated by
      $ podman generate systemd --new.

  $ podman auto-update    ← Podman will first looks up running containers with the
                            "io.containers.autoupdate" label set to "image" and then
                            query the container registry for new images.
                           *If that's the case Podman restarts the corresponding *
                           *SystemD unit to stop the old container and create a  *
                           *new one with the modified image.                     *

   (still marked as experimental while  collecting user feedback)

[[}]]

● TODO/un-ordered/classify [[{containerization,01_PM.TODO]]
- Kompose: docker-compose to Kubernetes convert tool.
  - https://github.com/kubernetes/kompose
  - https://github.com/tldr-pages/tldr/blob/master/pages/common/kompose.md

- CONTAINERIZATION TROUBLESHOOTING 101: [[{containerization.troubleshooting]]

  - PROBLEM: /var/lib/docker/devicemapper/devicemapper/data consumes too much space
    $ sudo du -sch /var/lib/docker/devicemapper/devicemapper/data
    14g     /var/lib/docker/devicemapper/devicemapper/data

    ($ docker logs 'container' knocks down the host server when output is processed)

    SOLUTION 1: Possible logs are not configured to be rotated creating "huge" file.
    Fix:
    1) "create/modify" /etc/docker/daemon.json  to look like:
       {
         "log-driver": "json-file",
         "log-opts": {
           "max-size": "10m",
           "max-file": "3"
         }
       }
    2) $ sudo systemctl restart docker.service

    SOLUTION 2: NOTE: maybe ulimit can fix it at global (Linux OS) scope. [[{01_PM.TODO}]]

  - PROBLEM: dns works on host, fails during builds or in running containers:
    └ SOLUTION 1) ALT 1:
      1) Add next lines to /etc/docker/daemon.json:
          {
        +   "dns": ["8.8.8.8","4.4.4.4"],
        +   "dns-search": ["companydomain",...]
          }
      2) $ sudo systemctl restart docker.service
      3) $ journalctl -u docker.service --since "1m ago"  # Check logs for related errors.

    └ SOLUTION 1) ALT 2:
      1) Edit     /lib/systemd/system/docker.service
             (/usr/lib/systemd/system/docker.service)
         to look somthing like:
         ...
      +  ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock \
            --dns 8.8.8.8 --dns 8.8.4.4  --dns-search default.svc.cluster.local \
            --dns-search svc.cluster.local --dns-opt ndots:2 \
            --dns-opt timeout:2 --dns-opt attempts:2

       NOTE/WARN!!!: This configuration can be overriden by values in:
       /etc/systemd/system/docker.service.d/*.conf

      2) $ sudo systemctl daemon-reload
      3) $ sudo systemctl restart docker.service
      4) $ journalctl -u docker.service --since "1m ago"  # Check logs for related errors.

    SOLUTION 2)  try to launch with --network host flag. ex.:
      ...
      docker_opts="${docker_opts} *--network host*"
      script="wget https://repo.maven.apache.org/maven2" # ← dns can fail with bridge
      echo "${mvn_script}" | docker run ${docker_opts} ${script}

  - INSPECTING LINUX NAMESPACES OF RUNNING CONTAINER:
    use nsenter ( util-linux package) to "enter" into the
    container (network, filesystem, ipc, ...) namespace.

    $ cat enternetworknamespace.sh
    #!/bin/bash

    # ref: man nsenter
    # run shell with network namespace of container.
    # allows to use ping, ss/netstat, wget, trace,.. in
    # in contect of the container.
    # useful to check network setup is the appropiate one.
    cont_pid=$( sudo docker inspect -f '{{.state.pid}}' $1 )
    shift 1
    sudo*nsenter*-t ${cont_pid}*- *
                                ^^
                           use network namespace of container

    ex ussage:
    $ ./enternetworknamespace.sh mywebcontainer01
    $ netstat -ntlp
    active internet connections (only servers)
    proto recv-q send-q local address           foreign address         state
    tcp        0      0 0.0.0.0:80              0.0.0.0:*               listen

    * netstat installed on host (vs container).
[[}]]

● sysdig tools:  [[{containerization.monitoring,containerization.security,troubleshooting,01_PM.todo]]
container-focused linux troubleshooting and monitoring tool.

once sysdig is installed as a process (or container) on the server,
it sees every process, every network action, and every file action
on the host. you can use sysdig "live" or view any amount of historical
data via a system capture file.

example: take a look at the total cpu usage of each running container:
   $ sudo sysdig -c topcontainers\_cpu
   | cpu% container.name
   | ----------------------------------------------------
   | 80.10% postgres
   | 0.14% httpd
   | ...
   |

example: capture historical data:
   $ sudo sysdig -w historical.scap

example: "zoom into a client":
   $ sudo sysdig -pc -c topprocs\_cpu container. name=client
   | cpu% process container.name
   | ----------------------------------------------
   | 02.69% bash client
   | 31.04%curl client
   | 0.74% sleep client
[[}]]

• cadvisor+prometheus+grafana: [[{containerization.monitoring]]
@[https://blog.couchbase.com/monitoring-docker-containers-docker-stats-cadvisor-universal-control-plane/]
@[https://dzone.com/refcardz/intro-to-docker-monitoring?chapter=6]
@[https://github.com/google/cadvisor/blob/master/docs/running.md#standalone]
[[}]]

• live restore:
 @[https://docs.docker.com/config/containers/live-restore/]
 keep containers alive during daemon downtime

• weave: [[{ci/cd.gitops,containerization.monitoring,01_pm.low_code,gui,01_PM.todo]]
@[https://github.com/weaveworks/weave]
- weaveworks company: """ delivers the most productive way for developers
  to connect, observe and control docker containers. """"

- weave net repository: over 8 million downloads to date.
  - It enables to get started with docker clusters and portable
    apps in a fraction of the time required by other solutions.
  - quickly, easily, and securely network and cluster containers
    across any environment. whether on premises, in the cloud, or hybrid,
    there’s no code or configuration.
  - build an ‘invisible infrastructure’
  - powerful cloud native networking toolkit. it creates a virtual network
    that connects docker containers across multiple hosts and enables their
    automatic discovery. set up subsystems and sub-projects that provide
    dns, ipam, a distributed virtual firewall and more.

- weave scope:
  - understand your application quickly by seeing it in a real time
    interactive display. pick open source or cloud hosted options.
  - zero configuration or integration required — just launch and go.
  - automatically detects processes, containers, hosts.
    no kernel modules, agents, special libraries or coding.
  - seamless integration with docker, kubernetes, dcos and aws ecs.

- cortex: horizontally scalable, highly available, multi-tenant,
  long term storage for prometheus.

- flux:
  - flux is the operator that  *makes gitops happen in your cluster*.
    it ensures that the cluster config matches the one in git and
    automates your deployments.
  - continuous delivery of container images, using version control
    for each step to ensure deployment is reproducible,
    auditable and revertible. deploy code as fast as your team creates
    it, confident that you can easily revert if required.

    learn more about gitops.
  @[https://www.weave.works/technologies/gitops/]
[[}]]

• clair  [[{containerization.security,qa,01_PM.todo]]
@[https://coreos.com/clair/docs/latest/]
open source project for the static analysis of vulnerabilities in
appc and docker containers.

vulnerability data is continuously imported from a known set of sources and
correlated with the indexed contents of container images in order to produce
lists of vulnerabilities that threaten a container. when vulnerability data
changes upstream, the previous state and new state of the vulnerability along
with the images they affect can be sent via webhook to a configured endpoint.
all major components can be customized programmatically at compile-time
without forking the project.
[[}]]

• skopeo:  [[{containerization.image.registry,containerization.troubleshooting]]
@[https://github.com/containers/skopeo]
@[https://www.redhat.com/en/blog/skopeo-10-released]
- command line utility for moving/copying container images between different types
  of container storages. (docker.io, quay.io, internal container registry
  local storage repository or even directly into a docker daemon).
- it does not require root permissions (for most of its operations)
  or even a docker daemon.
- compatible with oci images (standards) and original docker v2 images.
[[}]]

• security tunning:  [[{containerization.security,101]]
@[https://opensource.com/business/15/3/docker-security-tuning]
[[}]]

• lazydocker:  [[{qa.UX,]]
  @[https://github.com/jesseduffield/lazydocker]
  a simple terminal ui for both docker and docker-compose, written in
  go with the gocui library.
[[}]]

• convoy (volume driver for backups) : [[{containerization.storage.host,containerization,security.backups]]
@[https://rancher.com/introducing-convoy-a-docker-volume-driver-for-backup-and-recovery-of-persistent-data/]
introducing convoy a docker storage driver for backup and recovery of volumes
[[}]]

• Setup Insecure HTTP registry  [[{containerization.image.registry,containerization.security]]
@[https://www.projectatomic.io/blog/2018/05/podman-tls/]

   /etc/containers/registries.conf.

   # This is a system-wide configuration file used to
   # keep track of registries for various container backends.
   # It adheres to TOML format and does not support recursive
   # lists of registries.

   [registries.search]
   registries = ['docker.io', 'registry.fedoraproject.org', 'registry.access.redhat.com']

   # If you need to access insecure registries, add the registry's fully-qualified name.
   # An insecure registry is one that does not have a valid SSL certificate or only does HTTP.
   [registries.insecure]
  *registries = ['localhost:5000']*
[[}]]

• Protecting against Doki malware: [[{containerization.security,qa,01_PM.TODO]]
  https://containerjournal.com/topics/container-security/protecting-containers-against-doki-malware/

• 2+Millions images with Critical Sec.Holes
https://www.infoq.com/news/2020/12/dockerhub-image-vulnerabilities/

• OpenSCAP: Scanning Vulnerabilities: [[{containerization.security.openscap}]]
- Scanning Containers for Vulnerabilities on RHEL 8.2 With OpenSCAP and Podman:
@[https://www.youtube.com/watch?v=nQmIcK1vvYc]
[[}]]



[[containerization,01_PM.TODO}]]

[[}]]

[[containerization}]]
##################################
● Networking Summary for DevOps: # [[{101,01_PM.low_code,networking.load_balancer,web_balancer,01_PM.WiP]]
##################################
• HTTP balanced proxy Quick Setup with HAProxy
  REF: @[https://github.com/AKSarav/haproxy-nodejs-redis/blob/master/haproxy/]
  ┌──haproxy/haproxy.cfg ────────────  ┌ haproxy/Dockerfile ──────────────────────────
  │ global                             │ FROM haproxy
  │   daemon                           │ COPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg
  │   maxconn 256                      └──────────────────────────────────────────────
  │
  │ defaults
  │   mode http
  │   timeout connect 5000ms
  │   timeout client 50000ms
  │   timeout server 50000ms
  │
  │ frontend http─in
  │   bind *:80                             ← Listen on port 80 on all interfaces
  │   default_backend servers
  │
  │ backend servers                         ←  Forward to single backend "servers"
  │   server server1 host01:8081 maxconn 32 ←  composed of (single server) "server1"
  │                                            at host01:8081
  └─────────────────────────────────────────
• Reverse Proxy  [[01_PM.TODO]]
• Forward Proxy  [[01_PM.TODO]]

• CharlesProxy: Monitor TLS/HTTPS traffic: [[{network.TLS,troubleshooting,01_PM.TODO]]
@[https://www.charlesproxy.com/]
  HTTP proxy / HTTP monitor / Reverse Proxy enabling developers to view HTTP+SSL/HTTPS
  traffic between loal machine and Internet, including requests, responses and HTTP headers
  (which contain the cookies and caching information).
  [[}]]

• ┌ DNS Records ────────────────────────────────┐
  │ A       root domain name IP address         │
  │         Ex: mydomain.com → 1.2.3.4          │
  │         Not recomended for changing IPs     │
  ├─────────────────────────────────────────────┤
  │ CNAME   maps name2 → name1                  │
  │         Ex: int.mydomain.com → mydomain.com │
  ├─────────────────────────────────────────────┤
  │ Alias   Amazon Route 53 virtual record      │
  │         to map AWS resources like ELBs,     │
  │         CloudFront, S3 buckets, ...         │
  ├─────────────────────────────────────────────┤
  │ MX      mail server name → IP address       │
  │         Ex: smtp.mydomain.com → 1.2.3.4     │
  ├─────────────────────────────────────────────┤
  │ AAAA    A record for IPv6 addresses         │
  └─────────────────────────────────────────────┘

• SERVICE MESH EVOLUTION  [[{01_PM.low_code]]

  Summary extracted from @[https://isovalent.com/blog/post/2021-12-08-ebpf-servicemesh]
                         @[https://www.infoq.com/news/2022/01/ebpf-wasm-service-mesh/]

  Service Mesh: Takes care of (netwok)distributed concerns (visibility, security, balancing,
                service discovery, ...)

  1st GENERATION. Each app     2nd Generation. A common      3rd Generation. Sidecar
  links against a library.     sidecar is used.              functionality moved to
                                                             linux kernel usinb eBFP
  ┌─ App1 ────┐ ┌─ App2 ────┐  ┌─ App1 ────┐ ┌─ App2 ────┐
  │  ┌───────┐│ │  ┌───────┐│  │           │ │           │
  │  │Service││ │  │Service││  └───────────┘ └───────────┘   ┌─ App1 ────┐ ┌─ App2 ────┐
  │  │Mesh   ││ │  │Mesh   ││  ┌───────────┐ ┌───────────┐   │           │ │           │
  │  │Library││ │  │Library││  │ServiceMesh│ │ServiceMesh│   └───────────┘ └───────────┘
  │  └───────┘│ │  └───────┘│  │SideCar    │ │SideCar    │   ┌─ Kernel ────────────────┐
  └───────────┘ └───────────┘  └───────────┘ └───────────┘   │ ┌─ eBFP Service Mesh ┐  │
  ┌─ Kernel ────────────────┐  ┌─ Kernel ────────────────┐   │ └────────────────────┘  │
  │       ┌─ TCP/IP ─┐      │  │       ┌─ TCP/IP ─┐      │   │       ┌─ TCP/IP ─┐      │
  │       └──────────┘      │  │       └──────────┘      │   │       └──────────┘      │
  │       ┌─ Network─┐      │  │       ┌─ Network─┐      │   │       ┌─ Network─┐      │
  │       └──────────┘      │  │       └──────────┘      │   │       └──────────┘      │
  └─────────────────────────┘  └─────────────────────────┘   └─────────────────────────┘
                                 Envoy, Linkerd, Nginx,...     Cilium
                                 or kube-proxy

   App1 ←→ Kernel TCP/IP        App1 ←→ SideCar1              App1 ←→ Kernel eBFP
   Kernel TCP/IP ←→ App2        SideCar1 ←→ Kernel TCP/IP     Kernel eBFP ←→ App2
                                Kernel TCP/IP ←→ Sidecar2
                                Sidecar2 ←→ App2
   [[}]]

• nginx.conf summary [[{networking.nginx,web_balancer,01_PM.WiP]] #[nginx_conf_summary]
REF: @[https://raazkumar.com/tutorials/nginx/nginx-conf/]

• nginx ==   fast HTTP reverse proxy
           + reliable load balancer
           + high performance caching server
           + full-fledged web platform

•*nginx.conf building blocks*
  - worker process    : should be equal to number cores of the server (or auto)
  - worker connection : 1024 (per thread. nginx doesn't block)

  - rate limiting     : prevent brute force attacks.
  - proxy buffers     : (when used as proxy server)limits how much data to store as cache
                         gzip /brotil or compression
  - upload file size  : it should match php max upload size and nginx client max body size.
  - timeouts          : php to nginx communication time.
  - log rotation      : error log useful to know the errors and monitor resources
  - fastcgi cache     : very important to boost the performance for static sties.
  - SSL Configuration : there are default setting available with nginx itself
                        (also see ssl performance tuning).

•*Example nginx.conf:*
  user www-data;
  load_module*modules/my_favourite_module.so;
  pid /run/nginx.pid;
                                                   | Alternative global config for
                                                   | [4 cores, 8 threads, 32GB RAM]
                                                   | handling  50000request/sec
                                                   |
  worker_processes auto;                           | worker_processes 8;
                                                   | worker_priority -15;
  include /etc/nginx/modules-enabled/*.conf;       |
  worker_rlimit_nofile 100000;                     | worker_rlimit_nofile 400000;
                                                   | timer_resolution 10000ms;
                                                   |
  events {                                         | events {
    worker_connections 1024;                       |     worker_connections 20000;
    multi_accept on;                               |     use epoll;
  }                                                |     multi_accept on;
                                                   | }

 *http {               ←  global config*
    index index.php index.html index.htm;
   *# Basic Settings*

    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    sendfile_max_chunk 512;
    keepalive_timeout 300;
    keepalive_requests 100000;
    types_hash_max_size 2048;
    server_tokens off;

    server_names_hash_bucket_size 128;
    # server_name_in_redirect off;

    include /etc/nginx/mime.types; ← ········· types {
    default_type application/octet-stream;       text/html              html htm shtml;
    ##                                           application/javascript js;
    # SSL Settings                               ...
    ##                                         }

    #ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
    #ssl_prefer_server_ciphers on;
    #rate limit zone

    limit_req_zone $binary_remote_addr zone=one:10m rate=3r/m;
    #buffers

    client_body_buffer_size 128k;
    client_max_body_size 10m;
    client_header_buffer_size 32k;
    large_client_header_buffers 16 256k;
    output_buffers 1 32k;
    postpone_output 1460;
    #Porxy buffers
    proxy_buffer_size 256k;
    proxy_buffers 8 128k;
    proxy_busy_buffers_size 256k;
    proxy_max_temp_file_size 2048m;
    proxy_temp_file_write_size 2048m;

    ## fast cgi PHP
    fastcgi_buffers 8 16k;
    fastcgi_buffer_size 32k;
    fastcgi_connect_timeout 300;
    fastcgi_send_timeout 300;
    fastcgi_read_timeout 300;
    #static caching css/js/img

    open_file_cache max=10000 inactive=5m;
    open_file_cache_valid 2m;
    open_file_cache_min_uses 1;
    open_file_cache_errors on;
    #timeouts

    client_header_timeout 3m;
    client_body_timeout 3m;
    send_timeout 3m;

    # Logging Settings

    log_format main_ext ‘$remote_addr – $remote_user [$time_local] “$request” ‘
                        ‘$status $body_bytes_sent “$http_referer” ‘
                        ‘”$http_user_agent” “$http_x_forwarded_for” ‘
                        ‘”$host” sn=”$server_name” ‘
                        ‘rt=$request_time ‘
                        ‘ua=”$upstream_addr” us=”$upstream_status” ‘
                        ‘ut=”$upstream_response_time” ul=”$upstream_response_length” ‘
                        ‘cs=$upstream_cache_status’ ;

    access_log /dev/stdout main_ext;
    error_log /var/log/nginx/error.log warn;   Read more on nginx error log⅋common errors


    ##
    # Gzip Settings #brotil
    ##

    gzip on;
    gzip_disable “msie6”;

    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_buffers 16 8k;
    gzip_http_version 1.1;
    gzip_types text/plain text/css application/json application/javascript ↩
               text/xml application/xml application/xml+rss text/javascript ↩
               application/x-font-ttf font/opentype image/svg+xml image/x-icon;
    ##
    # Virtual Host Configs
    ##
    include /etc/nginx/conf.d/*.conf;
    include /etc/nginx/sites-enabled/*;
  }

 *server {             ← Domain level*
    listen 0.0.0.0:443 rcvbuf=64000 sndbuf=120000 backlog=20000 ssl http2;
    server_name example.com www.example.com;
    keepalive_timeout         60;
    ssl                       on;
    ssl_protocols             TLSv1.2 TLSv1.1 TLSv1;
    ssl_ciphers               'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:↩
                               DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:↩
                               !aNULL:!MD5:!DSS:!RC4';
    ssl_prefer_server_ciphers on;
    ssl_session_cache         shared:TLSSL:30m;
    ssl_session_timeout       10m;
    ssl_buffer_size           32k;
    ssl_certificate           /etc/letsencrypt/live/example.com/fullchain.pem;
    ssl_certificate_key       /etc/letsencrypt/live/example.com/privkey.pem;
    ssl_dhparam           /etc/ssl/certs/dhparam.pem;
    more_set_headers          "X-Secure-Connection: true";
    add_header                Strict-Transport-Security max-age=315360000;
    root       /var/www;

 *  location {         ← Directory level*
       root /var/www;
       index index.php index.html;
    }

 *  location ~ .php$ {*
      fastcgi_keep_conn on;
      fastcgi_pass   unix:/run/php5.6-fpm.sock;
      fastcgi_index  index.php;
      fastcgi_param  SCRIPT_FILENAME /var/www$fastcgi_script_name;
      include fastcgi_params;
      fastcgi_intercept_errors off;
      fastcgi_buffer_size 32k;
      fastcgi_buffers 32 32k;
      fastcgi_connect_timeout 5;
    }

 *  location ~* ^.+.(jpg|jpeg|gif|png|svg|ico|css|less|xml|html?|swf|js|ttf)$ {*
        root /var/www;
        expires 10y;
   }

  }

- /etc/nginx/conf.d/*: user defined config files

  See also:
  @[https://github.com/trimstray/nginx-admins-handbook]
  @[https://github.com/tldr-devops/nginx-common-configuration]
[[networking.nginx}]]

[[}]]

##############################
● Monitoring for DevOps 101  # [[{monitoring.101,]]
##############################
 • Infra vs App Monitoring: [[{02_doc_has.comparative]]
 └*Infrastructure Monitoring:*
   · Prometheus + Grafana (Opinionated)
     Prometheus periodically pulls multidimensional data from different apps/components.
     Grafana allows to visualize Prometheus data in custom dashboards.
     (Alternatives include Monit, Datadog, Nagios, Zabbix, ...)

 └*Application Monitoring*:
   · OpenTelemetry: replaces OpenTracing and OpenCensus.
     Cloud Native Foundation projects.
     It also serves as ¿front-end? for Jaeger and others.
   · Jaeger, New Relic: (Very opinionated)
   (Other alternatives include AppDynamics, Instana, ...)

 └ Log Management: (Opinionated)
   · Elastic Stack
     (Alternative include Graylog, Splunk, Papertrail, ...)
     Elastic search has evolved throught the years to become a
     full analytical platform.

   MUCH MORE DETAILED INFORMATION IS AVAILABLE AT:
 @[../Architecture/architecture_map.html]
  [[}]]
[[}]]

● Vagrant (VMs as code):[[{vagrant,01_PM.low_code,101,troubleshooting]]
  - External Links:
    - @[https://www.vagrantup.com/docs/index.html]
    - @[https://www.vagrantup.com/docs/cli/]                         CLI Reference
    - @[https://www.vagrantup.com/intro/getting-started/index.html]
    - @[https://www.vagrantup.com/docs/providers/]                   Providers list
    - @[https://app.vagrantup.com/boxes/search]                     *Boxes Search*
    - @[https://www.vagrantup.com/docs/networking/]                  Networking


 - * Vagrant Boxes *
   #################
   - Pre-built VMs avoiding slow and tedious process.
   - Can be used as base image to (quickly) clone an existing virtual machine.
   - Specifying the box to use for your Vagrant environment is always the first
     step after creating a new Vagrantfile.


 - * Vagrant Share *
   #################
@[https://www.vagrantup.com/intro/getting-started/share.html]
@[https://www.vagrantup.com/docs/share]
   - $ vagrant share   <- Quick how-to, share a Vagrant environment with anyone in the World.

   - three primary modes or features (not mutually exclusive, can be combined):
     ·*shareable URL* pointing to Vagrant environment.
      *URL "consumer" does not need Vagrant installed, so it can be shared *
      *with anyone. Useful for testing webhooks, demos with clients, ...   *

     ·*instant SSH access*
       $*$vagrant connect --ssh *  ← (local/remote) client
       (pair programming, debugging ops problems, etc....)
     · General sharing by exposing a tcp port and
       $*$ vagrant connect      *  ← (local/remote) client
       (pair programming, debugging ops problems, etc....)

 - Vagrant Command List:
   $ vagrant "COMMAND" -h
   $ vagrant list-commands  # Most frequently used commands

- FREQUENTLY USED
  box           manages boxes: installation, removal, etc.
  destroy       stops and deletes all traces of the vagrant machine
  global-status outputs status Vagrant environments for this user
  halt          stops the vagrant machine
  help          shows the help for a subcommand
  init          initializes new environment (new Vagrantfile)
  login         log in to HashiCorp's Vagrant Cloud
  package       packages a running vagrant environment into a box
  plugin        manages plugins: install, uninstall, update, etc.
  port          displays information about guest port mappings
  powershell    connects to machine via powershell remoting
  provision     provisions the vagrant machine
  push          deploys enviroment code → (configured) destination
  rdp           connects to machine via RDP
  reload        restart Vagrant VM, load new Vagrantfile config
  resume        resume a suspended vagrant machine
  snapshot      manages snapshots: saving, restoring, etc.
  ssh           connects to machine via SSH
  ssh-config    outputs OpenSSH connection config.
  status        outputs status of the vagrant machine
  suspend       suspends the machine
  up            starts and provisions the vagrant environment
  validate      validates the Vagrantfile
  version       prints current and latest Vagrant version

- OTHER COMMANDS
  cap             checks and executes capability
  docker-exec     attach to an already-running docker container
  docker-logs     outputs the logs from the Docker container
  docker-run      run a one-off command in the context of a container
  list-commands   outputs all available Vagrant subcommands, even non-primary ones
  provider        show provider for this environment
  rsync           syncs rsync synced folders to remote machine
  rsync-auto      syncs rsync synced folders automatically when files change


- QUICK HOW-TO:
  $ mkdir vagrant_getting_started
  $ cd vagrant_getting_started
  $ vagrant init                  # <- creates new Vagrantfile.



- "Advanced" Vagranfile Example: 3 VM's Cluster using Virtual Box
  ┌──────────────────────────────────────────────────────────────────────────┐
  │ # -*- mode: ruby -*-                                                     │
  │ # vi: set ft=ruby :                                                      │
  │                                                                          │
  │ VAGRANTFILE_API_VERSION = "2"                                            │
  │                                                                          │
  │ Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|                   │
  │    # Use the same key for each machine                                   │
  │    config.ssh.insert_key = false                                         │
  │                                                                          │
  │    config.vm.define "vagrant1" do |vagrant1|                             │
  │            vagrant1.vm.box = "ubuntu/xenial64"                           │
  │            vagrant1.vm.provider :virtualbox do |v|                       │
  │                    v.customize ["modifyvm", :id, "--memory", 1024]       │
  │            end                                                           │
  │            vagrant1.vm.network "forwarded_port", guest: 80, host: 8080   │
  │            vagrant1.vm.network "forwarded_port", guest: 443, host: 8443  │
  │            vagrant1.vm.network "private_network", ip: "192.168.0.1"      │
  │            # Provision through custom bootstrap.sh script                │
  │            config.vm.provision :shell, path: "bootstrap.sh"              │
  │    end                                                                   │
  │    config.vm.define "vagrant2" do |vagrant2|                             │
  │            vagrant2.vm.box = "ubuntu/xenial64"                           │
  │            vagrant2.vm.provider :virtualbox do |v|                       │
  │                    v.customize ["modifyvm", :id, "--memory", 2048]       │
  │            end                                                           │
  │            vagrant2.vm.network "forwarded_port", guest: 80, host: 8081   │
  │            vagrant2.vm.network "forwarded_port", guest: 443, host: 8444  │
  │            vagrant2.vm.network "private_network", ip: "192.168.0.2"      │
  │    end                                                                   │
  │    config.vm.define "vagrant3" do |vagrant3|                             │
  │            vagrant3.vm.box = "ubuntu/xenial64"                           │
  │            vagrant3.vm.provider :virtualbox do |v|                       │
  │                    v.customize ["modifyvm", :id, "--memory", 2048]       │
  │            end                                                           │
  │            vagrant3.vm.network "forwarded_port", guest: 80, host: 8082   │
  │            vagrant3.vm.network "forwarded_port", guest: 443, host: 8445  │
  │            vagrant3.vm.network "private_network", ip: "192.168.0.3"      │
  │    end                                                                   │
  │ end                                                                      │
  └──────────────────────────────────────────────────────────────────────────┘
[[vagrant}]]

● Yaml References: [[{yaml.101]]
@[http://docs.ansible.com/ansible/YAMLSyntax.html]
YAML                               JSON
---                                {
key1: val1                             "key1": "val1",
key2:                                 "key2": [
 - "thing1"                            "thing1",
 - "thing2"                            "thing2"
# I am a comment                     ]
                                   }

- Anchors allows to reuse/extends YAML code:
  ┌─ YAML ───────────┐···(generates)··>┌─ JSON ────────────────┐
  │ ---              │                 │                       │
  │ key1: &anchor    ← '&' Defines     │ {                     │
  │  K1: "One"       │ the anchor      │   "key1": {           │
  │  K2: "Two"       │                 │     "K1": "One",      │
  │                  │                 │     "K2": "Two"       │
  │                  │                 │   },                  │
  │ key2: *anchor    ← References/     │   "key2": {           │
  │                  │ uses the anch.  │     "K1": "One",      │
  │                  │                 │     "K2": "Two"       │
  │                  │                 │   }                   │
  │ key3:            │                 │   "key3": {           │
  │   <<: *anchor    ← Extends anch.   │     "K1": "One",      │
  │   K2: "I Changed"│                 │     "K2": "I Changed",│
  │                  │                 │     "K3": "Three"     │
  │                  │                 │   }                   │
  │   K3: "Three"    │                 │                       │
  │                  │                 │ }                     │
  └──────────────────┘                 └───────────────────────┘
  WARN!!!: Many NodeJS parsers break the 'extend' functionality.

- Extend Inline:
  - take only SOME sub-keys from key1 to inject into key2
  ┌─ YAML ──────     ┐···(generates)··>┌─ JSON ───────────┐
  │ ---              │                 │ {                │
  │ key1:            │                 │   "key1": {      │
  │  <<: &anchor     ← Inject into     │     "K1": "One", │
  │    K1: "One"     │ key1 and save   │     "K2": "Two"  │
  │  K2: "Two"       │ as anchor       │   },             │
  │                  │                 │                  │
  │ bar:             │                 │   "bar": {       │
  │  <<: *anchor     │                 │     "K1": "One", │
  │  K3: "Three"     │                 │     "K3": "Three"│
  │                  │                 │   }              │
  │                  │                 │ }                │
  └──────────────────┘                 └──────────────────┘

- yaml2js python Ulitity:
  - Add next lines to ~/.bashrc (or /etc/profile or ...):
  + alias yaml2js="python -c 'import sys, yaml, json; \
  +                json.dump(yaml.load(sys.stdin), sys.stdout, indent=4)'"

  Ussage:
  $ cat in.yaml | yaml2js > out.json

 *WARN:* - Unfortunatelly there is no way to override or
           extends lists to append new elements to existing ones,
           only maps/dictionaries with the '<<' operator:
           '<<' "inserts" values of referenced map into
           current one being defined.
