##  Apropos:
- Visit next Web site for a great experience:
  https://earizon.github.io/txt_world_domination/viewer.html?payload=../DevOps/linux.txt
- If you want to contribute to great gistory of this document you can
  take the next flight to:
@[https://www.github.com/earizon/DevOps]
  Your commits and pull-request will be immortalized in the Pantheon of the Unicode Gods.


LINUX ADMINISTRATION RECIPES

## External Links: [[{]]
- @[https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/tree/Documentation/]
- @[http://www.linuxfromscratch.org/]
- @[https://linux.die.net/man/]
- @[https://linux.die.net/Linux-CLI/]
- @[https://en.wikipedia.org/wiki/Linux_Standard_Base]
- @[https://docs.fedoraproject.org/en-US/fedora/f34/system-administrators-guide/]
- @[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/]
[[}]]

# Linux Basics [[{linux.101]]
- Linux itself is just an OS kernel in charge of sharing the LIMITED hardware
  resources amongst potentially many running tasks and users working simultaneously
  on the systems. More preciselly, the main tasks in charge of kernel control are:

  - Orchestate(schedule) how much time each running-task is allowed to
    run on each CPU before such task is put on stand-by to let
    another task proffit from such CPU.

  - Assign which process has access to the (limited)
    RAM memory on the system and move data on RAM used by stand-by processes
    to secondory storage (disks) in case of RAM space shortage.

  - Provide support for users and permissions, so that different users will
    be able to isolate and protect its data from other users.

  Kernel control is transparent to running tasks or processes, so user-space task
  will run with the perception that they run alone in its own CPU and with
  all available memory for themself. When the kernel puts them on-hold
  such tasks will be freezed and once restarted it will NOT notice any
  change to the state before being put on-hold. Only the kernel is aware of
  the complex trickeries needed to make tasks run in parallel and isolated
  from each other.

  Other important tasks offered by the kernel are:

 - Abstract the running hardware into standarized interfaces, so
   user application will not need to work differently with different hardware.
   For example an application will ask the kernel to write data to the disk and
   the kernel will take care of the internal difference between the miriads
   of diferent disk hardware technologies.

 - Provide an easy-to-use file-system to work with data on disk, so that apps
   can orginized data in terms of files and directories, instead of just
   bunch-of-bytes on the hard-disk.

 - Provide network communication and support for standard network protocols
   like TCP/UP, bluetooth, WiFI, ... so that each app does not need to reimplement
   them.

 - Provide mechanisms to allow two running tasks to communicate with each other
   at will.

*Kernel mode vs User mode*
- When the CPU is executing kernel code it's running with elevated privileges.
  The software has full control of the hardware and can do "anything" on
  the system and access all RAM, disk, network resources at will.

- Standard applications will run in user-mode and will have limited access
  to only the RAM memory assigned by the kernel. They will not be able to
  inspect the memory of other processes running on the system. In fact they
  are not aware of such memory due to the many trickeries done by the kernel
  to isolate each task.

*Files, files and more files*
- Any running process needs some incomming data to work with and
  produced new data that must be stored somewhere.
   This data can be provided by some storage system (hard-disk,
  usb, tape, ...), arrive (continuosly) from the network,
  or be generated by another concurrent process.
   Linux (UNIX actually) treats all input data sources and
  output data sinks as *"file devices"*.
  Internally there can be many differences (block devices with
  random access vs char devices with just sequential access), but
  running processes mostly always use the file methaphor to access
  all of them.
   Any running process will have 3 devices available "for free":
  - STDIN : The standard input  file.
  - STDOUT: The standard output file
  - STDERR: The standard error  file
   The standard shell provides many utilities to juggle with those
  three standard files. In particular it allows to easely forward
  the STDOUT output from a running-process to the STDIN input of
  another running process using the "|" pipe syntax:
  $ command1 *|*command2  # ← Send STDOUT output from command1 to
                                        STDIN   input   of command2

  STDOUT and STDERR by default are assigned to the running-process
  associated terminal (the console where the user has been logged).
  The shell allows also to redirect STDOUT/STDERR to any other
  file in our file system. Ex:
  $ command1 1>output.log         2>error.log
             ^^^^^^^^^^^^         ^^^^^^^^^^^
             redirects STDOUT(1)  redirects STDERR
             to output.log        to error.log

  $ command1 1>output.log         2>&1
             ^^^^^^^^^^^^         ^^^^
             redirects STDOUT(1)  redirects STDERR
             to output.log        to STDOUT (&1, aka output.log)
[[}]]

## Process model [[{linux.101.process_model,job_control.101,]]

- Linux follows a parent-child process model.

- Once loaded and initialized during the boot process,
  the kernel will launch an initial user-space process in
  charge of reading system configuration and (re)start
  all other user-space processes that builds a running system.

- Normally this initial process is systemd in modern
  systems (or initd in older or embedded ones).

- Each process can optionally launch new children processes
  up to the resource limits established on the running system.

- By default a child-process inherits the same user (and so, permissions)
  than its parent process. Some processes like the remote
  login "sshd" service (ssh is an acronymn for secure-shell) will
  change the child-process user/permission to a less privileged
  account.

- A simplified process-tree of a running-system will look like:      [[{02_doc_has.diagram]]

    PROCESS                                       USER    PROCESS   PARENT-ID
                                                         UNIQUE-ID
    systemd·······································root       1         0
          └─crond·································root      23         1
          |-cupsd·································root      70         1
          |-rtkit-daemon··························rtkit    100         1
          |-sshd··································root      10         1
          |    └─sshd·····························mike     300        10
          |         └─bash························mike     301       300
          |              └─firefox················mike     302       301
          |-systemd·······························alice    705         1
          |       └─at-spi-bus-laun···············alice    706       705
          |       |···············└─dbus-daemon···alice    707       706
          |       |-gnome-terminal················alice    883       705
          |                       ─bash-+·········alice    884       883
          |                             └─top·····alice    885       884
          |-systemd-journal·······················root      10         1
          ...
    Notice for example that the same process "bash" runs as a user or another
    ( *mike* or  *alice*) depending on the "path"
    followed until the process is executed.                          [[}]]

    - The initial sshd running as root user, will span a new sshd child process
      with restricted  *"mike"* privileges/permissions once the user
      has introduced its correct user and password in the remote ssh session, and
      from there on, all children will just be able to run with  *"mike"*
      privileges/permissions.

    - Similarly the root systemd process will span a new child process will
      restricted  *"alice"* privileges/permissions once logged in
      the local console, and from there on, all children will just be able to
      run with  *"alice"* privileges/permissions.


## Executable file vs in-memory process [[{linux.101]]
- Applications are stored on disk drives as files or "bunch-of-instructions and initial data".

- When the kernel executes and application it will read the executable file, load
  the "bunch-of-instructions" into RAM memory, setup the initial data, assign
  restricted privileges and finally allow the program-in-memory to be executed by
  any free-available CPU on the system.
  [[}]]


## Basic file permissions  [[{linux.101,security.aaa]]
Standar file permissions allows to assign different access permissions to
the owner of the file, the group owner of the file and anyone else.

$ ls -l myFileOfInterest.db

-rw?-r-?---? john accountDepartment ....  myFileOfInterest.db
└┼┘│└┼┘│└┼┘│  └┬─┘ └──────┬────────┘
 │ │ │ │ │ │   │          │
 │ │ │ │ │ │   │          └─ group owner
 │ │ │ │ │ │   └──────────── user  owner
 │ │ │ │ │ │
 │ │ │ │ │ └──────────────── sticky bit (hidden if not set)
 │ │ │ │ │
 │ │ │ │ └────────────────── permissions allowed to others: read           access
 │ │ │ │
 │ │ │ └──────────────────── SUID bit   (hidden if not set)
 │ │ │
 │ │ └────────────────────── permissions allowed to group : read           access
 │ │
 │ └──────────────────────── SUID bit   (hidden if not set)
 │
 └────────────────────────── permissions allowed to user  : read&amp;write access

Previous line can be read as:
"""Allow read and write permissions to file-owner "john",
       read permissions to group-owner "accountDepartment"
   and no   permissions to anyone-else """

            ┌──────┬─────────────────────────────┬─────────────────────────────┐
Permissions │Symbol│      FILE                   │     DIRECTORY               │
┌───────────┼──────┼─────────────────────────────┼─────────────────────────────┤
│       read│  r   │ Allows to read the content  │ Allows to list the files in │
│           │      │ of the file                 │ and file-attributes in the  │
│           │      │                             │ directory                   │
├───────────┼──────┼─────────────────────────────┼─────────────────────────────┤
│      write│  w   │ Allows to write, modify,    │ Allows to add and delete    │
│           │      │ append or delete the file   │ files into the directory and│
│           │      │ content.                    │ modify metadata (access     │
│           │      │                             │ or modification time, ...)  │
├───────────┼──────┼─────────────────────────────┼─────────────────────────────┤
│    execute│  x   │ Allows to execute the       │ Allows to enter into the    │
│           │      │ program or script           │ directory                   │
├───────────┼──────┼─────────────────────────────┴─────────────────────────────┤
│    sticky │  T   │ Only the person that created the file/dir. can change it, │
│           │      │ even if other people have write permissions to file/dir.  │
│           │      │ turn on: $ chmod +t someFileOrDir                         │
│           │      │ Normal /tmp (temporary user files) is an sticky directory │
├───────────┼──────┼───────────────────────────────────────────────────────────┤
│       suid│  S   │ Allow SUID/SGID (switch user/group ID). When executed it  │
│           │      │ it will be executed with creator/group of file, instead of│
│           │      │ current user.                                             │
│           │      │ turn on: $ chmod +s someFileOrDir                         │
└───────────┴──────┴───────────────────────────────────────────────────────────┘
[[}]]

## Network Time Protocol 101 [[{configuration.clock,troubleshooting]]

   # REF:@[https://www.server-world.info/en/note?os=Fedora_25&p=ntp&f=2]
   $ sudo dnf -y install ntp                       # <- STEP 1) Install ntp client on RedHat flavours
                                                   #   sudo apt install ntp on Debian/Ubuntu flavours
   $ edit /etc/ntp.conf                            # <- STEP 2) Create/Modify config. file
   ┌──────────────────────────────────────
   │ ...
   │ restrict 10.0.0.0 mask 255.255.255.0 nomodify notrap
   │   server 0.fedora.pool.ntp.org iburst
   │ # server 1.fedora.pool.ntp.org iburst
   │ # server 2.fedora.pool.ntp.org iburst
   │ # server 3.fedora.pool.ntp.org iburst
   │ # server ntp.nict.jp iburst
   │ # server ntp1.jst.mfeed.ad.jp iburst
   │ # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   │ # Enable/disable NTP servers "at will"
   $ sudo systemctl start ntpd                     # <- STEP 3) Start daemon as (SystemD) service
   $ ntpq -p                                       # <- STEP 4) check
   $ sudo systemctl enable ntpd                    # <- STEP 5) Enable at boot time.
   $ sudo timedatectl set-timezone Europe/London   # <- STEP 5) Set timezone
[[}]]

#[linux_users_groups_summary]
## User Management [[{linux.101,security.aaa]]
  $ useradd -D                       ← display (D)efault values to apply when creating new uses.
                                        (Default group / HOME, SHELL, ...)
                                        Use next flags to update defaults:
                                        --base-dir BASE_DIR  :  (default to /home) Ignored if --home-dir set
                                        --expiredate EXPIRE_DATE
                                        --inactive INACTIVE  :  day # after pass.expiration before disabling
                                        --gid GROUP: existing group name or ID for initial group (when --no-user-group used)
                                        --shell SHELL

  $ addgroup PROJECT01               ← Best pattern when allowing multi-user access to the linux machine.
                                        Create a new group per project ,then assign permissions to group (vs
                                        to user). This greately simplifies permissions administration in
                                        multi-user setups.


  $ useradd alice                 \   ← Create user alice.
      --shell=/usr/bin/git-shell  \   ← Assign shell (restricted git-shell in this example)
      --shell=/usr/bin/git-shell  \   ← upon succesful login alice will be presented with a git shell *1
      --groups PROJECT01,PROJECT02\   ← Assign PROJECT01 as default group for alice.
      --password ...              \   ← create an initial and strong password (or provide alice with ssh keys.
      --no-user-group             \   ← Do NOT create a new group (otherwise indicated by --gid)
      --no-create-home            \   ← Do not create the /home/alice directory. (Some sort of
                                        /var/lib/git/PROJECT01 directories already exist with read/write permissions
                                        for PROJECT01, ... groups)

                                      *1 By default /bin/bash o similar is used. git-shell restricts
                                         access to just git-related commands.
   Other useful 'useradd' options are:
   --skel SKEL_DIR :  skel. dir. to be copied in the user's home directory
   --key KEY=VALUE : Overrides /etc/login.defs defaults
                     (UID_MIN, UID_MAX, UMASK, PASS_MAX_DAYS and others).
                     Example: -K PASS_MAX_DAYS=-1 can be used when creating
                       system account to turn off password ageing, even though
                       system account has no password at all.
   --no-log-init   : Do not add user to lastlog and faillog databases
   --non-unique    : Allow duplicate (non-unique) existing UID in --uid. Most of the times used to
                     allow 'root' users with non-root name accounts.
   --system        : Create system account (no aging, uid chosen in SYS_UID_MIN-SYS_UID_MAX range)
   --root CHROOTDIR: Apply changes in chrooted directory.
   --uid UID       : numerical value for user's ID.
   --user-group    : Create group with the same name as user, and use as initial group
   --selinux-user SEUSER : SELinux user for the user's login


• su/sudo: Switch user:  [[{security.aaa,linux.101,01_PM.WiP]]
- su and sudo are mostly used to allow temporal root/superuser access
  to standard users for administrative tasks like installing new applications,
  re-configuring the network, ...

- sudo is ussually considered safer than su. Ubuntu was the first
  distribution to allow sudo-only. Others distributions are also
  changing to sudo-only as time passes.

- sudo offers also a plugable architecture not offered by su
  to provide different authentication and audit mechanisms.
  REF:
  - https://www.sudo.ws/ (Sudo Home page)
  - https://www.sudo.ws/plugins.html (sudo Third-party plugins)
    https://linux.die.net/man/1/su
    https://linux.die.net/man/8/sudo
- Ex. ussage:
  $ sudo vim /etc/passwd # edit /etc/passwd as root
  $ su  # Change to root user
[[}]]

## Linux ACLs  [[{security.aaa,01_PM.TODO]]
@[https://wiki.archlinux.org/index.php/Access_Control_Lists]
[[}]]

#[sysctl_summary]
## Kernel Tunning [[{configuration.sysctl,configuration.network,storage.fs,troubleshooting.performance,]]
• sysctl
  - Cli app to view/modify kernel parameters affecting the runtime behavior
    of network, file-systems, kernel logging, ...
  - Not to be confused with (SystemD) 'systemctl' used to add/remove/modify/...
    services (applications running in background), centralized app logging,...
  - TIP: *read comments in /etc/sysctl.conf  for most widely tunned params.*

  $ sysctl -a                    ← View all configured kernel params
                                   Output can be filtered out easily with
                                   'grep'.

  $ sysctl vm.min_free_kbytes    ← View a given param
                                   -n: opt. Show only value (vs param name)


  $ sudo sysctl -w net....=0     ← Set new value for kernel param.


  $ sudo edit /etc/sysctl.conf   ← make changes permanent after reboot
                                  edit /etc/sysctl.d/99-custom.conf for custom
                                  changes.


  $ sudo sysctl -p               ← apply any change in /etc/sysctl.conf
                                   Use -p /etc/sysctl.d/99-custom.conf for
                                   alternative (non default) path

  /proc/sys/ OVERVIEW:
  ├ abi/vsyscall32
  ├ debug
  │ ├ exception-trace
  │ └ kprobes-optimization
  ├ dev
  │ ├ parport
  │ │ ├ default/...
  │ │ └ parport0/...
  │ │   ├ devices
  │ │   ...
  │ ├ raid/speed_limit_max(min)
  │ ├ scsi/logging_level
  │ ├ tty/ldisc_autoload
  │ ...
  ├ kernel    REF:  https://www.kernel.org/doc/html/v5.7/admin-guide/sysctl/kernel.html
  │ ├ acct
  │ ├ sched_*
  │ ├ sem
  │ ├ watchdog*
  │ ...
  ├ net
  │ ├ core/... (40 tunnable params)
  │ ├ ipv4/* (devices, conf, route, tcp, udp,...)
  │ ├ ipv6/...
  │ ├ netfilter ... (65 tunnable params)
  │ ├ nf_conntrack_max
  │ ├ unix
  │ │ └ max_dgram_qlen
  │ ...
  ├ fs
  │ ├ inotify/...
  │ ├ nfs/...
  │ ├ ext4/...
  │ ├ xfs/...
  │ ...
  ├ sunrpc ... (11 tunnable params)
  ├ user/... (9 tunnable params)
  └ vm/... (48 tunnable params)
[[configuration.sysctl}]]

## Kernel Tunning with 'tuned' [[{linux.101,monitoring.101,configuration.kernel,01_PM.TODO,qa.UX]]
  /proc/sys/vm/ Kernel FS "control pannel"   [[{01_PM.TODO}]]
  https://tuned-project.org/
- About tuned:
  - OS Tunning is done in practice through:
    - long-profiling
    - continuous-monitoring
  - Tunning becomes harder if system load changes frequently.

    Ex.: A system with a peak of load certain hours a day
         can be tuned for performance     those known hours
         and    tuned for power-efficency the rest of day


- man 8 tuned: Dynamic Adaptive system tuning daemon
@[https://linux.die.net/man/8/tuned]

  - cron-friendly system service that lets to select a tuning profile
    (pre-build or custom).
  - Tuning include:
    - sysctl settings (/proc/sys/)
    - settings for disk-elevators
    - power management options
    - transparent hugepages
    - custom-scripts

- Install):
  $ sudo dnf -y install tuned   # <· RedHat/Fedora/CentOS package install
  $ sudo systemctl enable tuned # <· enable tuned service at boot
  ...
  $ sudo systemctl start  tuned # <· Start  tuned service now
  $ sudo systemctl status tuned # <· Check  tuned service status
  $ sudo systemctl status tuned
→ * tuned.service - Dynamic System Tuning Daemon
→    Loaded: loaded (/usr/lib/systemd/system/tuned.service; disabled; vendor preset: disabled)
→    Active: *active (running)* since Sun 2019-01-20 16:29:05 EST; 15s ago
→      Docs: man:tuned(8)
→            man:tuned.conf(5)
→            man:tuned-adm(8)
→  Main PID: 10552 (tuned)
→     Tasks: 4 (limit: 4915)
→    Memory: 15.7M
→    CGroup: /system.slice/tuned.service
→            └─10552 /usr/bin/python3 -Es /usr/sbin/tuned -l -P

*Ussage:*
$ *tuned-adm list*   #  ← List existing tunning profiles
→ Available profiles:
→ - balanced                    - General non-specialized
→ - desktop                     - Optimize for the desktop
→ - latency-performance         - deterministic performance             (increased power consumption)
→ - network-latency             - deterministic performance low-latency (increased power consumption)
→ - network-throughput          - Optimize for streaming network throughput
                                  generally only necessary on older CPUs or
                                  40G+ networks
→ - powersave                   - Optimize for low power consumption
→ - throughput-performance      - provides excellent performance across a
                                  variety of common server workloads
→ - virtual-guest               - Optimize for running inside a virtual guest
→ - virtual-host                - Optimize for running KVM guests
→ Current active profile: balanced

$ *sudo tuned-adm active*  #  ← query status
Current active profile: balanced

$ *sudo tuned-adm profile  powersave* # ←  select profile
[[}]]


[[}]]

## Job/Process control: Scheduling Tasks  [[{101,job_control.task_scheduling,01_PM.TODO]]
  Officially in Linux/UNIX/Posix, a Job or task is a running process.
  - cron    : program task to be run periodically
  - at
  - anacron :
    Unlike cron(8), it does not assume that the machine is running continuously.
    Hence,  it  can  be used on machines that aren't running 24 hours a day.

    Anacron checks whether this job has been executed in the last n days.
    If not, Anacron runs the job's shell command, after waiting for the
    number of minutes specified as the delay parameter.
    NOTE/WARN: Only the date , not hour is used.

    e.g.: On Debian-based systems, anacron will be activated hourly every day
          from 07:30  local  time  to 23:30  local time through cron job

## Job control++
  - nice      : run a process with modified scheduling priority.
  - STOP/CONT : kill -STOP $pid "freezes" a process.
  -             kill -CONT $pid un-freezes it.
  - chroot    : (Ch)ange Root, runs a process withing a modified view of   [[{security.101]]
                the OS File System, where the root seen by the process is
                just a subdirectory of the real root path seen by the OS.
                Basically containerization extends the idea of chroot to
                let processes view only a subset of the FS and network
                through (kernel supported) namespaces. [[}]]
  - cgroups   : linux kernel (C)ontrol groups allow processes to be organized
                into hierarchical groups whose usage of various types of
                resourcescan then be limited and monitored.

                /sys/fs/cgroup pseudo-FS is used to control them.

    └ cgroup  : kernel component modifying the behavior of processes.
     subsystem  (CPU time, available memory, ...).
                 also known as resource controllers (or simply, controllers).
  ...
[[}]]

## GNU Parallel  [[{job_control,performance.distributed]]
@[https://linux.die.net/man/1/parallel]
@[https://www.gnu.org/software/parallel/parallel_cheat.pdf]
@[https://www.reddit.com/r/programming/comments/ayew9r/never_got_around_to_learning_gnu_parallel_here_is/]
- Replacement for xargs and for-loops.
- It can also split a file or a stream into blocks and pass those to commands running in parallel.

Ex:

$ parallel --jobs 200% gzip ::: *.html  # ← Compress all *.html files in parallel
                                            200% → 2 per CPU thread

$ parallel lame {} -o {.}.mp3 ::: *.wav # ← Convert all *.wav to *.mp3 using lame

$ cat bigfile | \                       # ← Chop bigfile into 1MB blocks and grep
  parallel --pipe grep foobar               for the string foobar

*INPUT SOURCES*
$ parallel echo ::: cmd line input source
$ cat input_from_stdin | parallel echo
$ parallel echo ::: multiple input sources ::: with values
$ parallel -a input_from_file echo
$ parallel echo :::: input_from_file
$ parallel echo :::: input_from_file ::: and command line

*Replacement string*
{}                    ← mydir/mysubdir/myfile.myext
{.}                   ← mydir/mysubdir/myfile
{/}, {//}, {/.}       ← myfile.myext, mydir/mysubdir, myfile
{#}                   ← The sequence number of the job
{%}                   ← The job slot number
{2}                   ← Value from the second input source
{2.} {2/} {2//} {2/.} ← Combination of {2} and {.} {/} {//} {/.}
{= perl expression =} ← Change $_ with perl expression

$ parallel --keep-order "sleep {}; echo {}" ::: 5 4 3 2 1 # ← *Keep input order in output*

*Control the execution*
$ parallel --jobs 2 "sleep {}; echo {}" ::: 5 4 3 2 1  # ← Run 2 jobs in parallel

$ parallel --dryrun echo ::: Red Green Blue ::: S M L  # ← Dry-run. See will be executed
                                                           without real execution

*Remote execution*
$ parallel -S server1 -S server2 "hostname; echo {}" ::: foo bar

*Pipe mode*
cat bigfile | parallel --pipe wc -l

$ parallel -a bigfile --pipepart --block -1 grep foobar # ← Chop bigfile into one block per CPU
                                                            thread and grep for foobar
[[}]]


# Monitoring
## basic process monit/control [[{job_control.101,linux.101,monitoring.jobs,monitoring.i/o]]
$ *ps*   ← shows list of the processes running. i
         Without options: processes belonging to current user&amp;with a controlling terminal
  Ex. options include:
   -a: all processes from all users
   -u: add user names, %cpu usage, and %mem usage,...
   -x: add also processes without controlling terminals
   -l: add information including UID and nice value
   --forest: show process hierarchy.

$ *pstree* ← show parent/children process tree (-p flag show pid)

$ *top -n  * ← Display top by cpu processes once and finish
$ *top*      ← real-time display processes ordered by memory/CPU/...(as in CPU usage)

  Z,B,E,e   Global: 'Z' colors; 'B' bold; 'E'/'e' summary/task memory scale
  l,t,m     Toggle Summary: 'l' load avg; 't' task/cpu stats; 'm' memory info
  0,1,2,3,I Toggle: '0' zeros; '1/2/3' cpus or numa node views; 'I' Irix mode
  f,F,X     Fields: 'f'/'F' add/remove/order/sort; 'X' increase fixed-width

  L,&,<,> . Locate: 'L'/'&' find/again; Move sort column: '<'/'>' left/right
  R,H,V,J . Toggle: 'R' Sort; 'H' Threads; 'V' Forest view; 'J' Num justify
  c,i,S,j . Toggle: 'c' Cmd name/line; 'i' Idle; 'S' Time; 'j' Str justify
  x,y     . Toggle highlights: 'x' sort field; 'y' running tasks
  z,b     . Toggle: 'z' color/mono; 'b' bold/reverse (only if 'x' or 'y')
  u,U,o,O . Filter by: 'u'/'U' effective/any user; 'o'/'O' other criteria
  n,#,^O  . Set: 'n'/'#' max tasks displayed; Show: Ctrl+'O' other filter(s)
  C,...   . Toggle scroll coordinates msg for: up,down,left,right,home,end

  k,r       Manipulate tasks: 'k' kill; 'r' renice
  d or s    Set update interval
  W,Y       Write configuration file 'W'; Inspect other output 'Y'
  q         Quit

$ *iotop*    # ← Simple top-like I/O monitor
@[https://linux.die.net/man/1/iotop]

$ *kill - *   ← Display existing signals (Default to SIGTERM that most of the times
              will just terminate the process "cleanely")
→  1) SIGHUP   2) SIGINT   3) SIGQUIT  4) SIGILL   5) SIGTRAP
→  6) SIGABRT  7) SIGBUS   8) SIGFPE   9) SIGKILL 10) SIGUSR1
→ 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM
→ 16) SIGSTKFLT   17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP
→ 21) SIGTTIN 22) SIGTTOU 23) SIGURG  24) SIGXCPU 25) SIGXFSZ
→ 26) SIGVTALRM   27) SIGPROF 28) SIGWINCH    29) SIGIO   30) SIGPWR
→ 31) SIGSYS  34) SIGRTMIN    35) SIGRTMIN+1  36) SIGRTMIN+2  37) SIGRTMIN+3
→ 38) SIGRTMIN+4  39) SIGRTMIN+5  40) SIGRTMIN+6  41) SIGRTMIN+7  42) SIGRTMIN+8
→ 43) SIGRTMIN+9  44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13
→ 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12
→ 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9  56) SIGRTMAX-8  57) SIGRTMAX-7
→ 58) SIGRTMAX-6  59) SIGRTMAX-5  60) SIGRTMAX-4  61) SIGRTMAX-3  62) SIGRTMAX-2
→ 63) SIGRTMAX-1  64) SIGRTMAX

$ *kill [ -s (signal name)] 'process_id'* ← Send signal to process. kill -9 kills unconditionally
$ killall "process_name"  ← send signal to all processes matching full name
$ pkill "process_name"    ← send signal to all processes matching part of the name
$ skill  ← send a particular signal to command/username/tty.
        -L --- list the various signals that can be sent
        -u --- specify a username;
        -p --- process id (followed by the process id)
        -c --- command name (this is the same as killall)
        -t --- (tty number)
        -v --- verbose mode
        -i --- interactive mode.

PAUSE AND CONTINUE A PROCESS:
$ *kill -STOP "pid"* # Pauses
$ *kill -CONT "pid"* # Continues

$ *nice -20 make* ← Sets make priority to -20
                  -20 is maximum priority   (negative only allowed to root)
                   20 is the minimum priority.
$ *renice 10 "pid"* ← Changes priority of running process.
[[}]]


## Disk Space [[{troubleshooting.disk_full]]
   $ df -kh               <·· (D)isk (F)ree report.
                              -k : take block-size=1K
                              -h : human-readable, print sizes in powers of 1024 (e.g., 1023M)
   $ du -sch dir1 dir2    <·· (D)isk (U)ssage report for files&directories.
                              -s: summarize. Display only a total for each argument
                              -c: produce grand total
                              -h : human-readable
[[}]]

## `pv` monit data pipe progress [[{monitoring.pipes,monitoring.i/o,profiling.storage.FS,storage.profiling]]
• pv provides time elasped, %compl., progressbar, throughput, ETA, ...on STDERR.
• cat /dev/zero | pv | cat > /dev/null  # Check CPU/memory perf.
• pv file | nc -w 1 somewhere.com 3000  # Check network perf.
• pv -EE /dev/sda > disk-image.img      # Check disk image (skip errors)

  man pv summary
OUTPUT MODIFIERS
  --wait: until first byte has been transferred.
  --delay-start SEC (before showing info)
  --size SIZE: Set total size (if piping from STDIN and size is known)
  --line-mode: (count lines vs counting bytes)
               --null for null terminated.
  --interval SEC: (1 sec by default)
  --width N / --height N: Assume width / height for terminal
  --name NAME: Useful with --cursor for "compilcated pipelines"
               --cursor: Use cursor pos. escape sequences (vs CR)
  --force: (even if STDOUT is not terminal)

DATA TRANSFER MODIFIERS
  --rate-limit RATE
  --buffer-size BYTES
  --no-splice: Never  use splice(2). Ussually more efficient way of
               transferring data from/to pipe than regular read/write.
               but means that the transfer buffer may not be used.
               This prevents -A and -T from working.
  -E/--skip-errors (set twice to only report a read error once per file)
  --stop-at-size
  --watchfd PID[:FD]. Wath FIle Descriptor of process PID, and show its
                      progress.
  --remote PID : where PID is an running instance of pv already running.
                 Add other commands to such instance.
                 --pidfile FILE save PID of (first running) pv instance.
[[}]]

## Glances [[{monitoring.101,01_PM.low_code,QA.UX,troubleshooting]]
@[https://www.tecmint.com/glances-an-advanced-real-time-system-monitoring-tool-for-linux/]
@[https://github.com/tldr-pages/tldr/blob/master/pages/common/glances.md]
- Alternative to top
- Linux/FreeBSD python command-line,  curses-based, using psutils
  under the hood.
- It makes easier to find an application/process consuming lots
  of system resources  by highlighting programs consuming too much
  resources and providing maximum of information about the server.
- Allows to define thresholds (careful, warning and critical) in
  config files.

- Display info about:
  - CPU (user, kernel, idle processes).
  - RAM, Swap, Free memory,... etc.
  - Average CPU load for past 1min, 5mins and 15 mins.
  - Network Download/Upload rates of network connections.
  - Total number of processes, active ones, sleeping processes etc.
  - Disk I/O related (read or write) speed details
  - Currently mounted devices disk usages.
  - Top processes CPU/Memory usages, Names and file path of exec

- official packages for Debian/RedHat/...

 *USSAGE*
  $ glances
    GREEN : OK       (everything is fine)
    BLUE  : CAREFUL  (need attention)
    VIOLET: WARNING  (alert)
    RED   : CRITICAL (critical)

 *Shortcuts:*
  a – Sort processes automatically
  c – Sort processes by CPU%
  m – Sort processes by MEM%
  p – Sort processes by name
  i – Sort processes by I/O rate

  d – Show/hide disk I/O stats ols
  f – Show/hide file system statshddtemp
  n – Show/hide network stats
  s – Show/hide sensors stats
  y – Show/hide hddtemp stats
  l – Show/hide logs
  b – Bytes or bits for network I/Oools
  w – Delete warning logs
  x – Delete warning and critical logs
  x – Delete warning and critical logs
  1 – Global CPU or per-CPU stats
  h – Show/hide this help screen
  t – View network I/O as combination
  u – View cumulative network I/O
  q – Quit (Esc and Ctrl-C also work)


 *default thresholds: /etc/glances/glances.conf*
  (careful=50, warning=70 and critical=90)


- Client/Server mode:
  ON THE SERVER                                  ON THE CLIENT

$*# glances -s -B $ADDRESS -p $PORT *          $*# glances -c -P 172.16.27.56 *
                  (0.0.0.0)   (61209)
  Define the password for the Glances server
  Password:
  Password (confirm):
  Glances server is running on 0.0.0.0:61209
[[}]]

## Dstat (vmstat + iostat+ ifstat + mpstat) [[{monitoring.101,monitoring.memory,monitoring.I/O,monitoring.network,monitoring.cpu,01_PM.TODO,troubleshooting]]
@[https://linux.die.net/man/1/dstat]
- Dstat: moderm replacement joining the info from    [[{02_doc_has.comparative]]
  vmstat, iostat, ifstat an mpstat.                  [[}]]
- Dstat overcomes some of the limitations and adds some extra features.
- Dstat allows you to view all of your system resources instantly,
  you can eg.:
  - compare disk usage in combination with interrupts from disk controller.
  - compare network bandwidth numbers directly with the disk throughput (in the same interval).
[[}]]

## pidstat: process stats [[{monitoring.jobs,job_control,monitoring.i/o]]
                         [[profiling.jobs.IO.block,profiling.storage,storge.profiling]]
  By Sebastien Godard (http://pagesperso-orange.fr/sebastien.godard/)
 -d Report I/O statistics
    kB_rd/s
    kB_wr/s
    kB_ccwr/s:  Cancelled-writes eg. task truncates some dirty pagecache.
                (IO which another task has been accounted for).
    iodelay  : Block-I/O delay of task-monitored, measured in clock ticks.                    [[{profiling.jobs.]]
               including  delays waiting for sync-block-I/O and swaping-block-I/O completion. [[}]]

 --dec= 0|1|2*  number of decimal-places to use.
 -e 'program' arg1 arg2 ...  Exec. program + args to be monitored.
 -G cmd_name_regex Filter by processes mathching regex for its command-name .
 -H Display timestamp-in-seconds since epoch. (1970-01-01)
 -h Display activities horizontally in a single line, without statistics (for batch tasks)
 --human
 -I In SMP archs, indicate to divide task CPU ussage should be divided by total num.or proccessors.
 -l Display command-name + all its arguments.
 -p $PID Filter by process $PID Select tasks (processes)
         Use SELF for the pidstat process itself.
         Use ALL  for all tasks.
 -R Report realtime-priority and scheduling-policy info, including:
 -r Report pagefaults and memory utilization. [[profiling.jobs.pagefaults]]
   minflt/s : Total number of minor-faults per sec, (not loading from disk).
   minflt-nr: Total number of minor-faults for task-and-children for time-interval.
   majflt/s : Total number of major-faults per secd,
   majflt-nr: Total number of major-faults for task-and-children for time-interval.
   VSZ      : virtual memory usage of entire task in kilobytes.
   RSS      : Resident Set Size: (non-swapped physical memory used in kilobytes).
   %MEM     : Resident Set Size %
   -nr      : Total number of major-faults by task-and-children for time-interval.
 -s     Report stack utilization.  [[profiling.jobs.memory.stack]]
   StkSize  : memory in kilobytes reserved for the task as stack, (not necessarily used)
   StkRef   : memory in kilobytes used as stack, referenced by the task.
 -T {TASK*|CHILD|ALL}
 -t     display statistics per-thread.
 -U [ username ] Display  real-user-name or filter by user-name.
 -u Report CPU utilization.  [[{profiling.jobs.cpu]]
   %usr      : CPU used by task at user-level NOT include time spent running a Virt.CPU.
    usr-ms   : Total millisecs spent by task+children at user level.
   %system   : CPU used by task while executing at the system level (kernel).
    system-ms: Total millisecs spent by task+children at systeml level (kernel).
   %guest    : Percentage of CPU spent by the task    in virt. machine. (KVM/Qemu/...?)
    guest-ms : Total millisecs spent by task+children in virt. machine. (KVM/Qemu/...?)
    %wait    : Percentage of CPU spent by the task while waiting to run.
    %CPU     : Total percentage of CPU time used by the task. [[}]]
           Total  number  of milliseconds spent by the task and all its children in virtual
           machine (running a virtual processor).
 -v Report some kernel tables.  [[monitoring.jobs.kernel]]
 -w Report task switching activity: [[profiling.jobs.kernel]]       [[{profiling.jobs.kernel]]
    cswch/s  : Total number of voluntary context-switches per sec.
                   voluntary == task blocks waiting for a resource.
    nvcswch/s: Total number of non-voluntary context switches per sec.
               non voluntary == task executes for the duration of its time slice and then
               is forced to relinquish the processor. [[}]]

  ENV.VARS:
  S_COLORS := never|always|auto (display statistics in color)

EXAMPLES
 $ pidstat 2 5   <··· Dump 5-reports of CPU-statistics for every active task at 2-secs intervals.
 $ pidstat -r -p 1643 2 5
              Display five reports of page faults and memory statistics for PID 1643  at  two  second
              intervals.
 $ pidstat -C "fox|bird" -r -p ALL
              Display  global  page  faults and memory statistics for all the processes whose command
              name includes the string "fox" or "bird".

 $ pidstat -T CHILD -r 2 5
              Display five reports of page faults statistics at two second intervals  for  the  child
              processes  of  all  tasks  in the system. Only child processes with non-zero statistics
              values are displayed.
[[}]]

## iostat(CPU, I/O, FS)  [[{MONITORING.I/O,MONITORING.STORAGE,01_PM.TODO]]
@[https://linux.die.net/man/1/iostat]
$ iostat -xt 2  # -x Show extended statistics
                # -t Print time
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           4.27    0.00    4.27    2.26    0.00   89.20

Device  r/s  w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await  w_await aqu-sz rareq-sz wareq-sz svctm %util
sda    0.00 5.50  0.00 36.00   0.00   5.00  0.00 47.62    0.00    17.55   0.09     0.00     6.55  0.82  0.45
sdb    0.00 0.00  0.00  0.00   0.00   0.00  0.00  0.00    0.00     0.00   0.00     0.00     0.00  0.00  0.00
dm-0   0.00 3.00  0.00 12.00   0.00   0.00  0.00  0.00    0.00   123.00   0.37     0.00     4.00  0.50  0.15
dm-1   0.00 0.00  0.00  0.00   0.00   0.00  0.00  0.00    0.00     0.00   0.00     0.00     0.00  0.00  0.00
dm-2   0.00 7.50  0.00 30.00   0.00   0.00  0.00  0.00    0.00     0.47   0.00     0.00     4.00  0.40  0.30
       ^^^^ ^^^^  ^^^^ ^^^^^   ^^^^   ^^^^  ^^^^  ^^^^    ^^^^     ^^^^   ^^^^     ^^^^     ^^^^  ^^^^  ^^^^
       read                                            r_await  r_await  aqu-sz                  ignore elapsed
       req.                                            avg msec avg msec avg                     to be  time %
                                                       for read for read queue                   remove during
                                                       requests requests length                         which
                                                                         of req                         I/O req
                                                                         issued                         were
                                                                                                        issued
                                                                                                        BANDWIDTH
                                                                                                        USSAGE
[[}]]

## blktrace (block I/O traffic) [[{monitoring.storage.blocks]] (man 8 blktrace)
  generate traces of the i/o traffic on (low-level) block devices vs (high-level) File Systems.
 $ sudo blktrace -d /dev/sda -o - | blkparse -i -   <··· trace i/o for /dev/sda
                                                         Alt. using btrace script:
                                                         $ btrace /dev/sda

 $ blktrace /dev/sda /dev/sdb          <··· trace /dev/sda+/dev/sdb to current dir.
 $ blkparse sda sdb                    <··· View saved trace

SYNOPSIS
       blktrace -d dev [ -r debugfs_path ] [ -o output ] [ -w time ] [ -a action ] [ -A action_mask ] [ -v ]

DESCRIPTION
OPTIONS
    -a/--set-mask=hex-mask :  Set filter mask to hex-mask (see below for masks)
                              barrier : barrier attribute
                              complete: completed by driver
                              discard : discard / trim traces
                              fs      : requests
                              issue   : issued to driver
                              pc      : packet command events
                              queue   : queue operations
                              read    : read traces
                              requeue : requeue operations
                              sync    : synchronous attribute
                              write   : write traces
                              notify  : trace messages
                              drv_data: additional driver specific trace

       --act-mask=mask     : Add mask to current filter (see below for masks)
       --buffer-size=size  : Specifies (1024) buffer size for event extraction. Def 512KiB.
       --dev=dev           : Adds dev as a device to trace
       --input-devs=file   : Adds the devices found in file as devices to trace
       --num-sub-buffers=X : number of buffers to use. Def: 4 sub buffers.
       --listen            : Run in network listen/server mode.
       --host=hostname     : Run in network client mode
       --port=number       : Network port to use (default 8462)
       --no-sendfile       : Make the network client NOT use sendfile() to transfer data
       --output=basename   : Specifies base name for input files. Default is device.blktrace.cpu.
                             Specifying -o - runs  in  live mode with blkparse (writing data to standard out).
       --output-dir=dir    : Prepend file to output file name(s)
       --relay=rel-path    : Specifies debugfs mount point
       --version
       -w seconds
       --stopwatch=seconds


  REQUEST TYPES:
  - file system  : normal read/write op from a specific disk location at a given size.
                   typically originating from a user process
  - SCSI commands: blktrace sends the command data block as a payload so that blkparse can decode it.
[[}]]

## powertop  [[{monitoring.hardware]]
Allows to:
  - diagnose device/CPU power consumption issues
  - Tune/control device/CPU power management.

$ sudo powertop # ← Interactive mode if no other option is provided
$ sudo powertop --auto-tune  # ← Callibrate non-interactively
       ^^^^^^^^^^^^^^^^^^^^
       To enable at system boot add next systemd Unit:
       *STEP 1: Create/Edit powertop.service like:*
       $ sudoedit /etc/systemd/system/powertop.service
       (Add next lines)
     + [Unit]
     + Description=Powertop auto-tune
     +
     + [Service]
     + ExecStart=/usr/bin/powertop --auto-tune
     + RemainAfterExit=true
     +
     + [Install]
     + WantedBy=multi-user.target

       *STEP 2: Enable the new service like:*
       $ sudo systemctl daemon-reload
       $ sudo systemctl enable powertop
       $ sudo systemctl start powertop

       *STEP 3: Check it has run properly*
       $ sudo journalctl -u powertop
     → ...
     → systemd[1]: Started Powertop auto-tune.
     → powertop[4778]: modprobe cpufreq_stats failedLoaded 0 prior measurements
     → powertop[4778]: RAPL device for cpu 0
     → powertop[4778]: RAPL Using PowerCap Sysfs : Domain Mask d
     → powertop[4778]: RAPL device for cpu 0
     → powertop[4778]: RAPL Using PowerCap Sysfs : Domain Mask d
     → powertop[4778]: Devfreq not enabled
     → powertop[4778]: glob returned GLOB_ABORTED
     → powertop[4778]: *Leaving PowerTOP*

OTHER PERTINENT OPTIONS:
--calibrate    :  Runs  in  calibration  mode: When running on battery,
                 powertop can track power consumption as well as system
                 activity.
                  When there are enough measurements, powertop can start
                 to report  power  estimates.
-csv=file      : Generate CSV report.
-html=file     : Generate an HTML report.
-extech=$USBDEV: Use Extech Power Analyzer for analysis
                 USBDEV will be a USB adaptor similar to /dev/ttyUSB0
-iteration=$Num : Number of times to run each test.
-time[=seconds] : Generate report for specified number of seconds.
-workload=file  : Execute workload file as a part of calibration
....
[[}]]



## perf (kernel counters) [[{monitoring.kernel.counters,profiling.jobs,dev_stack.java,profiling.locks,01_PM.TODO]]
TODO @[http://www.brendangregg.com/perf.html]
TODO @[http://www.brendangregg.com/flamegraphs.html]
- Perf: new kernel-based subsystem that provide a framework for all things performance analysis.
- It covers hardware level (CPU/PMU, Performance Monitoring Unit)
  features and software features (software counters, tracepoints) as well.

$ perf stat           <- -gather perf-counter stats. @[https://linux.die.net/man/1/perf-stat]
     [--event=EVENT]  ← PMU event in the form:
                        - symbolic event name (perf list to list)
                        - raw PMU event (eventsel+umask) with format
                          rNNN where NNN is an hexadecimal event descriptor
     [--no-inherit]   ← child tasks do not inherit counters
     [--all-cpus]
     [--pid=$pid]     ← comma separated list of existing processes
     [--tid=$tid]     ← comma separated list of existing thread id
     [--scale]        ← scale/normalize counter values
     [--repeat=n]     ← repeat command, print average + stddev (max: 100)
     [--big-num]      ← print large numbers with thousands-local-separator
     [--cpu=]         ← comma separated list of cpus (All if not provided)
     [--no-aggr]      ← Do not aggregate counts across all monitored CPUs
                        in system-wide mode (-a). Only valid in system-wide mode.
     [--null]         ← don't start any counters
     [--verbose]      ← show counter open errors, etc,...
     [--field-separator SEP]
     [--cgroup name]  ← monitor only the container (cgroup) called "name".
                        - Only available in per-cpu mode. The cgroup filesystem
                        must be mounted. All threads belonging to container "name"
                        are monitored when they run on the monitored CPUs.
                        - Multiple cgroups can be provided. Each cgroup is applied
                        to the corresponding event, i.e., first cgroup to first event,
                        - It is possible to provide an empty cgroup
                          (monitor all the time) using, e.g., -G foo,,bar.
                        - Cgroups must have corresponding events, i.e., they always
                          refer to events defined earlier on the command line.
     [--output file]
     [--append]
     [--log-fd]      ← append to given fd instead of stderr.
     (-)
     <command> [<options>]
     ^^^^^^^^^
     Any command you can specify in a shell.

*example  *
$*perf stat*  *--* make -j # ← the command following the  *--* will be traced
                   ^^^^^^^
                 traced command
(Output will be similar to)
→ Performance counter stats for 'make -j':
→
→ 8117.370256  task clock ticks     #      11.281 CPU utilization factor
→         678  context switches     #       0.000 M/sec
→         133  CPU migrations       #       0.000 M/sec
→      235724  pagefaults           #       0.029 M/sec (page faults)
→ 24821162526  CPU cycles           #    3057.784 M/sec
→ 18687303457  instructions         #    2302.138 M/sec
→   172158895  cache references     #      21.209 M/sec
→    27075259  cache misses         #       3.335 M/sec
→
→ Wall-clock time elapsed:   719.554352 msecs

*example  *
$*perf stats*-r 4 --event=cycles:{k,u} -- make -j
             ^^^^          ^     ^^^^^
             repeat        │  split into
             4 times       │ kernel/userspace
...                        │
123,123,11  cycles:k       │
  1,123,11  cycles:u       │
                           │
0.00013913  secs elapsed   │
                           │
                    - '$*perf list*' will show all predefined
                      events (cycles, cache-misses, ...) organized
                      by hardware/software/tracepoint

*PROFILING*
-*CPU Profiling*
@[https://linux.die.net/man/1/perf-top]
  $ perf top
  Samples: 12K of event 'cycles:ppp', Event count (approx.): 54543453553535
Overhead  Shared Object              Symbol
13.11%    libQT5Core.so.5.7.0        [.] QHashData:NextNode
 5.11%    libc-2.24.so               [.] _int_malloc
 2.90%    perf                       [.] symbols__insert
...

• https://github.com/flamegraph-rs/flamegraph
  perf+dtrace flamegraph generator with additional support for Cargo projects! (Rust based)
  It can be used to profile anything, not just Rust projects!

-*Sleep-time Profiling*
 See also @[#latencytop]

- Syscall Pofiling:
  $ perf trace --durtion=100
   340.448 (1000.122 ms): JS Watchdog/15221 futex(uaddr: 0x7f3e9cd1a434, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f3eae487df0, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
   119.549 (1221.529 ms): tmux: server/2501 poll(ufds: 0x55edaa47c990, nfds: 11, timeout_msecs: 12189)            = 1
   395.984 (1000.133 ms): tuned/19297 futex(uaddr: 0x7f37a4027130, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f37aad37e30, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
   691.446 (1000.105 ms): JS Watchdog/15347 futex(uaddr: 0x7f6c829550b0, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f6c942a0df0, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
   755.478 (1000.029 ms): Timer/15227 futex(uaddr: 0x7f3eb5b5cc80, op: WAIT|PRIVATE_FLAG, utime: 0x7f3e9c2c1a60) = -1 ETIMEDOUT (Connection timed out)
   755.609 (1000.017 ms): Web Content/15215 poll(ufds: 0x7f3e9bd04760, nfds: 3, timeout_msecs: 4294967295)        = 1
   311.581 (1527.461 ms): Gecko_IOThread/15157 epoll_wait(epfd: 8<anon_inode:[eventpoll]>, events: 0x7f3d6d1f5200, maxevents: 32, timeout: 4294967295) = 1
   311.955 (1527.194 ms): firefox/15132 poll(ufds: 0x7f3d1ebd5610, nfds: 5, timeout_msecs: 4294967295)        = 1
   876.905 (1000.146 ms): dockerd/32491 futex(uaddr: 0x561e1da0b920, utime: 0xc42045bed8)                     = -1 ETIMEDOUT (Connection timed out)
   877.069 (1000.064 ms): dockerd/27832 futex(uaddr: 0x561e1da07950, utime: 0x7f50e7c61b90)                   = 0
   877.025 (1000.145 ms): dockerd/27904 futex(uaddr: 0xc420c82548)                                            = 0
   912.964 (1000.133 ms): JS Watchdog/15158 futex(uaddr: 0x7f3d57c4c0f0, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f3d65a41df0, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
   311.586 (1607.337 ms): Chrome_~dThrea/15346 epoll_wait(epfd: 11<anon_inode:[eventpoll]>, events: 0x7f6c9b9cd080, maxevents: 32, timeout: 4294967295) = 1
   937.245 (1000.102 ms): JS Watchdog/15276 futex(uaddr: 0x7feca361bbf0, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7fecb4e27df0, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
   214.944 (1927.025 ms): Timer/15164 futex(uaddr: 0x7f3d6d165be0, op: WAIT|PRIVATE_FLAG, utime: 0x7f3d542a1a60) = -1 ETIMEDOUT (Connection timed out)
   215.042 (1927.063 ms): Socket Thread/15166 poll(ufds: 0x7f3d539028f0, nfds: 8, timeout_msecs: 4294967295)        = 1
  1340.624 (1000.072 ms): JS Watchdog/15221 futex(uaddr: 0x7f3e9cd1a434, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f3eae487df0, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
  1396.377 (1000.131 ms): tuned/19297 futex(uaddr: 0x7f37a4027130, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f37aad37e30, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
  1691.606 (1000.059 ms): JS Watchdog/15347 futex(uaddr: 0x7f6c829550b0, op: WAIT_BITSET|PRIVATE_FLAG|CLOCK_REALTIME, utime: 0x7f6c942a0df0, val3: MATCH_ANY) = -1 ETIMEDOUT (Connection timed out)
  1877.200 (1000.115 ms): dockerd/27844 futex(uaddr: 0x561e1da0b9a0, utime: 0xc420460ed8)                     = -1 ETIMEDOUT (Connection timed out)
   876.826 (2000.665 ms): dockerd/27840 futex(uaddr: 0xc4206d7148)                                            = 0
  1877.252 (1000.149 ms): dockerd/27832 futex(uaddr: 0x561e1da07950, utime: 0x7f50e7c61b90)                   = 0
  1877.190 (1000.239 ms): dockerd/27904 futex(uaddr: 0xc420c82548)                                            = 0
  1877.189 (1000.372 ms): dockerd/32491 futex(uaddr: 0xc420c83948)                                            = 0


- record command's profile into perf.data:
  https://linux.die.net/man/1/perf-record

- display recorded perf.data:
  https://linux.die.net/man/1/perf-report

- General framework for bench.suites:
  https://linux.die.net/man/1/perf-bench

- Analyze lock events:                   [[{profiling.locks]]
  https://linux.die.net/man/1/perf-lock  [[}]]

• Example Profiling with Perf (FB 60 TB+ input data)              [[{]]
Extracted from:
  "Apache Spark @Scale: A  production use case"
@[https://engineering.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case/]
  ...  Tools we used to find performance bottleneck
  - Spark Linux Perf/Flame Graph support: Although the two tools
    above are very handy, they do not provide an aggregated view of CPU
    profiling for the job running across hundreds of machines at the same
    time. On a per-job basis, * we added support for enabling Perf *
   *profiling (via libperfagent for Java symbols) and can customize the *
   *duration/frequency of sampling. The profiling samples are aggregated*
   *and displayed as a Flame Graph across the executors using our       *
   *internal metrics collection framework.                              * [[}]]
[[{monitoring.kernel.counters}]]

#[latencytop]
## (x)latencytop [[{profiling.locks,troubleshooting.jobs,QA.UX]]
@[https://linux.die.net/man/8/xlatencytop]
- aimed at:
  - identifying and visualizing where (kernel and userspace) latencies are happening
  - What kind of operation/action is causing the latency

- LATENCYTOP FOCUSES ON THE CASES WHERE THE APPLICATIONS WANT TO RUN
  AND EXECUTE USEFUL CODE, BUT THERE'S SOME RESOURCE THAT'S NOT
  CURRENTLY AVAILABLE (AND THE KERNEL THEN BLOCKS THE PROCESS).

- This is done both on a system level and on a per process level,
  so that you can see what's happening to the system, and which
  process is suffering and/or causing the delays.

$ sudo latencytop          <- Press "s" + "letter" to display active processes
                              starting with that lettter.
                              Press "s" followed by 0 to remove the filter

NOTE: See also notes about disk write-read pending queue.

NOTE: There are newer alternatives based on BPF:
      http://www.brendangregg.com/blog/2016-10-27/dtrace-for-linux-2016.html

NOTE: See also the more advanced TimeChart:
    @[http://www.linux-magazine.com/Online/News/Timechart-Zoom-in-on-Operating-System]
    @[http://blog.fenrus.org/?p=5]
[[}]]

[[monitoring.kernel}]]

## SystemTap (stap) [[{monitoring.jobs,troubleshooting,01_PM.TODO]]
- C&P from https://lwn.net/Articles/852112/:  [[{02_doc_has.comparative]]
  SystemTap has been available since 2005, while bpftrace is a more
  recent contender that, to some, may appear to have made SystemTap
  obsolete. However, SystemTap is still the preferred tool for some
  real-world use cases. [[}]]

@[https://en.wikipedia.org/wiki/SystemTap]
  - tool to perform live analysis of running program (dynamic instrumentation).
  - It can interrupt normal control flow and execute code specified by a SystemTap script,
    temporarily modifying the program at runtime.
  - Use-cases:
    - extract, filter, summarize complex performance|functional diagnosys data.

  - Contributed by:
    - Red Hat/IBM, Intel, Hitachi, Oracle, community.

  See real example:
  @[https://developers.redhat.com/blog/2019/07/24/probing-golang-runtime-using-systemtap/]
[[}]]

## latrace LD audit [[{monitoring.jobs,monitoring.libc,troubleshooting,01_PM.TODO]] (man 1 latrace
- LD_AUDIT 2.4+ libc frontend

$ latrace [-ltfsbcCpADaoyIiBdvTFELVh] command [arg ... ]
  Run command and display its dynamic library calls using
  a LD_AUDIT libc feature (available from libc version 2.4 onward - see the
  section called "DISCUSSION" ). It is also capable to measure and display
  various statistics of dynamic calls.

  If the config file is provided, latrace will display symbol's arguments with
  detailed output for structures. The config file syntax is similar to the C
  language, with several exceptions.

  The latrace by default fully operates inside of the traced program. However
  another pipe mode is available, to move the main work to the latrace binary
  (see the section called "PIPE mode").
[[}]]

[[linux.101.process_model}]]

# Network  [[{network]]
## NetworkManager [[{configuration.network,troubleshooting.network]]
@[https://www.redhat.com/sysadmin/becoming-friends-networkmanager]
@[https://linux.die.net/man/8/networkmanager]
@[https://linux.die.net/man/5/networkmanager.conf]
@[https://linux.die.net/man/1/nmcli]
@[https://linux.die.net/man/8/networkmanager_selinux]

- widespread network configuration daemon
- Managed through cli (nmcli), text-GUI (nmtui) GUI (GNOME,...)
  files, web-console (Cockpit) or D-Bus interface.
  APIs and a library (libnm) is also provided.
-*NetworkManager allows users and applications to retrieve *
 *and modify the network's configuration at the same time, *
 *ensuring a consistent and up-to-date view of the network.*

- NetworkManager philosophy:
  "...attempts to make networking configuration and operation as
    painless and automatic as possible..."
  When there is partial or no configuration, NetworkManager checks
  the available devices and tries its best to provide connectivity
  to the host.

- NetworkManager allows advanced network administrators to
  provide their own configuration.

*NetworkManager Entities*
-*device*    : represents a network interface ("ip link")
               A NetworkManager devices tracks:
             - If it is managed by NetworkManager
             - The available connections for the device
             - The connection active on the device (if any)
-*connection*: represents the full configuration to
               be applied on a device and is just a list of
               properties.
               Properties belonging to the same configuration
               area are grouped into settings:
               Example:
             - ipv4 setting group:
               - addresses
               - gateway
               - routes

^^^^^^^^^^^^^^
 *NETWORK SETUP == activate a connection with a device*

$ nmcli device   # ← list the devices detected by NetworkManager
(output will be similar to)
→ DEVICE   TYPE      STATE           CONNECTION
→ enp1s0   ethernet  connected       ether-enp1s0
→ enp7s0   ethernet  disconnected    --
→ lo       loopback  unmanaged  --

$ nmcli device \        # ← turn off management
  set enp1s0 managed no     for enp1s0 device
                            (change is not persisted
                             and ignored on reboot)

$ nmcli   # List detailed connections for devices
enp1s0: connected to enp1s0
      "Red Hat Virtio"
      ethernet (virtio_net), 52:54:00:XX:XX:XX, hw, mtu 1500
      ip4 default
      inet4 192.168.122.225/24
      route4 0.0.0.0/0
      route4 192.168.122.0/24
      inet6 fe80::4923:6a4f:da44:6a1c/64
      route6 fe80::/64
      route6 ff00::/8
...


$ nmcli connection # ← list the available connections
→ NAME         UUID          TYPE       DEVICE
→ ether-enp1s0 23e0d89e-...  ethernet   enp1s0
→ ...


, i.e., to deconfigure the associated device, just instruct NetworkManager to put the connection down. For instance, to deactivate the ether-enp1s0 connection:

$ nmcli connection \  # ← deactivate connection
    down ether-enp1s0     (deconfigure associated
                          device)

$ nmcli connection \  # ← Reactivate
    up ether-enp1s0

$ nmcli connection \  # ← Show connection details
  show ether-enp1s0
*(man nm-settigs for full info about available parameters)*

connection.id:                  *ether-enp1s0*   ← human readable name
connection.uuid:                 23e0d89e-...
connection.stable-id:            --
connection.type:                 802-3-ethernet ← ethernet, wifi, bond!!, vpn, ...
connection.interface-name:       enp1s0         ← binds(restrict) to specific device
connection.autoconnect:          yes
...
ipv4.method                      auto           ← one of:
                                                  auto(DHCP)
                                                  manual(static IP in ipv4.addresses),
                                                  disabled, link local, shared
ipv4.addresses                   192.168.1.201/24
ipv4....
dhcp4.option[1]                  broadcast_address = 192.168.1.255
dhcp4....
[...]

$ nmcli connection \               ← Permanently change the connection
    modify  *ether-enp1s0* \
    ipv4.method manual
    ipv4.addresses 10.10.10.1/24 \
    ipv4.gateway 10.10.10.254 \
    ipv4.dns 10.10.10.254
$ nmcli connection up ether-enp1s0 ← New settings will only be applied
                                     on connection (re)activation

$ nmcli connection \            # ← avoid activation by NetworkManager
    modify ether-ens1s0 \           (you would have to activate manually)
    connection.autoconnect no

$ nmcli con add \            ← Create new connection
  type ethernet \              - DHCP will be used if no IPv4 config
  ifname enp0s1 \                is provided.
  con-name  *enp0s1_dhcp* \      (ipv4.method defaults to auto)
  autoconnect no               - Run interactively with '--ask' option
$ nmcli con                  ← Verify new connection
→  NAME         UUID         TYPE     DEVICE
→  ...
→ *enp0s1_dhcp* 64b499cb-... ethernet --
→  ...

$ nmcli con edit   ← Interactive editor-mode with inline help will open


TODO:
- "Many features deserve separate blog posts"
  - dispatcher scripts
  - connectivity checkers
  - split DNS
  - MAC address randomization
  - hotspot configuration
  - automatic configuration.

*Check Network status* @[https://linux.die.net/man/1/nm-online]
~:$ nm-online -q   ←     --timeout=10       (Defaults to 30secs)
~:$ echo $?              --exit             Exit immediately if nm is not running or connecting
→ 0                      --quiet            Don't print anything
                         --wait-for-startup Wait for nm startup instead of a connection

### Network Manager: Troubleshooting

*bypass network manager* by manually launching dhcp client:
  $ sudo dhclient eth0 # execute 'ip link' to show
                  └┬─┘   │   all network devices
                   └─────┘

 *ERROR*: Network disconnects after a few seconds:
Try disabling ModemManager service:
$ sudo systemctl stop    ModemManager.service
$ sudo systemctl disable ModemManager.service
[[}]]

## Network audit/control [[{monitoring.network,troubleshooting]]
• Socket Statistics ss: (replaces 'netstat'),  man 8 ss

 $ sudo ss -ntlp     <···-n : Do not resolve IP to DNS (much) faster.
                         -t : Show tcp (filter out UNIX, UDP, ...)
                         -l : Show socket waiting for (client) connetions
                         -p : Show process PID that controls the socket.
  Output example:
  State   Recv-Q Send-Q  Local       Peer
                         Addr:Port   Addr:Port
  LISTEN  0      128     *:80        *:*      users:(("lighttpd",pid=23515,fd=4))
  LISTEN  0      128     *:22        *:*      users:(("sshd",pid=571,fd=3))

 $ sudo ss -t -a -p  <··· Display all (-a) connected (vs -l/listening) TCP (-t)
                          sockets and processes using them (-p)
  STATE       ... ADDRESS:PORT        PEER ADDRESS:PORT
  ESTABLISHED ... 127.0.0.1:postgres  127.0.0.1:46404     users:(("postgres",pid=64032,fd=11))
  ESTABLISHED ... 127.0.0.1:37200     127.0.0.1:50004     users:(("sshd",pid=61411,fd=10))
  ESTABLISHED ... 127.0.0.1:postgres  127.0.0.1:45086     users:(("postgres",pid=43553,fd=11))
  TIME_WAIT   ...

 $ ss \               <··· Display all established ssh connections.
   -o state established
   '( dport = :ssh or sport = :ssh )'


 $ ss -x src /tmp/.X11-unix/* <·· Find all local processes connected to X server.


 $ ss \                                <····  List tcp sockets in state FIN-WAIT-1
   -o state fin-wait-2 \                      for http/https@193.233.7/24
  '( sport = :http or sport = :https )' \     and look at their timers.
  dst 193.233.7/24*


 $ ip route list     <····· DISPLAY IP ROUTING TABLE
  *default via 10.0.0.1 dev eth0*
  10.0.0.0/24     dev eth0    proto kernel scope link src 10.0.0.5
  169.254.0.0/16  dev eth0                 scope link metric 1002
  172.17.0.0/16   dev docker0 proto kernel scope link src 172.17.0.1
  168.63.129.16   via 10.0.0.1 dev eth0 proto static
  169.254.169.254 via 10.0.0.1 dev eth0 proto static
  ...

 $ ip link         <···· DISPLAY IP DEVICE STATUS
1: lo:     <LOOPBACK,           UP,LOWER_UP> ...  state UNKNOWN mode DEFAULT ...
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0:   <BROADCAST,MULTICAST,UP,LOWER_UP> ...  state UP      mode DEFAULT ...
    link/ether    00:0d:3a:26:bb:2b brd ff:ff:ff:ff:ff:ff
...

• SNIFF network traffic with tcpdump:
$ sudo tcpdump \    "snif" IP traffic
   -i eth0     \     <- in network interface eth0
   port 8090   \     <- filter by IP packets to/from port 8090
   -n          \     -n: Do not convert IPs to host-names (Avoiding slow dns reverse lookups)
   -A                -A: Show in text/ASCII format
                     Alternatively write to a file with flag:
                     -w output.tcpdump
                     This file can then be replay again tcpdump or with the
                     more advanced Wireshark.

• WireShark: [[{qa.UX]]
  - world’s most popular and widely-used open-source and cross-platform
    network protocol analyzer.
  - Previously known as Ethereal.
  - Latest version is 4.0. (2022-10)
  - Features include:
    - powerful display filter syntax with support and wizard dialogs.
      (for example to filter just traffic to a port, dump in ASCII the
      HTTP traffic from client to server, ...)
    - GUI views for Packet Detail, Packet Bytes
    - Hex dump imports
    - MaxMind geolocation.
    - Support for AT_NUMERIC address type (v4+) that allows simple
      numeric addresses for protocols that don’t have a more
      common-style address approach,
    - JSON mapping for Protobuf messages (v4)
    - extcap passwords support in tshark and related cli tools.
    - DVB Selection Information Table (DVB SIT), (v4)
    - gRPC Web (gRPC-Web) (v4)
    - SSH File Transfer Protocol (SFTP) (v4)

  REF: https://9to5linux.com/wireshark-4-0-released-as-worlds-most-popular-network-protocol-analyzer
[[}]]

 $ sudo traceroute \        <··· (attempts to) show IP ("hop" path) route
   "destination_IP_or_host"      from source to destination for an IP packet.


*Examine open ports in remote machine*
$ sudo nmap -nt remote_machine    ← (try to) query remote machine for open ports
[[monitoring.network}]]

## basic network traffic shaping [[{monitoring.network,troubleshooting.network]]
*Network interface shaping with Wondershaper*
@[https://github.com/magnific0/wondershaper/]
$ sudo ./wondershaper -a eth0 -u 4096 -d 8192 ← limit upload: 4Mbps, download:8Mbs

*Process network shaping with Firejail*
@[https://www.pcsuggest.com/bandwidth-traffic-shaping-in-linux-with-firejail/]
$ firejail  --net=enp2s0 firefox [/bash]
$ firejail --list | grep 'firefox' | awk -F: '{print$1}'
$ firejail  --bandwidth=PID set interface-name down-speed up-speed
[[}]]

## TCP/IP Utilities [[{linux.101,configuration.network,]]
$ *host (ip_address|domain_name)*  ← Performs lookup of an internet address (using the Domain Name System, DNS). Simply type:
or
**
$ *dig*    www.amazon.com  ←        query to DNS
$ dig -x 10.10.10.10     ← revers query to DNS
(check man page for more options)

$ *wget* www.myDomain.com/myPage  ← HTTP client
    Options:
    -m: archive/"m"irrow single web-site
    -nc: (no clobber) avoid overwriting local files

$   wget --spider \    ← parse bookmarks.html for links
    --force-html \
    -i bookmarks.html
(see man page for more info)

$ *curl*  ← Script oriented HTTP client.
          It can access dictionary servers (dict),
          ldap servers, ftp, http, gopher, ...

$ curl -M : To access the full/huge manual
$ curl -u username:password http://www.placetodownload/file
[[}]]

## Nethogs: bandwidth "top" per ps [[{monitoring.network,monitoring.jobs,troubleshooting.network]]
- Nethogs is a command line utility for linux that displays the network
  bandwidth used by each application or process in realtime.

$*$ sudo nethogs                                                                   *
  ...
    PID USER     PROGRAM                      DEV        SENT      RECEIVED
  2367  enlighten/opt/google/chrome/chrome    eth0       3.341      20.948 KB/sec
  2196  enlighten/usr/lib/firefox-7.0.1/fire  eth0       0.871       0.422 KB/sec
  3723  enlighten/usr/bin/pidgin              eth0       0.028       0.098 KB/sec
  2206  enlighten/usr/bin/skype               eth0       0.033       0.025 KB/sec
  2380  enlighten/usr/lib/chromium-browser/c  eth0       0.000       0.000 KB/sec
  0     root     unknown TCP                             0.000       0.000 KB/sec

    TOTAL                                                4.274      21.493 KB/sec"

- TODO:
  @[https://www.binarytides.com/linux-commands-monitor-network/]
[[}]]
[[network}]]

# Remote access  [[{security.remote_access]]
## ssh (text console)  [[{security.aaa,security.audit.user]]
  @[https://en.wikipedia.org/wiki/Secure_Shell]:
  standard way to access Linux&Other OSes remotely.
  A running "sshd" (ssh daemon) must be instaled and running
  on the remote machine. Used also for tunneling TCP connections.

 *Quickly connect to remote machine runnin sshd*
  (Remote machine must allow passwords login)
  $ ssh myUser@myRemoteMachine
 → myUser@mYRemoteMachine's password:
  (enter password to log-in to text terminal)

  ...
  Example ~/.ssh/config
  01 Host remoteHostAlias1 alias2 ...     ← allows to ssh alias2
  02    HostName 10.230.11.10
  03 #  ProxyCommand /usr/bin/corkscrew 10.10.10.10 8080 %h %p  ← "bypass firewall through HTTP proxy
  04    Port 12345                        ← 22 by default
  05    User myRemoteUser                 ← defaults to current active local user
  06    LocalForward   5555 removeIP:3333 ← Forward any local TCP conection to port 5555
                                            to port 3333 at remoteIP (as seen by sshd server)
                                            (ussually remoteIP == localhost)
  07    RemoteForward 13389 localIP:3389  ← Forward any TCP conection to port 5555 in sshd server
                                            to port 3389 in localIP (as seen by ssh client)
                                            (ussually localIP == localhost)
  08    TCPKeepAlive true                 ← "sort of ping" to avoid closing connection.



 *Passwordless authentication*
  - Useful to execute remote task automatically.
  # STEP 01: generate local private secret (key) and associated public key.
  $ ssh-keygen
  (WARN: leave passphrase blank to allow for automated task)


  # STEP 02: Copy associated public key to remote machine
  #          at  /home/myRemoteUser/.ssh/authorized-keys
  $ ssh-copy-id myRemoteUser@myRemoteMachine

  Now it must be possible to ssh into the remote machine with no password
  $ ssh myRemoteUser@myRemoteMachine

  Troubleshooting passwordless access:
  local  $ chmod go-rw ~/.ssh/*  # Fix permission. ssh is paranoid about it.
  remote $ chmod go-rw ~/.ssh/*  # Fix permission. ssh is paranoid about it.

 *Control ssh*

 ssh ... -S ${CONTROL_SOCKET} ... host1
         └────────┬─────────┘
         Later on we can just
         ┌────────┴─────────┐
 ssh     -S ${CONTROL_SOCKET} -O $ctr_cmd
                                 ^^^^^^^^
                                 check   : check master process is running.
                                 forward : request forwardings without command execution.
                                 cancel  : cancel forwardings
                                 exit    : request master to exit
                                 stop    : request the master to stop accepting further multiplexing requests.

    if [[ $6 == "stop" ]] ; then
       TUNNEL="${TUNNEL} -O exit "
    fi
    TUNNEL="${TUNNEL} $4@$2"
    $TUNNEL 1>/dev/null
}

  ssh client and daemon have a wide range of useful options, including
  reconnection, tunneling, HTTP-sock-proxy-establishment, ...
  See 'man 1 ssh' and 'man 8 sshd' for details.

  Related:
@[https://www.reddit.com/r/linuxadmin/comments/b9c7lw/using_a_yubikey_as_smartcard_for_ssh_public_key/]
[[}]]

## VNC Remote Desktop [[{security.aaa,security.remote_access,]]
- The VNC protocol allows to launch a graphical system in a
  remote system using the VNC server and access it remotely
  using the VNC clients.

- Running programs in the remote machine will "draw" to a
  local memory buffer shared by the VNC server.
  When a remote VNC client connects, the VNC server will
  transmit to the client the graphic buffer and finally
  the VNC client will show the buffer in the local display.
- Modern VNC client←→server protocols are highly optimized
  to save bandwith and allows for high-resolution displays
  with around 1Megabit of bandwidth.

- There is no limit to the number of remote VNC servers that
  can run in parallel, just limited by memory access.
  Next links offers an example of how to run many different
  VNC servers (1 or more desktops per user) in a single VNC server:
@[https://github.com/earizon/tigervnc_remote_desktop]
[[}]]

#[knockd_summary]
## Knockd ("Invisible Linux")  [[{security.aaa,security.remote_access]]
@[https://www.maketecheasier.com/make-linux-server-invisible-knockd/]
 REF(Michael Aboagye @ MakeTechEasier.com)
  - Knockd allows to  listen to traffic on an network interface/port
    waiting first for special sequences of port-hits.
  - clients (telnet, socat, ...) initiate port-hits by sending a TCP
     or packet to a port on the server.

*PRE-SETUP*
- Install and Configure Iptables
  $ sudo apt-get install iptables iptables-persistent
                                  ^^^^^^^^^^^^^^^^^^^
                                  takes over automatic
                                  loading of saved tables


* Knockd Install*
$ sudo apt-get install knockd  # apt like
$ sudo dnf     install knockd  # rpm like

*hide ssh service until "Knocked"*
$ iptables -A INPUT \
        -m conntrack \                        STEP 1
        --ctstate ESTABLISHED, RELATED \    ← Allow established/current
        -j  ACCEPT                          ← Allow

$ iptables  -A  INPUT \                       STEP 2
        -p tcp  --dport  22 \               ← block incomming con. to 22 (SSH)
        -j  REJECT

                                              STEP 3
$ netfilter-persistent save                 ← s save the firewall rules
$ netfilter-persistent reload


Configure  Knockd
$ sudo "vim" /etc/knockd.conf
  | [options]
  |   UseSyslog
  |
  | [openSSH]
  |   sequence = 7000,8000,9000
  |   seq_timeout = 5
  |   command = /sbin/iptables -A INPUT -s %IP% -p tcp --dport 22 -j ACCEPT
  |   tcpflags = syn
  |
  | [closeSSH]
  |   sequence = 8000,9000,7000
  |   seq_timeout = 5
  |   command = /sbin/iptables -D INPUT -s %IP% -p tcp --dport 22 -j ACCEPT
  |   tcpflags = syn

- command will be executed once the client-sequence  is recognised.
- tcpflags must be set on the client "knocks"

NOTE: Adding iptables "-A" flag (appned) causes the rule to be appended
to the end of th INPUT chain, causing all remaining connections to drop.
Replace by:
  command = /sbin/iptables -I INPUT 1 -s %IP% -p tcp --dport 22 -j ACCEPT

  "-I" flag (insert) ensures that the new rule is added to the top of the
  input chain to accept ssh connections.

*Enable Knockd Service*
- Add/edit START_KNOCKD option in /etc/default/knock lo look like:
  START_KNOCKD=1
$ sudo systemctl enable knockd
$ sudo systemctl start knockd

*Testing*

$ knock -v my-server-ip  7000 8000 9000
$ ssh my-server-ip
...
$ knock -v my-server-ip 9000 8000 7000
[[security.remote_access}]]

# SYSTEM INFO [[{linux.101,monitoring.101]]

## Basic System Info
$ uptime  ← shows how long the computer has been "up" since last reboot.
            number of users and the processor load
$ date    ← current date/time
$ cal     ← display calendar
$ uname   ← print information on the system such as OS type, kernel version...
            -a --- print all the available information
            -m --- print only information related to the machine itself
            -n --- print only the machine hostname
            -r --- print the release number of the current kernel
            -s --- print the operating system name
            -p --- print the processor type

$ cat /etc/release | sort | uniq  ← Shows OS identification
                                  (Distribution, major,minor,patch,flavour, ...)
$ free    ← human-readable memory report (-g: Gigabytes)
              total        used        free      shared  buff/cache   available
Mem:        6102476      812244     4090752       13112     1199480     4984140
Swap:       2097148           0     2097148

$ getconf -ag  ← Get all system config. parameters
→ ...
→ PAGESIZE                           4096
→ ..
→ ULONG_MAX                          18446744073709551615
→ USHRT_MAX                          65535
→ ...
→ _POSIX_...
→ ...
→ LFS_CFLAGS
→ LFS_LDFLAGS
→ LFS_LIBS
→ LFS_LINTFLAGS
→ LFS64_CFLAGS                       -D_LARGEFILE64_SOURCE
→ LFS64_LDFLAGS
→ LFS64_LIBS
→ LFS64_LINTFLAGS                    -D_LARGEFILE64_SOURCE
→ ...
→ GNU_LIBC_VERSION                   glibc 2.28
→ GNU_LIBPTHREAD_VERSION             NPTL 2.28
→ POSIX2_SYMLINKS                    1
→ LEVEL1_ICACHE_SIZE                 32768
→ LEVEL1_ICACHE_ASSOC                8
→ LEVEL2_CACHE_SIZE                  262144
→ LEVEL2_CACHE_ASSOC                 8
→ LEVEL2_CACHE_LINESIZE              64
→ LEVEL3_CACHE_SIZE                  3145728
→ LEVEL3_CACHE_ASSOC                 12
→ LEVEL3_CACHE_LINESIZE              64
→ LEVEL4_CACHE_SIZE                  0
→ LEVEL4_CACHE_ASSOC                 0
→ LEVEL4_CACHE_LINESIZE              0
→ ...

$ dmidecode types    <··· Decode DMI (system board)
 0 BIOS
 1 System
 2 Baseboard
 4 Processor
 6 Memory Module
 7 Cache
11 OEM Strings
12 System Config.Options
16 Physical Memory Array
17 Memory Device
24 Hardware Security
25 System Power Controls
32 System Boot
33 64-bit Memory Error
39 Power Supply
   ...

$ sudo dmidecode -t 17  <··· Show physical mem.banks
| OUPUT PHYSICAL MACHINE                      | OUPUT VIRTUAL MACHINE (Manufacturer: QUEMU,...)
| # dmidecode 3.2                             | # dmidecode 3.0
| Getting SMBIOS data from sysfs.             | Getting SMBIOS data from sysfs.
| SMBIOS 2.7 present.                         | SMBIOS 2.8 present.
|                                             |
| Handle 0x0008, DMI type 17, 34 bytes        | Handle 0x1100, DMI type 17, 40 bytes
| Memory Device                               | Memory Device
|     Array Handle: 0x0007                    |         Array Handle: 0x1000
|     Error Information Handle: Not Provided  |         Error Information Handle: Not Provided
|     Total Width: 64 bits                    |         Total Width: Unknown
|     Data Width: 64 bits                     |         Data Width: Unknown
|     Size: 8192 MB                           |         Size: 4096 MB
|     Form Factor: SODIMM                     |         Form Factor: DIMM
|     Set: None                               |         Set: None
|     Locator: ChannelA-DIMM0                 |         Locator: DIMM 0
|     Bank Locator: BANK 0                    |         Bank Locator: Not Specified
|     Type: DDR3                              |         Type: RAM
|     Type Detail: Synchronous                |         Type Detail: Other
|     Speed: 1333 MT/s                        |         Speed: Unknown
|     Manufacturer: Samsung                   |         Manufacturer: QEMU
|     Serial Number: 939BED25                 |         Serial Number: Not Specified
|     Asset Tag: None                         |         Asset Tag: Not Specified
|     Part Number: M471B1G73DB0-YK0           |         Part Number: Not Specified
|     Rank: Unknown                           |         Rank: Unknown
|     Configured Memory Speed: 1333 MT/s      |         Configured Clock Speed: Unknown
|                                             |         Minimum Voltage: Unknown
| Handle 0x0009, DMI type 17, 34 bytes        |         Maximum Voltage: Unknown
| Memory Device                               |         Configured Voltage: Unknown
|     Array Handle: 0x0007                    |
|     Error Information Handle: Not Provided  |
|     Total Width: 64 bits                    |
|     Data Width: 64 bits                     |
|    *Size: 8192 MB            *              |
|    *Form Factor: SODIMM      *              |
|    *Set: None                *              |
|    *Locator: ChannelB-DIMM0  *              |
|    *Bank Locator: BANK 2     *              |
|    *Type: DDR3               *              |
|    *Type Detail: Synchronous *              |
|    *Speed: 1333 MT/s         *              |
|    *Manufacturer: 04CB       *              |
|     Serial Number: A8750300                 |
|     Asset Tag: None                         |
|     Part Number:                            |
|     Rank: Unknown                           |
|     Configured Memory Speed: 1333 MT/s      |
[[}]]

## vmstat RT stats for CPU/procs/mem/paging/block IO/traps
    $ vmstat [options] [delay [count]] (global stats) [[{monitoring.101,monitoring.memory,monitoring.i/o]] (man 8 vmstat)
  Display .

  The first report produced gives averages since the last reboot.
  Additional reports give information on a sampling period of length delay.
  The process and memory reports are instantaneous in either case.

 -a --active: Display active and inactive memory
              FROM: https://unix.stackexchange.com/questions/305606/linux-inactive-memory
              * active memory are pages which have been accessed "recently"
              * inactive memory are pages which have not been accessed "recently"
              """A high ratio of active to innactive memory can indicate
                 memory pressure, but that condition is usually accompanied by
                 pagin/swapping which is easier to understand"""
 -f --forks : display number of forks (fork,vfork,clone) since boot. This
 -m --slabs : Display slabinfo (memory assigned to kernel objects)
              https://en.wikipedia.org/wiki/Slab_allocation
 -s --stats : Displays a table of various event counters and memory statis‐
            tics. (without repeating)
 -d --disk  : Report disk statistics
 -D --disk-sum: Report some summary statistics about disk activity.
 -p --partition device: Detailed statistics about partition
 -S --unit  :=  1000 (k), 1024 (K), 1000000 (m), 1048576 (M) bytes.
            swap (si/so) and block (bi/bo) not affected
 -t --timestamp: Append timestamp to each line
 *-w --wide  : Wide output mode* (Recomended)

OUTPUT FIELD DESCRIPTION
*VM MODE*                                         | *DISK MODE(--disk)*
Procs                                           | Reads
  r: # of runnable procs(running|waiting for)   |   total  :   Total reads completed successfully
  b: # of processes  *in uninterruptible sleep* |   merged : grouped reads(resulting in 1 I/O)
                                                |   sectors: Sectors read successfully
Memory *1                                       |        ms: milliseconds spent reading
(inacti,cache,buff can be freed if needed)     |
  swpd  : virtual memory used                   |
  free  : idle (ready to use) memory            | Writes
  buff  : memory used as disk buffers           |     total:   Total writes completed successfully
  cache : memory used as cache                  |    merged: grouped writes (resulting in 1 I/O)
  inact : inactive memory (-a option)           |   sectors: Sectors written successfully
          (still cached for possible reuse)     |        ms: milliseconds spent writing
  active: memory Used by processes              |
                                                | IO
 Swap                                           |   cur: I/O in progress
   si: Amount of memory swapped in from disk(/s)|     s: seconds spent for I/O
   so: Amount of memory swapped to disk(/s)     +--------------------------------------------------------

IO                                              | *DISK PARTITION MODE (--partition)*
  bi: Blocks received from block device         |           reads: Total # of reads issued to part.
  bo: Blocks     sent   to block device         |    read sectors: Total read sectors for partition
                                                |          writes: Total # of writes issued to part.
System                                          |requested writes: Total # of write requests made for part
  in: interrupts per second, including the clock+---------------------------------------------------------
  cs:  *context switches per second*
                                                | *SLAB MODE (--slabs)*
CPU (percentages of total CPU time)             |   cache: Cache name
  us: Time spent running non-kernel code(user  )|     num: # of currently active objects
  sy: Time spent running     kernel code(system)|   total: Total # of available objects
  id: Time spent idle                           |    size: Size of each object
  wa:  *Time spent waiting for IO*              |   pages: # of pages with at least one active object
  st: Time stolen from a virtual machine        +---------------------------------------------------------

*All linux blocks are currently 1024 bytes.*



* 1: https://stackoverflow.com/questions/6345020/what-is-the-difference-between-buffer-vs-cache-memory-in-linux
    - buffers are associated with a specific block device,
      caching filesystem metadata(dir contents, file permissions),
      as tracking in-flight pages (what's being written
      from or read to for a particular block device).
      The Kernel tries to cache just enough buffers for predicted
      "next-reads" to block devices.

    - cache contains real application file data (file content).
      The Kernel tries to cache as much as possible until there is
      no more free memory for apps.
[[}]]

## ps_mem: Accurate mem.use [[{monitoring.memory,troubleshooting.memory]]
  REF: https://github.com/pixelb/ps_mem/:

$ sudo pip install ps_mem  <··· PIP (P)ackage (I)nstaler for (P)ython
  USSAGE:
  ps_mem [-h|--help] [-p PID,...] [-s|--split-args] [-t|--total] [-w N]
         [-d|--discriminate-by-pid] [-S|--swap]

  $ sudo ps_mem                        <··· Simple ussage:
   Private  +   Shared  =  RAM used       Program

   34.6 MiB +   1.0 MiB =  35.7 MiB       gnome-terminal
  139.8 MiB +   2.3 MiB = 142.1 MiB       firefox
  291.8 MiB +   2.5 MiB = 294.3 MiB       gnome-shell
  272.2 MiB +  43.9 MiB = 316.1 MiB       chrome (12)
  913.9 MiB +   3.2 MiB = 917.1 MiB       thunderbird
  ---------------------------------
                            1.9 GiB


  $ sudo ps_mem -p $(pgrep -d, -u $USER)  <···  Show only ps_mem for current $USER
  → ...

[[}]]

## SysV shared memory [[{linux.101,configuration.memory,monitoring.memory,]]
- SysV shared memory segments are accounted as a cache,
  though they do not represent any data on the disks.
_
#  ipcs -m  #  check size of shared-memory-segments
[[}]]
[[monitoring}]]


# BACKUPS  [[{security.backup,linux.101,storage.compression]]
• https://github.com/y-scope/clp  [[{security.audit,scalability.storage,01_PM.low_cost]]
  C&P from https://www.uber.com/en-DE/blog/reducing-logging-cost-by-two-orders-of-magnitude-using-clp/
    """CLP reduces log size by x169 ...
      CLP allows to search directly on the compressed logs .... With
    just Phase 1, this cost is reduced to $10,000/year for a one month
    retention period. But more importantly, it now enables us to retain
    the logs as our engineers requested: we were able to restore the
    logging level from WARN back to INFO, increase our retention period
    by 10x, and still reduce our storage costs by 17x."""
  [[}]]
## tar (tape-archive) (man 1 tar)
  - standard tool for archiving saving a (many) files and directories to a single
    tape or disk archive.
  - Individual files can be restored from the .tar file when needed.

Examples:
*Create archive file:*
$ tar czf _home_myUser_myProject_01.tar.gz /home/myUser/myProject
      ↑↑↑ ^^^^^^^^^^^^^                    ^^^^^^^^^^^^^^^^^^^^^^
      │││ prefixing with absolute          directory to archive
      │││ path it's just an optional
      │││ convention.
      │││
      ││└── name of output file
      │└─── use gzip for compression (.gz extension)*1
      └──── compress
*1: bzip2 ('j' option instead of 'z' and '.bz2' extension instead of gz)
    is also quite common, with better compression, but also more CPU intensive.

*Restore backup from archive file:*
$ cd /home/myUser
$ tar xzf _home_myUser_myProject_01.tar.gz
      ↑↑↑
      ││└── name of  input file
      │└─── use gzip for de-compression
      └──── de-compress

*List contents of archive:*
$ tar tzf _home_myUser_myProject_01.tar.gz
      ↑↑↑
      ││└── name of  input file
      │└─── use gzip for de-compression
      └──── *list*

## Remote Incremental [[{security.backup.incremental,storage.network]]
- EasyUp: KISS incremental remote backup around rsync+ssh
@[https://github.com/earizon/easyup]
- Rsnapshot: filesystem snapshot utility on top of rsync.
@[http://rsnapshot.org/]
  rsnapshot makes it easy to make periodic snapshots of local machines, and remote machines over ssh.
  The code makes extensive use of hard links whenever possible, to greatly reduce the disk space required
  and rsync to save bandwidth (backup only changes)
- Live backups with inotify + rsync + bash: Backup on "real-time changes"
@[https://linuxhint.com/inotofy-rsync-bash-live-backups/]
- Bacula:
@[http://www.bacula.org/]
  """Bacula is a set of Open Source, computer programs that permit to manage backup,
     recovery, and verification of computer data across a network of computers of different
     kinds,  offering many advanced storage management features that make it
     easy to find and recover lost or damaged files."""
     -@[http://www.bacula.org/9.0.x-manuals/en/main/index.html]
     *Director Daemon*supervises all the backup, restore, verify and archive operations.
      Sysadmin uses Director to schedule backups and to recover files..
     *Console service*allows the administrator or user to communicate with the Director
      (three versions: text-based, QT-based, wxWidgets)
     *File Daemon*It's installed on the machine to be backed up and is responsible for
      providing the file attributes and data when requested by the Director
      as well as for the file system dependent part of restoring the file attributes and data
      during a recovery operation.
     *Storage daemons*are software programs in charge of storage and recovery of the
      file attributes and data to the physical backup media or volumes. In other words, it is
      responsible for reading and writing your tapes (or other storage media, e.g. files)
     *Catalog Services*are responsible for maintaining the file indexes and
      volume databases for all files backed up allowing sysadmin or user to
      quickly locate and restore any desired file. The Catalog services sets
      Bacula apart from simple backup programs like tar and bru, because the catalog
      maintains a record of all Volumes used, all Jobs run, and all Files saved, permitting
      efficient restoration and Volume management. Bacula currently supports three different
      databases, MySQL, and PostgreSQL one of which must be chosen when building Bacula.
     *Monitor Service*Allows the administrator or user to watch current status of Directors,
      File Daemons and Bacula Storage Daemons. Currently, only a GTK+ version is available.

- Symple remote backups with ssh
  $ tar cjf - myDirToBackup \       # local
    | ssh myUser@myRemoteMachine \  # ssh pipe
    "cd myBackupPath && tar -xjf -" # remote

- "Real time" backup with rsync and bash
@[https://github.com/Leo-G/backup-bash]
[[}]]

#[multicore_compression_tools_summary]
## multi-core compression tools [[{storage.compression,performance.CPU,02_doc_has.comparative]]
  NOTE: bzip2 offers a much better compression than gz when source
  files have low entropy (many repeated words, white spaces,...) while
  consuming (much) more CPU.
  Ideal to save bandwidth while highly increasing CPU ussage.

  Ex:
  - two runs to avoid FS cache effects
  - Input: (dir1), 430M, with 70% binary (hard to compress) data.

    430M
  STANDARD UTILITY                           MULTI-CORE ALTERNATIVE
  NO COMPRESSION                          |
  ==================================      |
  $ sudo tar cf output.tar dir1           |
                                          |
  real  0m0,096s                          |
  user  0m0,048s                          |
  sys   0m0,049s                          |
  OUTPUT SIZE: 410M                       |

  GZIP                                    |   PIGZ (DEF.OPTIONS, 4 CORES)
  ==================================      |   ==============================
  $ sudo tar cf - dir1 | \                |   $ sudo tar cf - directory1 | \
     gzip  > output.tar.gz                |         pigz  > /dev/null
  TIME: 13,5s                             |   TIME:  5,0s
  OUTPUT SIZE: 337M                       |   OUTPUT SIZE: 337M

  BZIP2                                   |   PBZIP2 (DEF.OPTIONS, 4 CORES)
  ==============================          |   =================================
  $ sudo tar cf dir1 | \                  |   $ sudo tar cf - dir1 | \
     bzip2 > output.tar.bz2               |    pbzip2 > output.tar.bz2
  TIME: 66,0s                             |   TIME: 31,8s
  OUTPUT SIZE: 331M                       |   OUTPUT SIZE: 332M


  XZ                                      |   PIXZ  (DEFAULT OPTIONS, 4 CORES)
  ==============================          |   =================================
  $ sudo tar cf dir1 | \                  |   $ sudo tar cf - dir1 | \
     xz    > output.tar.bz2               |     pbzip2 > output.tar.bz2
  TIME: 151,7s                            |   TIME: 46,8s
  sys     0m2,179s                        |   sys     0m1,706s
  OUTPUT SIZE: 313M                       |   OUTPUT SIZE: 317M

From: @[https://www.linuxlinks.com/best-linux-multi-core-compression-tools/]
• pigz    Parallel implementation of gzip. It's a fully functional replacement for gzip
• PBZIP2  Parallel implementation of the bzip2 block-sorting file compressor
• PXZ     Runs LZMA compression on multiple cores and processors
• lbzip2  Parallel bzip2 compression utility, suited for serial and parallel processing
• plzip   Massively parallel (multi-threaded) lossless data compressor based on lzlib
• lrzip   Compression utility that excels at compressing large files
• pixz    Parallel indexing XZ compression, fully compatible with XZ. LZMA and LZMA2
[[}]]


## Zstandard  [[{01_PM.TODO]]
@[https://www.2daygeek.com/zstandard-a-super-faster-data-compression-tool-for-linux/]
- super-fast compression tool [[}]]

## Android 2 Linux Backups  [[{storage.android,]]
@[https://github.com/FlamingTuri/android-file-backup]

Script solving common problems when transferring files/folders from/to Android.
- transfer big folders
- keep the files original modification date

PREREQUISITES
- On Linux:
  - adb installed.
    Download (~10MB size Toolkit) from @[https://developer.android.com/studio/releases/platform-tools]
    More info at @[https://developer.android.com/studio/command-line/adb]

  - $ sudo apt-get install pv  ← pv allows to view the backup compression progress (-z option)

- On Android:
  - enable USB debugging
  - enable USB file transfer
    (accept dialog asking to trust your computer RSA fingerprint)

Execution Example:
  $ ./adb-backup.sh    ← backup "DCIM", "Download", "Pictures"
     /sdcard/DCIM        from Android to Linux FS.
     /sdcard/Download
     /sdcard/Pictures
[[storage.android}]]

[[security.backup}]]

# Audit  [[{security.audit]]

## basic user audit  [[{security.audit.user,101,security.aaa]]
$ who     ← Displays current users logged in + logged-in time
$ w       ← Displays who is logged into the system and WHAT THEY ARE DOING
            (procs. they are running).
$ users   ← Displays only user names who are currently logged in
$ last    ← Displays records of users-logged-in time, remote IP or PTTY, reboot time,
$ lastlog ← Displays list of users and what day/time they logged into the system.
$ whoami  ← Tells the user who they are currently logged in as
$ ac      ← Tell how much time users are logged in.
            (sudo apt install acct, sudo dnf install psacct, ...)
            It pulls its data from the current wtmp file.
            Ex:
            $ ac
            → total     1261.72
            $ ac -              <··· total hours by user
            → shark        5.24
            → nemo         5.52
            → shs       1251.00
            → total     1261.76
            $ ac -d | tail -10  <··· daily counts of how many hours users were logged in
            → Jan 11  total        0.05
            → Jan 12  total        1.36
            → Jan 13  total       16.39
            → Jan 15  total       55.33
            → Jan 16  total       38.02
            → Jan 17  total       28.51
            → Jan 19  total       48.66
            → Jan 20  total        1.37
            → Jan 22  total       23.48
            → Today   total        9.83
[[security.audit.user}]]


## Asciinema: TTY recordings [[{]]
@[https://asciinema.org/]
- Includes a command line tool to record the terminal plus
  a Javascript player to replay the session in a browser.
  https://asciinema.org/docs/how-it-works
  - Lightweight, purely text-based approach to terminal recording.
  - See demos at:
    - https://asciinema.org/
    - Example asciinema recording "embedded" in github markdown:
      https://github.com/mvndaemon/mvnd
[[}]]

## Linux Audit framework [[{monitoring.audit_framework,02_doc_has.diagram]]
  REF: @[https://www.youtube.com/watch?v=x2u_prS2HmM]
*ARCHITECTURE:*                                   *LOG "TOPICS":*
                                                  ┌───────────────────
 audit.rules   auditd.conf                        │user
     +           +                                │group
     │           │                                │Audit ID
     │           │                                │Remote Hostname
     │           │                                │Remote Host Address
     │           │  ┌─> audispd                   │System call
     v           v  │                             │System call args.
  auditctl ───> *auditd*         ┌─>  *aureport*  │File
     │           ^  │            │                │File Operations
     │           ║  └─> /var/log/audit/audit.log  │Session
     │           ║               │                │Success│Failure
     │           ║               └─>  *ausearch*
     │           ║
     └──────┐    ║
            │    ║
Running     │    ║      autrace
process     │    ║        │
    +   ┌───│──────────┐  │
    │   │   └─> *audit*│  │
    └──→+          ^   +←─┘
        │          ║   │                Lecture:
        │ KERNEL════╝  │                ═══ : control
        └──────────────┘                ─── : data─flow

  - *auditd daemon* centralize log writing to disk/network

  - *audit module@KERNEL* handles the audit rules
    (audtictl gets in charge of passing audit.rules to audit@KERNEL
    It also intercepts system calls.

  - CLI UTILITIES:
  |-  *aureport* creates human-readable reports. Useful options:
  |  --summary
  |  --failed
  |  --start, --end (aureport understands 'today', 'yesterday', 'now',
  |                  'recent', 'this-week', 'this-month', 'this-year')
  |  --auth, --avc, --login, --user, --executable, --syscall
  |
  |-  *ausearch* "drills deeper" into the details of a
  |   particular event. Ex.
    $*# ausearch -sv no --comm httpd *
        └──────────────────────────┘
        Search audit log for recently denied events triggered by
        httpd ("Apache", lighthttpd). Useful also for debugging
        SELinux related problems.

  - autrace is the "ptrace" or "strace" for audit events
    (audit the audit)

  - Audispd (dispatcher) provides a plugin system to dispath
    to other places (remote machines, Prometheus,...)
  In RHEL 8+ it has been integrated into auditd.
  (REF https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8-beta/html/8.0_beta_release_notes/new-features)

## Audit Daemon "Full Journey"
└ INITIAL SETUP:
 1) Audit Daemon Configuration -→  /etc/audit/auditd.conf
    Defines how the audit system works once the daemon is running.
    Default settings ussually are right-enough
 2) Audit Rules:
   *WARN: No default rules are set*
  r*WARN: First match wins!
    - Used to define *what we are interested in auditing*
    - three basic types of audit rules:
      - Basic audit system parameters
      - File and directory watches
      - System call audits
    Ex. Audit rules
    | # basic audit system parameters
    | -D   ← Delete all previous rules (recommended)
    | -b   ← How many buffer  do we want to have
    | -f 1 ← 0: ignore failures
    |        1: syslog
    |        2: panic
    | -e 1 ← 0: disable logging
    |        1: enable
    |        2: inmutable (not even root can change without reboot)
    | # some file and dirs. watches
    | -w /home/myUser/MySecrets  -p rxwa
    | -w /sbin/auditctl          -p x
    | # Example system call rule
    | -a entry,always -S umask

 3) Audispd Daemon Configuration (now part of auditd)

└ Daily use:
  Normally you find an event class of interest with aureport and then
  drill down into the nitty-gritty with ausearch

$ sudo aureport --summary
Summary Report
======================
Range of time in logs:    12/31/1969 19:00:00.000 - 04/20/2019 07:42:51.715
Selected time for report: 12/31/1969 19:00:00     - 04/20/2019 07:42:51.715
Number of changes in configuration             : 971
Number of changes to accounts, groups, or roles: 36
Number of logins                               : 2992
Number of failed logins                        : 497
Number of authentications                      : 3161
Number of failed authentications               : 438
Number of users                                : 8
Number of terminals                            : 30
Number of host names                           : 16
Number of executables                          : 24
Number of commands                             : 14
Number of files                                : 2
Number of AVC's                                : 3
Number of MAC events                           : 3052
Number of failed syscalls                      : 0
Number of anomaly events                       : 92
Number of responses to anomaly events          : 0
Number of crypto events                        : 52922
Number of integrity events                     : 0
Number of virt events                          : 0
Number of keys                                 : 0


$ sudo aureport --login --failed
04/15/2019 16:54:29 (unknown) 211.252.85.100 ssh /usr/sbin/sshd no 21896
04/15/2019 14:35:25 root      211.252.85.100 ssh /usr/sbin/sshd no 20413
04/07/2019 20:10:13 (unknown) 51.68.35.150   ssh /usr/sbin/sshd no 11272
04/07/2019 19:59:25 apache    51.68.35.150   ssh /usr/sbin/sshd no 10312
...

$ sudo aureport --syscall -fail
...
   date        time    syscall    pid     comm      auid   event
1. 04/09/2019  .....   87         4006    semodule  11112  53039
                       ┌───┬─·····························─┴───┘
$ sudo auserach -i -a  53039
.....

$ sudo aureport -e -i --summary | mkbar events   <- CREATE PLOT OF EVENTS

$ sudo aureport -s -i | \                        <- Show relationship among users,system calls,...
  aws '/^`[0-9]/ { printf "%s %s\n", $6, $4}' | \   with mkgraph
  sort | uniq | mkgraph syscall-vs-program

  man:
  (5)audispd.conf  (8)audispd   (8)auditd   (8)ausearch
  (5)auditd.conf   (8)auditctl  (8)aureport (8)autrace

- /usr/share/doc/audit:
 Contains README with basic design information and
 sample .rules files for different scenarios:
 -   capp.rules: Controlled Access Protection Profile (CAPP)
 -   lspp.rules: Labeled Security  Protection Profile (LSPP)
 - nispom.rules: National Industrial Security Program Operating Manual Chapter 8 (NISPOM)
 -   stig.rules: Secure Technical Implementation Guide (STIG)

[[monitoring.audit_framework}]]

## logrotate  [[{security.backup,security.secret_management,storage.troubleshooting]]
  Avoid filling disks with logs
- automatic rotation, compression, removal, and mailing of log files.
  (daily, weekly, monthly, or when size is exceeded)

synopsys:
$ logrotate --state "file" option_flags config_file ..
                                        └────┬────┘
                           - any number of config files
                             may be given on the command line.
                           - order is important. later ones override first ones

- tipically run on daily cron.

 *option flags:*
  --debug: no changes will be made to the logs or to the logrotate state file.
  --force: force rotation, even if logrotate thinks this is not necessary.
  --mail : command to use when mailing logs. command must support next args:
          1) e-mail subject ,  2) the recipient.
          message read from standard input
          default mail command: /bin/mail -s.
  --state "statefile": tells logrotate to use an alternate state file. this is
          useful if logrotate is being run as a different user for various sets
          of log files.
          default state file is /var/lib/logrotate.status.
  --usage
  --help
  --verbose

 *configuration file*
  - rules: local definitions override global ones,
           later definitions override earlier ones
  - example file:

  | compress              ← compress using gzip (by default)
  |                         use compresscmd     to set gzip alternative
  |                         use compressoptions to set gzip alternative
  | # this is a comment
  | /var/log/directory1 "/var/log/directory2/file name with spaces.log" {
  |     rotate 5          ← optional. 5 weekly rotations
  |     weekly            ←           daily|weekly|monthly|yearly
  |
  |     dateext           ← optional. suffix with date instead of plain number.
  |     dateformat %y%m%d    *warn*: generated string must be lexically sortable
  |                                  since "rotate" option will sorts all rotated
  |                                  filenames lexically to find out which logfiles
  |                                  are older and should be removed.
  |
  |     size 100k         ← optional. rotate whenever it grows over 100k in size.
  |     mail www@my.org   ← optional. old logs files mailed (uncompressed)
  |     sharedscripts     ← optional. script will be run once (after old logs have been
  |                          compressed), vs once for each log.
  |
  |     postrotate                        ┐  ← see also prerotate|firstaction|lastaction
  |         /usr/bin/killall -hup syslogd │
  |     endscript                         ┘
  | }

   *warn*: use wildcards with caution. "*" will rotate all files,
          *including previously rotated ones*.

  other config. file options:
  - copy: make a copy of the log file, but don't change the original at all.
          (create snapshot)
  - copytruncate: copy and truncate original log file in place
  - create mode owner group:
           ^
           octal format

  - delaycompress: postpone compression of previous log file to next rotation cycle.
    useful when some program cannot be told to close its logfile and thus might
    continue writing to the previous log file for some time.

  - extension ext: allows to rotate names like mylog.foo → mylog.1.foo.gz
                                                          (mylog.foo.1.gz by default)
  - ifempty: rotate log file even i empty, overriding any previous notifempty option
  - include file_or_directory:  include file in config.
    (files read in alphabetic order for dirs)
  - mailfirst   : mail just-rotated file vs about-to-expire file.
  - maillast    : mail about-to-expire file vs  just-rotated file.
  -*maxage     n:*remove rotated logs older than n days.

  - size size   : rotate only if log grow bigger than size bytes.
                  (suffix k|m|g allowed)

  - minsize size: rotate when file grow bigger than size bytes,
                  but not before any additionally specified time interval
                  (daily, weekly, monthly, or yearly).

  - missingok   : don't issue error if a file is missing.
  - nocreate    : do not create new log file.
  - (no)shred   :
    do not use shred when deleting old log files. see also shred.
  - olddir $dir : move old logs to $dir
                   *warn*: $dir must be on the same physical device
  - rotate count: log rotated count times before being removed|mailed

  - see also: sharedscripts
  - shred       : delete log files using shred -u instead of unlink().            [secret_management]
                  this should ensure that logs are not readable after their
                  scheduled deletion; this is off by default. see also noshred.
  - start count :
  - tabooext [+]list: defaults to .rpmorig, .rpmsave, ,v, .swp, .rpmnew, ~,
                      .cfsaved and .rhn-cfg-tmp-*.

 *files*:
  - /var/lib/logrotate.status    default state file.
  - /etc/logrotate.conf          default configuration options.
[[}]]



## logwatch [[{security.audit.user,security.aaa]]
  - perl script utility to summarize logs. Ussage:
    (Doesn't look to work on Fedora, works on Debian)

$ sudo logwatch --detail Low --range today
(output will be similar to)
→  ################### Logwatch 7.4.3 (12/07/16) ####################
→         Processing Initiated: Wed Jul 31 17:41:33 2019
→         Date Range Processed: today
→                               ( 2019-Jul-31 )
→                               Period is day.
→         Detail Level of Output: 0
→         Type of Output/Format: stdout / text
→         Logfiles for Host: 24x7
→  ##################################################################
→
→  --------------------- dpkg status changes Begin ------------------------
→
→  Installed:
→     libdate-manip-perl:all 6.57-1
→     libsys-cpu-perl:amd64 0.61-2+b1
→     libsys-meminfo-perl:amd64 0.99-1
→     logwatch:all 7.4.3+git20161207-2
→
→  ---------------------- dpkg status changes End -------------------------
→
→
→  --------------------- pam_unix Begin ------------------------
→
→  su:
→     Sessions Opened:
→        root → www-data: 2 Time(s)
→
→  sudo:
→     Sessions Opened:
→        user1 → root: 2 Time(s)
→
→  systemd-user:
→     Unknown Entries:
→        session closed for user www-data: 2 Time(s)
→        session opened for user www-data by (uid=0): 2 Time(s)
→
→
→  ---------------------- pam_unix End -------------------------
→
→
→  --------------------- SSHD Begin ------------------------
→
→
→  Deprecated options in SSH config:
→     KeyRegenerationInterval - line 19
→     RSAAuthentication - line 31
→     RhostsRSAAuthentication - line 38
→     ServerKeyBits - line 20
→
→  Users logging in through sshd:
→     user1:
→        81.61.178.46 (81.61.178.46.dyn.user.ono.com): 1 time
→
→  **Unmatched Entries**
→  reprocess config line 31: Deprecated option RSAAuthentication : 1 time(s)
→  reprocess config line 38: Deprecated option RhostsRSAAuthentication : 1 time(s)
→
→  ---------------------- SSHD End -------------------------
→
→
→  --------------------- Sudo (secure-log) Begin ------------------------
→
→
→  user1 => root
→  ---------------
→  /bin/bash                      -   1 Time(s).
→  /usr/sbin/logwatch             -   1 Time(s).
→
→  ---------------------- Sudo (secure-log) End -------------------------
→
→
→  --------------------- Disk Space Begin ------------------------
→
→  Filesystem      Size  Used Avail Use% Mounted on
→  /dev/root        47G   25G   20G  55% /
→
→
→  ---------------------- Disk Space End -------------------------
→
→
→  ###################### Logwatch End #########################
[[}]]

## Logreduce(IA)filter [[{security.audit.user.IA,QA.UX,01_PM.TODO]]
@[https://opensource.com/article/18/9/quiet-log-noise-python-and-machine-learning]
@[https://pypi.org/project/logreduce/] logreduce@pypi
  - Quiet log noise with Python and machine learning
[[}]]
## Tiger: Kiss-IDS [[{security.IDS,01_PM.low_code,qa.UX,security.audit.user,]]
@[https://www.nongnu.org/tiger/]
(Unix/Linux)
- open source shell-scripts collection for security audit and host intrusion detection.
- Very extensible.
- It scans system configuration files, file systems, and user configuration files
  for possible security problems and reports them.

*Install*
- Debian/Ubuntu/Mint/...
  $ sudo apt install tiger
  (Output will be similar to)
  →┌─────────────────────────Tripwire Configuration ├────────────────────────────┐
  →│ Tripwire uses a pair of keys to sign various files, thus ensuring their ... │
  →│ ...                                                                         │
  →│                                                                             │
  →│        <Yes>                                                          <No>  │
  →└─────────────────────────────────────────────────────────────────────────────┘
  (Pressing Yes)
  →┌────────────────────────┤ Tripwire Configuration ├────────────────────────┐
  →│                                                                          │
  →│ Tripwire keeps its configuration in a encrypted database that is         │
  →│ generated, by default, from /etc/tripwire/twcfg.txt                      │
  →│                                                                          │
  →│ Any changes to /etc/tripwire/twcfg.txt, either as a result of a change   │
  →│ in this package or due to administrator activity, require the            │
  →│ regeneration of the encrypted database before they will take effect.     │
  →│                                                                          │
  →│ Selecting this action will result in your being prompted for the site    │
  →│ key passphrase during the post-installation process of this package.     │
  →│                                                                          │
  →│ Rebuild Tripwire configuration file?                                     │
  →│                                                                          │
  →│                    <Yes>                       <No>                      │
  →│                                                                          │
  →└──────────────────────────────────────────────────────────────────────────┘
  (Press yes)
  →┌────────────────────────┤ Tripwire Configuration ├─────────────────────────┐
  →│                                                                           │
  →│ Tripwire keeps its policies on what attributes of which files should be   │
  →│ monitored in a encrypted database that is generated, by default, from     │
  →│ /etc/tripwire/twpol.txt                                                   │
  →│                                                                           │
  →│ Any changes to /etc/tripwire/twpol.txt, either as a result of a change    │
  →│ in this package or due to administrator activity, require the             │
  →│ regeneration of the encrypted database before they will take effect.      │
  →│                                                                           │
  →│ Selecting this action will result in your being prompted for the site     │
  →│ key passphrase during the post-installation process of this package.      │
  →│                                                                           │
  →│ Rebuild Tripwire policy file?                                             │
  →│                                                                           │
  →│                    <Yes>                       <No>                       │
  →└───────────────────────────────────────────────────────────────────────────┘
  (Press yes)
  ...
  (enter required passphrases)
  ...
 *The Tripwire binaries are located in /usr/sbin and the database is located  *
 *in /var/lib/tripwire. It is strongly advised that these locations be stored *
 *on write-protected media (e.g. mounted RO floppy). See                      *
 */usr/share/doc/tripwire/README.Debian for details.                          *

  $ tar -xzf tiger-3.2rc3.tar.gz
  $ cd tiger-3.2/
  $ sudo ./tiger

*tigerrc (Configuration)*
*Running*
  $ sudo tiger
  → Tiger UN*X security checking system
  →    Developed by Texas A&M University, 1994
  →    Updated by the Advanced Research Corporation, 1999-2002
  →    Further updated by Javier Fernandez-Sanguino, 2001-2015
  →    Contributions by Francisco Manuel Garcia Claramonte, 2009-2010
  →    Covered by the GNU General Public License (GPL)
  →
  → Configuring...
  →
  → Will try to check using config for '2018' running Linux 4.17.17-x86_64-linode116...
  → --CONFIG-- [con005c] Using configuration files for Linux 4.17.17-x86_64-linode116. Using
  →            configuration files for generic Linux 4.
  → Tiger security scripts *** 3.2.3, 2008.09.10.09.30 ***
  → 14:57→ Beginning security report for localhost.
  → 14:57→ Starting file systems scans in background...
  → 14:57→ Checking password files...
  → 14:57→ Checking group files...
  → 14:57→ Checking user accounts...
  ...
  → 14:59→ Checking NFS export entries...
  → 14:59→ Checking permissions and ownership of system files...
  → Security report is in `/var/log/tiger/security.report.localhost.190115-14:57'.


- security report will be generated in the ./log
→ ...
→ Security report is in `log//security.report.tecmint.181229-11:12'.
$ sudo cat log/security.report.tecmint.181229-11\:12

To display more information on a specific security message:
 - run the tigexp (TIGer EXPlain) command and provide the msgid as an
argument, where “msgid” is the text inside the [] associated with
each message.

For example, to get more information about the following messages,
where [acc001w] and [path009w] are the msgids:

--WARN-- [acc015w] Login ID nobody has a duplicate home directory (/nonexistent) with another user.
--WARN-- [path009w] /etc/profile does not export an initial setting for PATH.
[[security.IDS}]]
[[security.audit}]]

# Notification: [[{monitoring.notifications]]
## Sending eMails with CURL [[{monitoring.notifications,security.notifications,01_PM.low_code]]
  https://blog.edmdesigner.com/send-email-from-linux-command-line/
  (cross-platform)

  $ curl -sv --user 'api:key-7e55d003b...f79accd31a' \  [[{monitoring.notifications.mailgun]]
      https://api.mailgun.net/v3/sandbox21a78f824...3eb160ebc79.mailgun.org/messages \
      -F from='Excited User <developer@yourcompany.com>' \
      -F to=sandbox21a78f824...3eb160ebc79.mailgun.org \
      -F to=user@gmail.com \
      -F subject='Hello' \
      -F text='Testing some Mailgun awesomeness!' \
     --form-string html='<h1>EDMdesigner Blog</h1><br /><cite>This tutorial helps me understand email sending from Linux console</cite>' \
      -F attachment=@logo2yellow.jpg                    [[}]]
[[}]]

## 'sendmail' + Gmail (SSMTP Package) [[{monitoring.notifications.gmail]]
  $ sudo apt install ssmtp                             <- PRESETUP) Install "lightweight"
                                                          (sendonly) ssmtp 'sendmail'

  $ echo "Subject: hello" | sendmail test@example.com  <- test install. It will fail with
                                                          error: Cannot open mailhub:25.
  $ sudo editor /etc/ssmtp/ssmtp.conf
    | UseSTARTTLS=YES
    | UseTLS=YES
    | root=<from-email-address>
    | mailhub=smtp.gmail.com:587
    | AuthUser=<your-account's-user-name>
    | AuthPass=<USER-OR-APP-PASSWORD>
               └─────────┬──────────┘
  ┌──────────────────────┘
  └>(PRE-SETUP) For Google an "App Generated Password" replaces the
    normal user password. Each time User Password is changed this
    app generated password must be changed too. To generate this pass:
    -> https://myaccount.google.com/?nlr=1
      -> Select Security.
        -> Under "Signing in to Google," select "App Passwords".
          The option can be disabled if:
          1) 2-Step Verification is not set up for your account.
          2) 2-Step Verification is only set up for security keys.
          3) Your account is through work/school/...
          4) You turned on Advanced Protection.
          -> choose Select app (Bottom of page)
            -> choose mail App -> device ->_*Generate Pass*

    WARN: If ssmtp runs without error but we can NOT see the mail into the
          inbox probably an incomming filter exists. To fix it:
          -> Gmail Web App -> Setting -> "Filters and Blocked Addresses"
            ─> Remove filter >> from:(...@gmail.com) to:(..@gmail.com)  Skip Inbox <<

  NOTE: smtp.gmail.com limits: 100 recipients max per message, 500 messages/day.
[[}]]

## Telegram Notifications  [[{monitoring.notifications.telegram]]
  REF: https://ugeek.github.io/blog/post/2019-03-14-crea-un-bot-de-telegram-con-bash-y-una-sola-linea-de-terminal.html (ES)
- Telegram API allows to easely craete bots to notify any event in
  our server (someone logged-in through ssh, ...)

└ PRE-SETUP. STEP 1: Go to:
  https://t.me/BotFather                      Alt 1 (mobile)
  https://web.telegram.org/#/im?p=@BotFather  Alt 2: web browser

 (BotFather is the "fathers of all Bots", created by Telegram to easify bot creation)

└ PRE-SETUP STEP 2:
  -  type  "/newbot"
  - We will be asked for the name of our new bot
    (Name must end with bot)
  - Once the bot name is accepted we will receive the  *access-token to the HTTP API*
    NOTE:  each Telegram user has an ID that can be retrieved starting the "@userinfobot" bot)

└ PRE-SETUP STEP 3: (Optional)
  - If we want to add the Bot to a group or channel to send messages
    or any other function, we must retrieve the Group or Channel ID. To
    do so send a message to @ChannelIdBot from the channel/group.

└ CREATE-THE-BOT:
- CREATE-THE-BOT STEP 1: Write a test script
  $ editor  *my_bot_telegram.sh*  # ← edit a new script  (using vim/nano/"editor"...)
  _____________________________________________________
  #!/bin/bash

  TOKEN="XXXXXXXXX:XXXXXXX_XXXXXXXXXXXXXXXXXXXXXXXXXXX" # Replace with token provided
  ID="escribe tu id aquí"                               # Replace with ID Provided
  MENSAJE="This is a *test* message"
  URL="https://api.telegram.org/bot$TOKEN/sendMessage"

  curl -s -X POST $URL -d chat_id=$ID -d text="$MENSAJE"
  _____________________________________________________

- CREATE-THE-BOT STEP 2: Set exec permissions
  $ sudo chmod +x  *my_bot_telegram.sh*

- CREATE-THE-BOT STEP 3: Test it!
  $  *./my_bot_telegram.sh*

### Example use 1: Show ssh connection IP*
  Add some lines similar to the next one to ~/.bashrc:

  TELEGRAM_TOKEN="?????????:???????????????????????????????????" # Replace with token provided
  TELEGRAM_ID="?????????"                                        # Replace with ID Provided
  URL="https://api.telegram.org/bot${TELEGRAM_TOKEN}/sendMessage"
  # We want to exec curl only if it's a real ssh login
  # ps -ef | egrep "..." will execute next line only if a match exists.
  # The match only exists if our parent process is sshd (real ssh access)
  # Using the regex "$PPID.*[s]shd" instead of just "$PPID.*sshd" filters out the egrep command
  ps -ef | egrep "$PPID.*[s]shd" &amp;&amp; \
    curl -s -X POST $URL -d chat_id=${TELEGRAM_ID} -d text="New ssh connection to $HOSTNAME from IP $SSH_CLIENT " 1>/dev/null 2>&1 &

### Example use 2: Notify server restarts*
$ crontab -e
(add next multi-line removing new lines and ending '\')
@reboot ( sleep 100 ;  curl -s -X POST \
  https://api.telegram.org/bot"$TELEGRAM_TOKEN"/sendMessage \
  -d chat_id="${TELEGRAM_ID}" \
  -d text="new ssh with IP ${SSH_CLIENT}")
[[monitoring.notifications.telegram}]]

## Pushover (iOS/Android push) [[{security.notifications]]
REF: @[https://www.cyberciti.biz/mobile-devices/android/how-to-push-send-message-to-ios-and-android-from-linux-cli/]
• Pushover Service with no need for AWS/Azure/GCP accounts:
  - $5 ONE-TIME purchase. !!!!
  - Up to 7500 messages/month.
  - (More options exists for IT teams)
• PRESETUP)
  - Sign up for Pushover.
  - download Pushover for Android/iOS.
  - subscribe to service (7-day trial account).
  - Once logged in, register cli application.
    to retrieve a  *secret API token*.
• Ussage:
  - Integrate Pushover API into (monitoring) scripts. Ex. script:
    push_to_mobile(){        # ← Ussage push_to_mobile $title $message
      local t="${1:cli-app}"
      local m="$2"
      [[ "$m" != "" ]] &amp;&amp; curl -s \
       *--form-string "token=${PUSHOVER_API_TOKEN}"*\
        --form-string "user=${PUSHOVER_API_USER}" \
        --form-string "title=$t" \
        --form-string "message=$m" \
        https://api.pushover.net/1/messages.json
    }
[[}]]

## alert/monitoring best-patterns [[{security.notifications,qa.best_patterns,monitoring.notifications,01_PM.WiP]]
- Simple e-mail/telegram do NOT scale
  You will receive many useless notification from proper behaviour
  (correct logins, ...)

  - *A proper monitoring system will notify only when there's a real problem:*
  - Enable messaging agent (email, telegram,...)
  - Enable cpu/mem/net/disk login inputs and (Prometheus) output.
  - Run a single Prometheus server that scrapes each endpoint every minute.
  - Add Grafana for nice graphs.
  - Add Alert Manager and set up thresholds to notify you:
    -  via OpsGenie/PagerDuty/VictorOps/Slack.
  - Check graphs once a week, unless you get notifications.

- You can "start small" and use Monit:
  - single binary that can be configured to watch stuff and react to problems.
    - Start a service when stoped.
    - Clean up when disk is full.

- "Sensu": Slightly better (and more complex) than Monit, allowing to:
  - run any script as a check
  - fix stuff if it's broken for X consecutive checks
  - alert if it's still broken Y consecutive checks.
  - Sensu can also alert to pretty much anything.
    "I wrote handlers that alert to Slack, Jira and a pager."
[[}]]

[[monitoring.notifications}]]

# Text Search and processing [[{101.text_utils]]
## Text view [[{]]
  Displaying text:
$ head -n 20 /path/to/textFile # shows first 20 lines (-n 20). 10 lines by default if -n not provided.
$ tail -n 20 /path/to/textFile # shows last  20 lines (-n 20). 10 lines by default if -n not provided.
$ tail -f    /path/to/textFile # shows stream ("f"lush) of lines as they are appended to the file
$ less       /path/to/textFile # Views text. Add scroll control backwards and forwards.
                               # embedded systems use "more", that just scroll forwards.
$ cat        /path/to/textFile # dump text content to standard output (STDOUT)
$ cat file1 file2 file3 ....   # concatenates files'content and dumps into STDOUT
$ tac file1 file2 file3 ....   # concatenates files'content and dumps into STDOUT in reverse order


 man 1 column:
$ column -n 140 /usr/share/dict/words  # ← One token per line input
→ A             archdeacon      effort's      mads             salient's
→ A's           archdeacon's    effortless    madwoman         salients
→ ...
→ archbishop    effigy          madras        salesperson's    étude's
→ archbishop's  effigy's        madras's      salespersons     études
→ archbishopric effluent        madrases      saleswoman

$ COLUMNS=100 column -t -s, input.csv # ← -t(able):
1     2     3                             -s(eparator): Indicates column sep
a     b     c                                          (white space by default)
x     y     z
[[}]]

## Text info [[{]]
$ wc /path/to/textFile  ← Display total count of words, lines and bytes
                          Options:
                            -w count only words
                            -l count only lines
                            -w count only bytes


$ diff file1 file2     ← Compares two text files and output a difference report indicating:
                         '> line_in_file2_not_in_file1
                         '< line_in_file1_not_in_file2
$ sdiff                ← Similar to diff but with report in colum mode (more human readable)
$ diff3                ← diff for three files
[[}]]

## SORTING FILE CONTENT !!!! [[{]]
  $ sort file1       <-·· Sorting alphabetically lines in file
                         (-r to reverse, -g for numerical sort)

  $ sort file1 \
    -t ':'           <-·· Use ':' as separator,
    -k 4 -k 1             sort first by column 4, then column 1.


  $ uniq file1       <··· Eliminates duplicate entries from a file
                          Commonly used with sort like:
                          $ cat file.txt | sort | uniq
                          Options: -c: display number of occurances of each duplicate
                                   -u: list only unique entries
                                   -d: list only duplicate entries


  $ join file1 file2 <-·· Join two lines together assuming they share
                          at least one common value on the relevant line,
                          skiping lines withouth common value.
[[}]]

## Text edit  [[{]]
  By default we can use the following tools like:
  $ tool inputFile   # Apply to given file
   or
  $ command1 ... | tool1 | tool2 | tool3 # Pipe execution output (stdout) as input
                                           for next command.

└ Cut by column:
  $ cut -d "," -f 1,3,7 file1.csv <- Use "," as column delimiter,
                                     show then columns 1,3,7
  $ cut -c 1-50         file.txt  <- show characters 1 to 50 in each line
  $ cut -5, 8, 20-      file.txt  <- show characters 1 to 5 and 8 and
                                     from 20 to the end

$ tr "u" "d" file1  ← translate all instances of characters in a text file
$  cat some_file | tr '[A-Z]' '[a-z]' > new_file  ← Convert all capital letters to lowercase

$ nl file1.txt  ← Display file1.txt to STDOUT prefixing with line numbers

$ sed "s/ *up/ down/g" file1.txt <- Replaces " *up*" by " *down*".
                                    /g flag: replace all ocurrences (vs only first match).
sed stays for "Stream editor", and has a lot of powerful flags like searching for regular expresions ...
[[}]]

[[101.text_utils}]]

#[systemd_summary]
# SystemD  [[{configuration.systemd]]
## Service Configuration [{configuration.systemd]]
@[http://freedesktop.org/wiki/Software/systemd/]
@[https://www.freedesktop.org/software/systemd/man/systemd.service.html]
@[http://www.tecmint.com/create-new-service-units-in-systemd/]
"service unit"                  "targets"
- createNew                       unit_collection
- run                             "wants"
- lifespan:daemon|run-once

$ systemctl --type=service           <··· Check unit_collections.
$ systemctl status firewalld.service <··· Check status of service
$ sudo systemctl isolate \           <··· Change to new runlevel (run-level)
        multi-user.target
$ sudo systemctl \
   enable|start|stop|restart|disable \
     firewalld.service

  ┌─ /etc/systemd/system/MyCustomScript.service ┐ Don't forget to exec:
  │ [Unit]                                      │ $ sudo systemctl daemon-reload
  │ Description = making network connection up  │ (systemctl config is on RAM)
  │ After = network.target                      │
  │ [Service]                                   │
  │ ExecStart = /root/scripts/conup.sh          │
  │ [Install]                                   │
  │ WantedBy = multi-user.target                │
  └─────────────────────────────────────────────┘

◆ SYSTEMD CORE: manager, systemd

◆ SYSTEMD UTILITIES
  $ systemctl     : main control tool from systemd.
  $ journalctl    : Queyr systemd Journal
  $ notify        : Notify Serv.Mgn about start-up-completion/...daemon status changes
  $ analyze       : analyze and debug system-manager.
  $ systemd-cgls  : recursively shows contents of selected control group hierarchy in a tree.
  $ systemd-cgtop : shows "top" control groups of local control group hierarchy.
  $ loginctl      : Control SystemD login Manager.
  $ systemd-nspawn: Spawn a command or OS in a light-weight container.  In many ways it is
                    is similar to chroot(1), but more powerful by fully virtualizing
                    FS hierarchy, process tree, various IPC subsystems and host+domain name.

◆ SYSTEMD DAEMONS       ◆ SYSTEMD TARGETS
  · systemd       :       · bootmode   · reboot     · logind
  · journald      :       · basic      · multiuser  · graphical
  · networkd      :       · shutdown   · dbus dlog  · user-session
  · logind        :                                 · display service
  · user sessiona :
[[}]]

## File name extensions for unit types  [[{configuration.systemd]]
* *.target     *: define groups of units. They achieve little themselves and serve to call
  *            *  other units that are responsible for services, filesystems ...
  *            *  (equivalent to the classical SysV runlevels)
* *.service    *: handle services that SysV-init-based distributions will typically
  *            *  start or end using init scripts.
* *.(auto)mount*: mounting and unmounting filesystems
* *.path       *: allow systemd to monitor files and directories specified
  *            *  when an access happens in path, systemd will start the appropriate unit
* *.socket     *: create one or more sockets for socket activation.
  *            *  service unit associated will start the service when a connection request
  *            *  is received.

## CONFIG. FILE LAYOUT
(NOTE: /etc takes precedence over /usr)
 *Maintainer   *: /usr/lib/systemd/system              ( + $ systemctl daemon-reload)
 *Administrator*: /etc/systemd/system/[name.type.d]/ ) ( + $ systemctl daemon-reload)
 *runtime      *: /runtime/systemd/system

• chkservice UI ncurses tool for managing systemd
  units in terminal.
@[https://github.com/linuxenko/chkservice]
[[}]]

## Journalctl(find logs)  [[{configuration.systemd,monitoring.101,security.audit.user]]
### Display/filter/search system logs
  # journalctl                      # <· all logs
  # journalctl -b                   # <· Boot Messages
  # journalctl -b -1                # <· Last Boot Messages
  # journalctl --list-boots         # <· list system boots
  # journalctl --since "3 hour ago" # <· Time range
                       "2 days ago" #
      --until "2015-06-26 23:20:00" #
  # journalctl -u nginx.service     # <· by unit (can be specified multiple times)
  # journalctl -f                   # <· Follow ("tail")
  # journalctl -n 50                # <· most recent (50) entries
  # journalctl -r                   # <· reverse chronological order
  # journalctl -b -1  -p "crit"     # <· By priority:
                                    #     -b -1     : FROM emergency
                                    #     -p "crit" : TO: Critical
  # journalctl _UID=108             # <· By _UID

### Output Formats ( -o parameter )

   json: json one long-line
   json-pretty:
   verbose:
   cat:  very short form, without any date/time or source server names
   short: (default), syslog style
   short-monotonic: similar to short, but the time stamp second value is shown with precision


 *NOTE:* journal is "synchronous". Eacth time someone tries to write it checks if
        ther is space or something needs to be deleted. (vs remove each 24 day,...)

## Clean/Compact/Delete logs
  $ sudo journalctl --vacuum-time=2d   ← Retain only last two days
  $ sudo journalctl --vacuum-size=500M ← Retain only last 500 MB
[[}]]

## Rsyslog (ancient log system) [[{monitoring,security.audit.user,02_doc_has.comparative,]]
@[github.com/rsyslog/rsyslog.git]
@[https://www.rsyslog.com]

The Rocket-fast Syslog Server
- Year: 2004
- (primary) author: Rainer Gerhards
- Implements and extends syslog protocol (RFC-5424)
- Adopted by RedHat, Debian*, SuSE, Solaris, FreeBSD, ...
  *Replaced by journald* in Fedora 20+

Important extensions include:
- ISO 8601 timestamp with millisecond and timezone
- addition of the name of relays in the host fields
  to make it possible to track the path a given message has traversed
- reliable transport using TCP
- GSS-API and TLS support
- logging directly into various database engines.
- support for RFC 5424, RFC 5425, RFC 5426
- support for RELP (Reliable_Event_Logging_Protocol)
- support for buffered operation modes:
  messages are buffered locally if the receiver is not ready
- complete input/output support for systemd journal
- "Infinite" logs. Can store years of logs from
                   hundreds of machines.

## Journald [[{]]
@[https://www.loggly.com/blog/why-journald/]
@[https://docs.google.com/document/pub?id=1IC9yOXj7j6cdLLxWEBAGRL6wl97tFxgjLUEHIX3MSTs]
- system service for collecting and storing log data,
  introduced with systemd.
- easier for admins to find relevant info.
- replaces simple plain text log files with a special file format
  optimized for log messages with index-like queries,
  adding Structure to Log Files
- It does *not* include a well-defined remote logging implementation,
  relying on existing syslog-protocol implementations to relay
  to a central log host,(and  *losing most of the benefits*)
- retains full syslog compatibility by providing the same API in C,
  supporting the same protocol, and also forwarding plain-text versions
  of messages to an existing syslog implementation.
  Obviously the format, as well as the journald API allow for structured data.

Syslog-protocol Problems:
- syslog implementations (ussually) write log messages to plain text files
  with lack of structure.
- syslog protocol does *NOT* provide a means of separating messages
  by application-defined targets (for example log messages per virt.host)
  This means that, for example, web servers generally write their own access
  logs so that the main system log is not flooded with web server status messages.
- log files write messages terminated by a newline:
  (very) hard for programs to emit multi-line information such as backtraces
  when an error occurs, and log parsing software must often do a lot of work
  to combine log messages spread over multiple lines.

journalctl:
- The journald structured file format does not work well with standard
  UNIX tools optimized for plain text. The journalctl tool will be used.
- very fast access to entries filtered by:
  date, emitting program, program PID, UID, service, ...
- Can also access backups in single files or directories of other systems.
[[}]]

## Modern logging
- Modern architectures use many systems where it becomes impractical to
  read logs on individual machines.
- Centralized logging are usually stored in a (time-series) database
  address many of the same issues that journald does without the problems
- Journald allows applications to send key-value fields that the
  centralized systems could use directly instead of relying on these heuristics.
 *Sadly, journald does not come with a usable remote logging solution*.
  - systemd-journal-remote is more of a proof-of-concept than an actually
    useful tool, lacking good authentication among other things.
[[}]]
[[configuration.systemd}]]

# APPLICATION PACKAGE MANAGERS  [[{packaging]]
## DNF  ("yum++") [[{packaging.rpm,01_PM.WiP]]
@[https://dnf.readthedocs.io/en/latest/]
@[https://github.com/rpm-software-management]
*MOST COMMONLY USED:*
$ dnf search  "*myPackage*"     # ← Show matching pattern in package name|description
$ dnf list    "*myPackage*"     # ← Show matching pattern in package name
                                    Filter like:

$ sudo dnf install myNewPackage # ← Install package and dependencies
                                    (-y flag to avoid confirmation prompt)

$ sudo dnf history              # ← Check dnf install history
$ sudo dnf history undo 13      # ← Undo/rollback install

$ sudo dnf upgrade              # ← Upgrade all upgradable packages
                                    (patch security bugs)

*Package Report:*
$ dnf list *installed*     # ← report all installed packages
$ dnf list *available*     # ← report all available packages
                                    in any accessible repository
$ dnf list *obsoletes*     # ← report obsoleted by packages
                                    in any accessible repository
$ dnf list *recent   *     # ← report packages recently added
                                    into accessible repositories
$ dnf list *upgrades *     # ← report available packages upgrading

*Avoid dnf/yum update certain packages:*
(This can be needed in critical systems where no downtime is allowed for some service)

- Add next line to /etc/dnf/dnf.conf (new Fedora/RedHat distros) or
                   /etc/yum.conf     ( old Fedora/RedHat distros)
|exclude=kernel* another_package_name_or_name_pattern


*List all available versions of a package *
YUM: sorting by version number:
$ yum list docker-ce --showduplicates | sort -r

*Report (remote) package repositories*
$ dnf repolist
(example output)
→ ...
→ Using metadata from Mon Sep 10 16:21:18 2018
→ repo id                  repo name
→ base                     CentOS-7 - Base
→ centos-openshift-origin  CentOS OpenShift Origin
→ centos-sclo-rh           CentOS-7 - SCLo rh
→ centos-sclo-sclo         CentOS-7 - SCLo sclo
→ code                     Visual Studio Code
→ docker-ce-stable         Docker CE Stable - x86_64
→ *epel                    Extra Packages for Enterprise Linux 7 - x86_64   12,672
→ extras                   CentOS-7 - Extras
→ go-repo                  go-repo - CentOS
→ nodesource               Node.js Packages for Enterprise Linux 7 - x86_64    144
→ openlogic                CentOS-7 - openlogic packages for x86_64            113
→ pgdg94                   PostgreSQL 9.4 7 - x86_64
→ updates                  CentOS-7 - Updates

### SCL (Software Collections) [[{]]
  https://www.softwarecollections.org/en/
  $ python --version
  Python 2.7.5

  $ scl enable rh-python35 bash   <··· Example Ussage:
  $ python --version
  Python 3.5.1

$ sudo yum --disablerepo=\* \            <·····  Install from a given repository:
  --enablerepo=my-cool-repo \
  install myPackage

 $ dnf groupinfo "Development Tools"     <····· $ sudo dnf groupinstall "Development Tools"
 Group: Development Tools                       to install all the group
  Description: A basic development environment.
  Mandatory Packages:
    autoconf
    automake
    binutils
    ...
  Default Packages:
    byacc
    cscope
    ...
  Optional Packages:
    ElectricFence
    ant
    babel
    ...
[[}]]
[[packaging.rpm}]]

## apt (Debian/Ubuntu/...)  [[{packaging.apt]]
  (o 'aptitude' ncurses variant)
- @[https://help.ubuntu.com/community/AptGet/Howto#Maintenance_commands]

- 'apt', introduced by Ubuntu in 2014, is recomended over "older" 'apt-get' and 'apt-cache'.
  by unifying both of them while removing most "obscure" options.
  It also add a progress bar by default.


  $ apt search regexPackageName                ← Show all package with similar name
  $ apt show   package1                        ← Show package details (Version, Origin,
                                                 Maintainer,  *Bugs Reporting*, dependencies,
                                                 recomended extra-packages,  *homepage*,
                                                 Description,
  $ dpkg -L "package_name"                     ← list files in package
  $ dpkg -c foo.deb                            ← lists files in the manually downloaded package
                                                 "./foo.deb".
  INSTALL/UPDATE
  $ sudo apt      install package1 package2    ← (-s to simulate)
  $ sudo apt    reinstall package1             ← Reinstall.
  $ sudo apt     upgrade  package1             ← Upgrade
  $ apt list --upgradable                      <· list what can be updated.
  $ sudo apt     update                        ← Refresh local cache with remote info about
                                                 newest package version.
                                       KEY-POINT Run periodically and after modifications
                                                 to /etc/apt/*.
  $ sudo apt      upgrade                     ← upgrade all installed packages.
  $ sudo apt full-upgrade                     ← Upgrades packages with dependencies auto-handling
  $ sudo apt dist-upgrade                     ← like 'upgrade' adding the "smart upgrade" checkbox.
                                                It tells APT to use "smart" conflict resolution
                                                system, TRYING TO UPGRADE MOST IMPORTANT PACKAGES
                                                AT EXPENSE OF LESS IMPORTANT ONES.
                                             ¡¡¡does not upgrade from a previous version!!!

  $ apt-cache depends --recurse \              ← Show installed package dependencies.
     --installed $package ||grep '[ ]'

  Cleaning/Removal*
  $ sudo apt-get remove "package_name"         ← keep   configuration files
  $ sudo apt-get purge  "package_name"         ← Remove configuration files
  $ sudo apt-get autoclean   ← removes .deb files for packages no longer installed.
                               (saving space in /var/cache/apt/archives)
  $ apt-get clean            ← like autoclean, but removing all packages from package-cache.
                               Not recomended with slow-connections.
                               ($ du -sh /var/cache/apt/archives)
  $ apt-get autoremove       ← removes packages no longer needed

  $ apt-get check            ← update package lists          [[packaging.apt.troubleshooting]]
                               check for broken dependencies

  $ sudo apt -f install      ← Fix Broken Packages ("complains" about "unmet dependencies")
  $ dpkg-reconfigure package1

  $ echo "'package1' hold" |\  ← Put package1 on hold  (AVOID NON-CONTROLLED UPGRADE).
  sudo dpkg --set-selections    WARN: (SIDE-EFFECT) prevents upgrades to packages
                                      depending on updated versions of package1.
                                      ('sudo apt dist-upgrade'  will override it)
  - Setting  *http-proxy*
    - alt 1: Temporary proxy session:
      # export http_proxy=http://username:password@yourproxyaddress:proxyport

    - alt 2: APT configuration file method
      Add next line to /etc/apt/apt.conf
      | Acquire::http::Proxy "http://yourproxyaddress:proxyport";


 *Advanced Package Search*
$*$ dpkg -l $search_term    * ← find packages whose names contain "search_term".
                                It also shows whether a package is installed on your system

$*$ dpkg -S $search_pattern * ← "what package provides this file/package?" reverse lookup.
                                List packages providing or owning the file
                                or package matching search_pattern. 'dlocate' is a faster
                                (but not installed by default) alternative.
                                'apt-file' also allows to search over all available packages,

### APT Sources [[01_PM.TODO]]
  - /etc/apt/sources.list file
    @[https://help.ubuntu.com/community/SourcesList]
[[{packaging.apt}]]

## apk (Alpine)  [[{packaging,01_PM.TODO]]
@[https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management]
- Alpine Linux is designed to run from RAM, implying that package management
  involves two phases:

  - Installing / Upgrading / Deleting packages on a running system.
  - Restoring a system to a previously configured state
    (e.g. after reboot), including all previously installed packages
    and locally modified configuration files. (RAM-Based Installs Only)


  - apk: tool used to install/upgrade/delete software.
  - lbu: tool used to capture the data necessary to
         restore a system to previously configured state.

apk:
  add      Add new packages to the running system
  del      Delete packages from the running system
  fix      Attempt to repair or upgrade an installed package
  update   Update the index of available packages
  info     Prints information about installed or available packages
  search   Search for packages or descriptions with wildcard patterns
  upgrade  Upgrade the currently installed packages
  cache    Maintenance operations for locally cached package repository
  version  Compare version differences between installed and available packages
  index    create a repository index from a list of packages
  fetch    download (but not install) packages
  audit    List changes to the file system from pristine package install state
  verify   Verify a package signature
  dot      Create a graphviz graph description for a given package
  policy   Display the repository that updates a given
           package, plus repositories that also offer the package
  stats    Display statistics, including number of
           packages installed and available, number of
           directories and files, etc.
  manifest Display checksums for files contained in a given package


- Alpine packages:

  Alpine repositories:  directory with *.apk files +
  ├ APKINDEX.tar.gz index
  ├ file1.apk       ← digitally signed tar.gz (often called "a-pack")
  ├ file2.apk
  ├ ...

- The list of repositories to check is stored in /etc/apk/repositories
  (one repo per line)
  Ex:
  /media/sda1/apks
  http://nl.alpinelinux.org/alpine/v3.7/community
   *@edge* http://nl.alpinelinux.org/alpine/edge/main
   *@edgecommunity* http://nl.alpinelinux.org/alpine/edge/community
   *@testing* http://nl.alpinelinux.org/alpine/edge/testing
  ^ "tagged" repo
    will be used like
    # apk add stableapp newapp@ *edge* bleedingapp@ *testing*
    by default only untagged repositories are used

Update the Package list
  # apk update (Fill catch locally the latest APKINDEX.tar.gz from remote repos)

Add Packages (transitive dependencies is automatic):
  # apk add openssh openntp vim

Add a packager from dev. repository (dangerous):
  # apk add cherokee --update-cache \
    --repository http://dl-3.alpinelinux.org/alpine/edge/testing/
    --allow-untrusted

Add local Package
  # apk add --allow-untrusted /path/to/file.apk


Remove a Package

  # apk del openssh

Upgrade a Running System

  # apk update
  # apk upgrade

To upgrade only a few packages, use the add command with the -u or --upgrade option:

apk update
apk add --upgrade busybox
Note: Remember that when you reboot your machine, the remote repository will
not be available until after networking is started. This means packages newer
than your local boot media will likely not be installed after a reboot. To
make an "upgrade" persist over a reboot, use a local cache.

Search for Packages
 # apk search -v        ← list all packages along with descriptions
 # apk search -v 'acf*' ← list all packages part of the ACF system
 # apk search -v --description 'NTP' ← list all packages that list NTP as
                                       part of their description,

Information on Packages
  # apk info -a zlib ← -w: show just webpage info
                       -a: show          all info
[[}]]
[[packaging}]]


# Tracing (Application&Kernel fine grined monitoring)
## Dynamic Tracing Tools [[{monitoring.jobs,profiling,security.encryption]]
                         [[dev_stack.mysql,dev_stack.postgresql,dev_stack.java]]
@[https://www.tecmint.com/bcc-best-linux-performance-monitoring-tools/]

### *sslsniff*                                                                   [[{monitoring.network]]
  @[https://github.com/iovisor/bcc/blob/master/tools/sslsniff_example.txt]
    traces the write/send and read/recv functions of OpenSSL,
    GnuTLS and NSS.  Data passed to this functions is printed as plain
    text. Useful, for example, to sniff HTTP before encrypted with SSL
    See also:
  @[https://github.com/tldr-pages/tldr/blob/master/pages/common/httpflow.md]    [[}]]

### *tcpconnect*                                                                 [[{monitoring.network]]
  @[https://github.com/iovisor/bcc/blob/master/tools/tcpconnect_example.txt]
    traces the kernel function performing active TCP connections
    (eg, via a connect() syscall; accept() are passive connections). Some example
    output (IP addresses changed to protect the innocent):ç

  $*# ./tcpconnect *
    PID    COMM         IP SADDR            DADDR            DPORT
    1479   telnet       4  127.0.0.1        127.0.0.1        23
    1469   curl         4  10.201.219.236   54.245.105.25    80
    1469   curl         4  10.201.219.236   54.67.101.145    80
    1991   telnet       6  ::1              ::1              23
    2015 ssh 6 fe80::2000:bff:fe82:3ac fe80::2000:bff:fe82:3ac 22               [[}]]

### *tcplife *                                                                  [[{monitoring.network]]

  @[https://github.com/iovisor/bcc/blob/master/tools/tcplife_example.txt]
    summarizes TCP sessions that open and close while tracing. For example:
    # ./tcplife
    PID   COMM       LADDR           LPORT RADDR           RPORT TX_KB RX_KB MS
    22597 recordProg 127.0.0.1       46644 127.0.0.1       28527     0     0 0.23
    3277  redis-serv 127.0.0.1       28527 127.0.0.1       46644     0     0 0.28
    22598 curl       100.66.3.172    61620 52.205.89.26    80        0     1 91.79
    22604 curl       100.66.3.172    44400 52.204.43.121   80        0     1 121.38
    22624 recordProg 127.0.0.1       46648 127.0.0.1       28527     0     0 0.22
    3277  redis-serv 127.0.0.1       28527 127.0.0.1       46648     0     0 0.27
    22647 recordProg 127.0.0.1       46650 127.0.0.1       28527     0     0 0.21
    3277  redis-serv 127.0.0.1       28527 127.0.0.1       46650     0     0 0.26
    [...]                                                                      [[}]]

### *ttysnoop*                                                                 [[{security.audit.user]]
  @[https://github.com/iovisor/bcc/blob/master/tools/ttysnoop_example.txt]
    watches a tty or pts device, and prints the same output that is
    appearing on that device. It can be used to mirror the output from a shell
    session, or the system console.                                           [[}]]

### *argdist*                                                                  [[{profiling,troubleshooting]]
  @[https://github.com/iovisor/bcc/blob/master/tools/argdist_example.txt]
    - It probes indicated functions collecting parameter values into a
     *histogram or a frequency count* allowing to understand the distribution
      of values a certain parameter takes, filter and print interesting parameters
     *without attaching a debugger*, and obtain general execution statistics on
      various functions.
    Ex:
    # ./argdist -p 2420 -c -C 'p:c:malloc(size_t size):size_t:size' ← find out allocation sizes common to 2420 process
    p:c:malloc(size_t size):size_t:size
            COUNT      EVENT
            3          size = 16             ← 16 looks to be the most commonly used alloc.size
    [01:42:34]
    p:c:malloc(size_t size):size_t:size
            COUNT      EVENT
            4          size = 16
    ...                                                                      [[}]]

### *bashreadline*                                                     [[{security.audit.user]]
  @[https://github.com/iovisor/bcc/blob/master/tools/bashreadline_example.txt]
    prints bash commands from all running bash shells on the system   [[}]]

### *biolatency*                                                       [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/biolatency_example.txt]
    traces block device I/O (disk I/O), and records the distribution
    of I/O latency (time), printing this as a histogram when Ctrl-C is hit [[}]]

### *biosnoop*                                                         [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/biosnoop_example.txt]
    biosnoop traces block device I/O (disk I/O), and prints a line of output [[}]]

### *biotop*                                                           [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/biotop_example.txt]
    lock device I/O top, biotop summarizes which processes are
    performing disk I/O. It's top for disks.                          [[}]]

### *bitesize*                                                         [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/bitesize_example.txt]
    show I/O distribution for requested block sizes, by process name  [[}]]

### *bpflist*                                                          [[{monitoring.bft,kernel.bft]]
    displays information on running BPF programs and optionally also
    prints open kprobes and uprobes                                   [[}]]

### *cachestat*                                                        [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/cachestat_example.txt]
    shows hits and misses to the file system page cache               [[}]]

### *cachetop*                                                         [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/cachetop_example.txt]
    show Linux page cache hit/miss statistics including read and write hit % per
    processes in a UI like top.                                       [[}]]

### *capable*                                                          [[{security.audit]]
  @[https://github.com/iovisor/bcc/blob/master/tools/capable_example.txt]
    capable traces calls to the kernel cap_capable() function, which does
    security capability checks, and prints details for each call.     [[}]]

### *cpudist*                                                          [[{monitoring.cpu,profiling.cpu]]
  @[https://github.com/iovisor/bcc/blob/master/tools/cpudist_example.txt]
    summarizes task on-CPU time as a histogram, showing how long tasks
    spent on the CPU before being descheduled                         [[}]]

### *cpuunclaimed*                                                     [[{monitoring.cpu,profiling.cpu]]
  @[https://github.com/iovisor/bcc/blob/master/tools/cpuunclaimed_example.txt]
    samples the length of the CPU run queues and determine when there are
    idle CPUs, yet queued threads waiting their turn.                 [[}]]

### *criticalstat*                                                     [[{monitoring.kernel,profiling.kernel]]
    traces and reports occurences of atomic critical sections in the  [[profiling.locks]]
    kernel with useful stacktraces showing the origin of them.
  @[https://github.com/iovisor/bcc/blob/master/tools/criticalstat_example.txt] [[}]]

### *dbslower*                                                         [[{profiling,dev_stack.mysql,dev_stack.postgresql]]
  @[https://github.com/iovisor/bcc/blob/master/tools/dbslower_example.txt]
    traces queries served by a MySQL or PostgreSQL server, and prints
    those that exceed a latency (query time) threshold.

### *dbstat*                                                           [[{profiling,dev_stack.mysql,dev_stack.postgresql]]
  @[https://github.com/iovisor/bcc/blob/master/tools/dbstat_example.txt]
    traces queries performed by a MySQL or PostgreSQL database process, and
    displays a histogram of query latencies.                          [[}]]

### *dcstat*                                                           [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/dcstat_example.txt]
    dcstat shows directory entry cache (dcache) statistics.           [[}]]

### *deadlock*                                                         [[{profiling.locks.deadlock]]
  @[https://github.com/iovisor/bcc/blob/master/tools/deadlock_example.txt]
    This program detects potential deadlocks on a running process. The program
    attaches uprobes on `pthread_mutex_lock` and `pthread_mutex_unlock` to build
    a mutex wait directed graph, and then looks for a cycle in this graph.  [[}]]

### *drsnoop*                                                          [[{monitoring.memory,profiling.memory]]
  @[https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt]
    While tracing, the processes alloc pages，due to insufficient memory available
    in the system, direct reclaim events happened, which will increase the waiting
    delay of the processes.
    drsnoop traces the direct reclaim system-wide, and prints various details.  [[}]]

### *execsnoop*                                                        [[{monitoring.jobs]]
  @[https://github.com/iovisor/bcc/blob/master/tools/execsnoop_example.txt]
    Traces new process                                                [[}]]

### *fileslower*                                                       [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/fileslower_example.txt]
    shows file-based synchronous reads and writes slower than a threshold [[}]]

### *filetop*                                                          [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/filetop_example.txt]
    filetop shows reads and writes by file, with process details.     [[}]]

### *gethostlatency*                                                   [[{monitoring.network]]
  @[https://github.com/iovisor/bcc/blob/master/tools/gethostlatency_example.txt]
    traces host name lookup calls                                     [[}]]

### *hardirqs*                                                         [[{monitoring.hardware,profiling.hardware]]
  @[https://github.com/iovisor/bcc/blob/master/tools/hardirqs_example.txt]
    traces hard interrupts (irqs), and stores timing statistics
    in-kernel for efficiency                                          [[}]]

### *llcstat*                                                         [[{monitoring.memory,profiling.memory]]
  @[https://github.com/iovisor/bcc/blob/master/tools/llcstat_example.txt]
  traces cache reference and cache miss events system-wide, and summarizes
  them by PID and CPU.                                               [[}]]

### *mdflush*                                                         [[{monitoring.storage,profiling.storage]]
  @[https://github.com/iovisor/bcc/blob/master/tools/mdflush_example.txt]
  traces flushes at the md driver (kernel software RAID) level       [[}]]

### *memleak*                                                         [[{monitoring.memory,profiling.memory]]
  @[https://github.com/iovisor/bcc/blob/master/tools/memleak_example.txt]
    traces and matches memory allocation and deallocation requests, and
    collects call stacks for each allocation. memleak can then print a summary
    of which call stacks performed allocations that weren't subsequently freed  [[}]]

### *nfs...*
### *offcputime, offcpu...*                                           [[profiling.locks.profiling]]
  @[https://github.com/iovisor/bcc/blob/master/tools/offcputime_example.txt]
    shows stack traces that were blocked, and the total duration they
    were blocked.                                                    [[}]]

### *oomkill*                                                         [[{monitoring.memory]]
  @[https://github.com/iovisor/bcc/blob/master/tools/oomkill_example.txt]
    simple program that traces the Linux out-of-memory (OOM) killer,
    and shows basic details on one line per OOM kill:                [[}]]

### *wakeuptime*                                                      [[{profiling.locks.profiling]]
  @[https://github.com/iovisor/bcc/blob/master/tools/wakeuptime_example.txt]
  measures when threads block, and shows the stack traces for the
  threads that performed the wakeup, along with the process names of the waker
  and target processes, and the total blocked time.                  [[}]]
[[}]]

## Python,Java,NodeJS  [[{profiling.java,profiling.python,profiling.nodejs]]
                       [[ monitoring.java,monitoring.python,monitoring.nodejs]]
### *ugc*
  @[https://github.com/iovisor/bcc/blob/master/tools/lib/ugc_example.txt]
    traces garbage collection events in high-level languages, including Java,
    Python, Ruby, and Node.

### *ucalls*
  @[https://github.com/iovisor/bcc/blob/master/tools/lib/ucalls_example.txt]
   ucalls summarizes method calls in various high-level languages, including Java,
   Perl, PHP, Python, Ruby, Tcl, and Linux system calls.

### *uflow*
  @[https://github.com/iovisor/bcc/blob/master/tools/lib/uflow_example.txt]
    uflow traces method entry and exit events and prints a visual flow graph that
    shows how methods are entered and exited, similar to a tracing debugger with
    breakpoints. This can be useful for understanding program flow in high-level
    languages such as Java, Perl, PHP, Python, Ruby, and Tcl which provide USDT
    probes for method invocations.

### *uobjnew*
  @[https://github.com/iovisor/bcc/blob/master/tools/lib/uobjnew_example.txt]
    summarizes new object allocation events and prints out statistics on
    which object type has been allocated frequently, and how many bytes of that
    type have been allocated. This helps diagnose common allocation paths, which
    can in turn cause heavy garbage collection.

### *ustat*
  @[https://github.com/iovisor/bcc/blob/master/tools/lib/ustat_example.txt]
    ustat is a "top"-like tool for monitoring events in high-level languages. It
    prints statistics about garbage collections, method calls, object allocations,
    and various other events for every process that it recognizes with a Java,
    Node, Perl, PHP, Python, Ruby, and Tcl runtime.

### *uthreads*
  @[https://github.com/iovisor/bcc/blob/master/tools/lib/uthreads_example.txt]
    traces thread creation events in Java or raw (C) pthreads, and prints
    details about the newly created thread. For Java threads, the thread name is
    printed; for pthreads, the thread's start function is printed, if there is
    symbol information to resolve it.

###  filelife
###  fileslower
###  vfscount
###  vfsstat
###  dcstat, ...
[[}]]

## Kernel Monitorization [[{monitoring,monitoring.kernel,monitoring.I/O,monitoring.network,monitoring.jobs,]]
  Kernel Monit.Diagram   [[02_doc_has.diagram,troubleshooting]]
  ↑        ┌─────────────────────────────────────────────────────────────────────────┐
  │        │                          APPLICATIONS                                   |
  │        ├─────────────────────────────────────────────────────────────────────────┤
  │        │[ltrace][ldd]            System Libraries [gethostlatency]         [perf]│
  │        ├─────────────────────────────────────────────────────────────────────────┤
  │        │[strace][sydgid]           System Call Interface [*3]              [perf]│
  │        │[opensnoop] [statsnoop] [syncsnoop]                                      │     CPU
[perf]   ↑ ├───────────────────────┬───────┬─────────────────────┬┬──────────────────┤Interconnect  ┌───────────────┐
[dtrace] │ │ VFS [opensnoop]       │       │SOCKETS  [ss]        ││SCHEDULER   [perf]|              |CPU1[perf]     |
         │ │                       │       │                     ││                  ├──────────────┤               ├───┐
[stap]  L K├───────────────────────┤       │─────────────────────┤│[perf][latencytop] ←───[top]     └───────────────┘   │
[lttnp] I E│ FS [lsof][fstrace]    │       │TCP/UPD  [*2]        ││[mpstat]          │   [ps]      Memory│              │
[lttnp] N R│ [filelie][pcstat]     │       │[tcptop] [tcplife]   ││                  │  ╱[pidstat]    BUS│ [tiptop]     │
[ktap]  U N├───────────────────────┤       │tcpconnect,tcpaccept │├──────────────────┤ ╱                 │ [perf]       │
  │     X E│ VOLUME MANAGERS       │       │tcpconnlat,tcpretrans││VIRTUAL MEMORY    │╱                  │              │
  │      │L│ [mdflush]             │       │                     ││[vmstat]          ←                 ┌───┐            │
  │      │ │                       │       │─────────────────────┤│[slabtop]         ├────────────────→│RAM│            │
  │      │ ├───────────────────────┤       │IP                   ││[free]            │                 └───┘            │
  │      │ │ BLOCK DEVICE          │       │[ip]                 ││[memleak]         │                 [numastat]       │
  │      │ │ Interface             │       │[route]              ││[comkill]         │                 [lstop]          │
  │      │ │ [iostat] [iotop]      │       │[iptables]           ││[slabratetop]     │                                  │
  │      │ │ [blktrace] [pidstat]  │       │─────────────────────│├──────────────────┤                                  │
  │      │ │ [biosnoop][biolatency]│       │Ethernet [ss]        ││CLOCKSOURCE       │                                  │
  │      │ │ [biotop][blktrace]    │       │[tcpdump]            ││[/sys/...]        │                                  │
  │      │ ├──·────────────────────┴───────┴─────────────────────┴┴──────────────────┤                                  │
  │      │ │  ·[hardirqs][criticalstat]  Device Drivers                              │                                  │
  ↓      ↓ └──·──────────────────────────────────────────────────────────────────────┘             I/O [perf]           │
              ·                  Expander-Interconnect                           ┌──────────┐     BUS  [tiptop]         │
              ·    ─┬──────────────────────────────────────────────┬─────────────┤I/O Bridge├───────────────────────────┘
              ·     │                                              │             └──────────┘
              └┐    │                                              │
              ┌v────┴───────────┐                          ┌───────┴───────────┐[nicstat]
              │I/O Controller *1│                          │ Network Controller│[ss]
              └─────────────────┘                          └───────────────────┘[ip]
             ┬──────┴───────┬                               ┬──────┴────┬
             │              │                               │           │
            Disk[*1]       Swap [swapon]                   Port        Port
                                                           [ping] [traceroute]
                                                           [ethtool] [snmpget]
                                                           [lldptool]
  OTHERS: [sar] [dstat] [/proc]

   ┌───┐[sar -m FAN]       ┌────────────┐[ipmitool]
   │FAN│                   │POWER SUPPLY│[dmidecode]
   └───┘                   └────────────┘

### BCC – Dynamic Tracing toolkit for creating efficient Linux kernel tracing, [[{01_PM.TODO]]
  performance Monitoring, networking and much more.
- It makes use of extended BPF (Berkeley Packet Filters), formally known as eBPF [[}]]
@[https://github.com/iovisor/bcc]

### Linux Tracer comparative:  [[{02_doc_has.comparative,01_PM.TODO]]
  http://www.brendangregg.com/blog/2015-07-08/choosing-a-linux-tracer.html
  [[}]]
[[}]]

## ltrace: Library Call Tracer [[{monitoring.jobs]]
@[https://linux.die.net/man/1/ltrace]
Summary:
*ltrace                           | ltrace -c  # ← Count time and calls for each library call*
*                                                  and report a summary on program exit.     *
  [-e filter|-L]                 |   [-e filter|-L]
  [-l|--library=library_pattern] |   [-l|--library=library_pattern]
  [-x filter]                    |   [-x filter]
  [-S]                           |   [-S]
  [-b|--no-signals]              |
  [-i] [-w|--where=nr]           |
  [-r|-t|-tt|-ttt]               |
  [-T]                           |
  [-F pathlist]                  |
  [-A maxelts]                   |
  [-s strsize]                   |
  [-C|--demangle]                |
  [-a|--align column]            |
  [-n|--indent nr]               |
  [-o|--output filename]         |   [-o|--output filename]
  [-D|--debug mask]              |
  [-u username]                  |
  [-f]                           |   [-f]
  [-p pid]                       |   [-p pid]
  [ [--] command [arg ...] ]     |   [ [--] command [arg ...] ]

 - runs the specified command until it exits, intercepting/recording:
   + dynamic library calls  by process
     - Display functions and funct.parameters.
     - external prototype libraries is needed
       for human-readable output.
       (ltrace.conf(5), section PROTOTYPE LIBRARY DISCOVERY )

   + signals which received by process

   + system calls           by process

[[}]]

## strace: System call tracer [[{monitoring.jobs]] (man 1 strace)
SUMMARY
strace                    | strace -c  ← -c: Count time, calls, and errors
                          |                  for each system call and report summary on exit.
                          |                  -f aggregate over all forked processes
  [ -dDffhiqrtttTvVxx ]   |   [ -D ]
  [ -acolumn ]            |
  [ -eexpr ] ...          |   [ -eexpr ] ...
                          |   [ -Ooverhead ]
  [ -ofile ]              |
  [ -ppid ] ...           |
  [ -sstrsize ]           |
  [ -uusername ]          |
  [ -Evar=val ] ...       |
  [ -Evar ] ...           |   [ -Ssortby ]
                          |   [ -Ssortby ]
  [ command [ arg ... ] ] |   [ command [ arg ... ] ]


strace runs specified command until it exits intercepting:
  +  system calls called by a process
     - system-call-name + arguments + return-value is printed to STDERR (or -o file)
       Ex output:
       open("/dev/null", O_RDONLY) = 3
       open("/foo/bar", O_RDONLY) = -1 ENOENT (No such file or directory)

  +  signals    received by a process
       Ex output:
       $ strace sleep 111
       → ...
       → sigsuspend([] <unfinished ...>
       → --- SIGINT (Interrupt) ---     ← Signal received
       → +++ killed by SIGINT +++

If a system call is being executed and meanwhile another one is being called
from a different thread/process then strace will try to preserve the order
of those events and mark the ongoing call as being unfinished.
When the call returns it will be marked as resumed. Ex. output:
  → [pid 28772] select(4, [3], NULL, NULL, NULL  *unfinished ... *
  → [pid 28779] clock_gettime(CLOCK_REALTIME, {1130322148, 939977000}) = 0
  → [pid 28772] *<... select resumed>* )      = 1 (in [3])

Interruption of a (restartable) system call by a signal delivery is
processed differently as kernel terminates the system call and also
arranges its immediate reexecution after the signal handler completes.

read(0, 0x7ffff72cf5cf, 1)              = ? *ERESTARTSYS (To be restarted)*
--- SIGALRM (Alarm clock) @ 0 (0) ---
rt_sigreturn(0xe)                       = 0
read(0, ""..., 1)                       = 0

- explain: decode error returned from strace
  https://linux.die.net/man/1/explain
[[}]]

## mpstat-CPU stats (ints., hypervisor...) [[{monitoring.cpu,monitoring.hypervisor]]
@[https://linux.die.net/man/1/mpstat]
mpstat
  [ -A ]                        ==  -I ALL -u -P ALL
  [ -I { SUM | CPU | ALL } ]    ==  Report interrupts statistics
  [ -u ]                            Reports cpu utilization (default)
  [ -P { cpu [,...] | ON | ALL } ]  Indicates the processor number
  [ -V ]
  [ secs_interval [ count ] ]
    secs_interval = 0 => Report from times system startup (boot)

mpstat writes to standard output activities for each available processor,
Global average activities among all processors are also reported.


- CPU output columns:
 %usr   :  executing at the user level (application).
 %nice  :  executing at the user level with nice priority.
 %sys   :  executing at the system level (kernel).
           It does NOT include time spent servicing hardware
           and software interrupts.
 %iowait:  idle during which the system had an outstanding disk I/O request.
 %irq   :  time spent by the CPU or CPUs to service hardware interrupts.
 %soft  :  time spent by the CPU or CPUs to service software interrupts.
*%steal :  time spent in involuntary wait by the virtual CPU or CPUs     *
*           while the hypervisor was servicing another virtual processor.*
 %guest : time spent by the CPU or CPUs to run a virtual processor.
 %idle  : time that the CPU or CPUs were idle and the system did not have
          an outstanding disk I/O request.
[[}]]




# Integrated Admin Tools [[{configuration.foreman,QA.UX,01_PM.TODO]]
## Foreman Server Lifecycle Mng. [[{]]
@[https://theforeman.org/introduction.html]
@[https://github.com/theforeman/foreman_bootdisk]
[[}]]

## cockpit Web UI [[{QA.UX,configuration.cockpit,monitoring,security.aaa,security.selinux]]
                 [[troubleshooting]]
@[https://cockpit-project.org/]
- easy-to-use, integrated, "glanceable", web-based interface for servers.

@[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8-beta/html/managing_systems_using_the_cockpit_web_interface/getting-started-with-cockpit_system-management-using-cockpit]

- UI with wide range of administration tasks, including:
  - Managing services, user accounts, system services
    network interfaces,  firewall, virtual machines,
    kernel-dump-configuration, SELinux, software updates,
    system subscriptions.
  - system logs tracing,
  - diagnostic reports.
[[}]]
[[}]]


# Non Classified:
## ZSwap For low-mem systems  [[{configuration.memory,troubleshooting.memory,storage.compression]]
- Increase system performance in systems with memory-preasure
@[https://www.maketecheasier.com/use-zswap-improve-old-linux-performance/]

-*Debian/Ubuntu flavour Setup*
    STEP 1: Edit /etc/default/grub and add "zswap.enabled=1" option to "GRUB_CMDLINE_LINUX_DEFAULT".
      For example, a line like
      | GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"
      becomes
      | GRUB_CMDLINE_LINUX_DEFAULT="quiet splash zswap.enabled=1".

    STEP 2: Update Grub configuration:
    $ sudo update-grub

    STEP 3: Reboot and the zswap module will be enabled automatically.

    STEP 4: After you reboot, check if the module is active:
    $ cat /sys/module/zswap/parameters/enabled

-*Fedora/RedHat flavours Setup*
  STEP 1: Edit /etc/default/grub and add "zswap.enabled=1" option to GRUB_CMDLINE_LINUX

  STEP 2: Update Grub configuration:
  $ sudo grub2-mkconfig -o $(sudo find /boot/ -name grub.cfg)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                            Depending on whether your computer
                            boots from a BIOS or UEFI system
                            the path will change

  STEP 3: Reboot and the zswap module will be enabled automatically.

  STEP 4: After you reboot, check if the module is active:
  $ cat /sys/module/zswap/parameters/enabled
  (must display "Y,")

-*Tunning*
  - Check adding the (grub) option "zswap.max_pool_percent=20" to see if performance increase.
    It’s recommended NOT to go above 50% since more than that can have detrimental effects
    on systems with low amounts of RAM.
[[}]]


